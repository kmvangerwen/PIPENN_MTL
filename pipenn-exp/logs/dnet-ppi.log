
## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 12-06-2023#13:31:50 @@
## tf-version: 2.4.1|| keras-version: 2.4.0|| float_type: float64
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1170, 21)]        0         
_________________________________________________________________
conv1d (Conv1D)              (None, 1170, 64)          9408      
_________________________________________________________________
dropout (Dropout)            (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization (BatchNo (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu (PReLU)              (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1170, 128)         57344     
_________________________________________________________________
dropout_1 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_1 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 1170, 128)         114688    
_________________________________________________________________
dropout_2 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_2 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 1170, 64)          57344     
_________________________________________________________________
dropout_3 (Dropout)          (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu_3 (PReLU)            (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 1170, 32)          14336     
_________________________________________________________________
dropout_4 (Dropout)          (None, 1170, 32)          0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 1170, 32)          128       
_________________________________________________________________
p_re_lu_4 (PReLU)            (None, 1170, 32)          37440     
_________________________________________________________________
time_distributed (TimeDistri (None, 1170, 1)           33        
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 12-06-2023#13:37:19 @@
## tf-version: 2.4.1|| keras-version: 2.4.0|| float_type: float64
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1170, 21)]        0         
_________________________________________________________________
conv1d (Conv1D)              (None, 1170, 64)          9408      
_________________________________________________________________
dropout (Dropout)            (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization (BatchNo (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu (PReLU)              (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1170, 128)         57344     
_________________________________________________________________
dropout_1 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_1 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 1170, 128)         114688    
_________________________________________________________________
dropout_2 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_2 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 1170, 64)          57344     
_________________________________________________________________
dropout_3 (Dropout)          (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu_3 (PReLU)            (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 1170, 32)          14336     
_________________________________________________________________
dropout_4 (Dropout)          (None, 1170, 32)          0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 1170, 32)          128       
_________________________________________________________________
p_re_lu_4 (PReLU)            (None, 1170, 32)          37440     
_________________________________________________________________
time_distributed (TimeDistri (None, 1170, 1)           33        
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________
Shape featuresTable: (343, 1) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 21)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1756 valLoss: 0.1336 valAcc: 11.32% AUC: 51.78%
Epoch: 01 Loss: 0.1537 valLoss: 0.1333 valAcc: 11.32% AUC: 51.92%
Epoch: 02 Loss: 0.1436 valLoss: 0.1337 valAcc: 11.32% AUC: 50.71%
Epoch: 03 Loss: 0.1392 valLoss: 0.1339 valAcc: 11.32% AUC: 50.68%
Epoch: 04 Loss: 0.1372 valLoss: 0.1337 valAcc: 11.34% AUC: 51.63%
Epoch: 05 Loss: 0.1366 valLoss: 0.1333 valAcc: 11.35% AUC: 53.22%
Epoch: 06 Loss: 0.1359 valLoss: 0.1327 valAcc: 11.36% AUC: 55.11%
Epoch: 07 Loss: 0.1351 valLoss: 0.1321 valAcc: 11.36% AUC: 56.73%
Epoch: 08 Loss: 0.1345 valLoss: 0.1318 valAcc: 11.39% AUC: 57.87%
Epoch: 09 Loss: 0.1339 valLoss: 0.1315 valAcc: 11.38% AUC: 58.87%
Epoch: 10 Loss: 0.1336 valLoss: 0.1314 valAcc: 11.39% AUC: 59.50%
Epoch: 11 Loss: 0.1333 valLoss: 0.1312 valAcc: 11.38% AUC: 60.24%
Epoch: 12 Loss: 0.1327 valLoss: 0.1311 valAcc: 11.35% AUC: 60.62%
Epoch: 13 Loss: 0.1324 valLoss: 0.1308 valAcc: 11.33% AUC: 61.45%
Epoch: 14 Loss: 0.1319 valLoss: 0.1306 valAcc: 11.32% AUC: 61.92%
Epoch: 15 Loss: 0.1318 valLoss: 0.1305 valAcc: 11.32% AUC: 61.97%
Epoch: 16 Loss: 0.1317 valLoss: 0.1303 valAcc: 11.32% AUC: 62.20%
Epoch: 17 Loss: 0.1312 valLoss: 0.1302 valAcc: 11.32% AUC: 62.45%
Epoch: 18 Loss: 0.1309 valLoss: 0.1301 valAcc: 11.33% AUC: 62.57%
Epoch: 19 Loss: 0.1306 valLoss: 0.1299 valAcc: 11.37% AUC: 62.72%
Epoch: 20 Loss: 0.1305 valLoss: 0.1298 valAcc: 11.46% AUC: 62.78%
Epoch: 21 Loss: 0.1303 valLoss: 0.1297 valAcc: 11.50% AUC: 62.90%
Epoch: 22 Loss: 0.1297 valLoss: 0.1296 valAcc: 11.66% AUC: 62.83%
Epoch: 23 Loss: 0.1298 valLoss: 0.1295 valAcc: 11.74% AUC: 63.04%
Epoch: 24 Loss: 0.1296 valLoss: 0.1295 valAcc: 11.84% AUC: 62.95%
Epoch: 25 Loss: 0.1293 valLoss: 0.1295 valAcc: 11.98% AUC: 63.00%
Epoch: 26 Loss: 0.1292 valLoss: 0.1294 valAcc: 12.09% AUC: 63.21%
Epoch: 27 Loss: 0.1292 valLoss: 0.1293 valAcc: 12.20% AUC: 63.12%
Epoch: 28 Loss: 0.1291 valLoss: 0.1293 valAcc: 12.37% AUC: 63.12%
Epoch: 29 Loss: 0.1284 valLoss: 0.1293 valAcc: 12.69% AUC: 62.99%
Epoch: 30 Loss: 0.1283 valLoss: 0.1294 valAcc: 12.80% AUC: 62.85%
Epoch: 31 Loss: 0.1281 valLoss: 0.1294 valAcc: 13.18% AUC: 62.76%
Epoch: 32 Loss: 0.1281 valLoss: 0.1293 valAcc: 13.40% AUC: 62.82%
Epoch: 33 Loss: 0.1277 valLoss: 0.1292 valAcc: 13.03% AUC: 62.80%
Epoch: 34 Loss: 0.1276 valLoss: 0.1293 valAcc: 13.79% AUC: 62.61%
Epoch: 35 Loss: 0.1277 valLoss: 0.1293 valAcc: 14.36% AUC: 62.62%
Epoch: 36 Loss: 0.1270 valLoss: 0.1294 valAcc: 14.97% AUC: 62.49%
Epoch: 37 Loss: 0.1268 valLoss: 0.1292 valAcc: 14.72% AUC: 62.81%
Epoch: 38 Loss: 0.1265 valLoss: 0.1290 valAcc: 14.70% AUC: 62.99%
Epoch: 39 Loss: 0.1266 valLoss: 0.1292 valAcc: 15.83% AUC: 62.60%
Epoch: 40 Loss: 0.1263 valLoss: 0.1292 valAcc: 16.65% AUC: 62.50%
Epoch: 41 Loss: 0.1262 valLoss: 0.1294 valAcc: 18.65% AUC: 62.03%
Epoch: 42 Loss: 0.1260 valLoss: 0.1292 valAcc: 17.94% AUC: 62.26%
Epoch: 43 Loss: 0.1258 valLoss: 0.1294 valAcc: 17.72% AUC: 61.89%
Epoch: 44 Loss: 0.1255 valLoss: 0.1294 valAcc: 19.88% AUC: 61.83%
Epoch: 45 Loss: 0.1251 valLoss: 0.1295 valAcc: 21.48% AUC: 61.56%
Epoch: 46 Loss: 0.1249 valLoss: 0.1295 valAcc: 22.73% AUC: 61.88%
Epoch: 47 Loss: 0.1247 valLoss: 0.1294 valAcc: 21.59% AUC: 61.57%
Epoch: 48 Loss: 0.1244 valLoss: 0.1297 valAcc: 25.01% AUC: 61.43%
Epoch: 49 Loss: 0.1246 valLoss: 0.1299 valAcc: 26.54% AUC: 61.37%
Epoch: 50 Loss: 0.1239 valLoss: 0.1300 valAcc: 28.20% AUC: 61.38%
Epoch: 51 Loss: 0.1239 valLoss: 0.1301 valAcc: 26.74% AUC: 61.07%
Epoch: 52 Loss: 0.1241 valLoss: 0.1303 valAcc: 29.22% AUC: 60.99%
Epoch: 53 Loss: 0.1230 valLoss: 0.1309 valAcc: 32.82% AUC: 60.72%
Epoch: 54 Loss: 0.1227 valLoss: 0.1315 valAcc: 36.60% AUC: 60.76%
Epoch: 55 Loss: 0.1226 valLoss: 0.1307 valAcc: 31.04% AUC: 60.59%
Epoch: 56 Loss: 0.1223 valLoss: 0.1306 valAcc: 31.26% AUC: 60.54%
Epoch: 57 Loss: 0.1224 valLoss: 0.1314 valAcc: 36.10% AUC: 60.59%
Epoch: 58 Loss: 0.1221 valLoss: 0.1311 valAcc: 33.72% AUC: 60.25%
Epoch: 59 Loss: 0.1214 valLoss: 0.1318 valAcc: 38.25% AUC: 60.36%
Epoch: 60 Loss: 0.1210 valLoss: 0.1324 valAcc: 40.91% AUC: 60.28%
Epoch: 61 Loss: 0.1205 valLoss: 0.1339 valAcc: 44.50% AUC: 59.81%
Epoch: 62 Loss: 0.1208 valLoss: 0.1336 valAcc: 43.98% AUC: 59.83%
Epoch: 63 Loss: 0.1202 valLoss: 0.1343 valAcc: 45.65% AUC: 59.74%
Epoch: 64 Loss: 0.1195 valLoss: 0.1351 valAcc: 47.74% AUC: 59.67%
Epoch: 65 Loss: 0.1199 valLoss: 0.1336 valAcc: 44.07% AUC: 59.80%
Epoch: 66 Loss: 0.1188 valLoss: 0.1365 valAcc: 50.92% AUC: 59.43%
Epoch: 67 Loss: 0.1189 valLoss: 0.1359 valAcc: 49.73% AUC: 59.61%
Epoch: 68 Loss: 0.1177 valLoss: 0.1379 valAcc: 52.97% AUC: 59.19%
Epoch: 69 Loss: 0.1177 valLoss: 0.1363 valAcc: 50.52% AUC: 59.65%
Epoch: 70 Loss: 0.1175 valLoss: 0.1394 valAcc: 55.33% AUC: 59.27%
Epoch: 71 Loss: 0.1171 valLoss: 0.1402 valAcc: 57.28% AUC: 59.48%
Epoch: 72 Loss: 0.1167 valLoss: 0.1386 valAcc: 53.72% AUC: 59.24%
Epoch: 73 Loss: 0.1163 valLoss: 0.1419 valAcc: 57.99% AUC: 59.00%
Epoch: 74 Loss: 0.1159 valLoss: 0.1445 valAcc: 60.87% AUC: 58.90%
Epoch: 75 Loss: 0.1152 valLoss: 0.1422 valAcc: 58.77% AUC: 59.07%
Epoch: 76 Loss: 0.1147 valLoss: 0.1428 valAcc: 59.37% AUC: 59.17%
Epoch: 77 Loss: 0.1144 valLoss: 0.1476 valAcc: 63.74% AUC: 58.69%
Epoch: 78 Loss: 0.1136 valLoss: 0.1513 valAcc: 65.89% AUC: 58.58%
Epoch: 79 Loss: 0.1128 valLoss: 0.1465 valAcc: 62.06% AUC: 58.95%
Epoch: 80 Loss: 0.1129 valLoss: 0.1538 valAcc: 66.89% AUC: 58.36%
Epoch: 81 Loss: 0.1120 valLoss: 0.1600 valAcc: 69.75% AUC: 58.29%
Epoch: 82 Loss: 0.1121 valLoss: 0.1524 valAcc: 65.56% AUC: 58.59%
Epoch: 83 Loss: 0.1116 valLoss: 0.1539 valAcc: 65.22% AUC: 58.41%
Epoch: 84 Loss: 0.1107 valLoss: 0.1565 valAcc: 66.10% AUC: 58.21%
Epoch: 85 Loss: 0.1106 valLoss: 0.1598 valAcc: 68.64% AUC: 58.25%
Epoch: 86 Loss: 0.1094 valLoss: 0.1640 valAcc: 69.11% AUC: 58.02%
Epoch: 87 Loss: 0.1094 valLoss: 0.1699 valAcc: 72.18% AUC: 58.34%
Epoch: 88 Loss: 0.1084 valLoss: 0.1689 valAcc: 72.15% AUC: 58.47%
Epoch: 89 Loss: 0.1075 valLoss: 0.1872 valAcc: 76.67% AUC: 58.25%
Epoch: 90 Loss: 0.1076 valLoss: 0.1694 valAcc: 71.27% AUC: 58.34%
Epoch: 91 Loss: 0.1060 valLoss: 0.1952 valAcc: 77.63% AUC: 58.05%
Epoch: 92 Loss: 0.1055 valLoss: 0.1758 valAcc: 72.84% AUC: 58.01%
Epoch: 93 Loss: 0.1058 valLoss: 0.1792 valAcc: 72.92% AUC: 57.84%
Epoch: 94 Loss: 0.1052 valLoss: 0.1867 valAcc: 74.10% AUC: 57.57%
Epoch: 95 Loss: 0.1043 valLoss: 0.1878 valAcc: 74.97% AUC: 58.04%
Epoch: 96 Loss: 0.1036 valLoss: 0.1936 valAcc: 75.91% AUC: 58.09%
Epoch: 97 Loss: 0.1025 valLoss: 0.2150 valAcc: 79.66% AUC: 58.16%
Epoch: 98 Loss: 0.1025 valLoss: 0.2163 valAcc: 79.79% AUC: 58.02%
Epoch: 99 Loss: 0.1016 valLoss: 0.2008 valAcc: 76.44% AUC: 57.95%
Epoch: 100 Loss: 0.1009 valLoss: 0.2162 valAcc: 78.61% AUC: 58.12%
Epoch: 101 Loss: 0.1010 valLoss: 0.1997 valAcc: 75.36% AUC: 57.72%
Epoch: 102 Loss: 0.1003 valLoss: 0.2133 valAcc: 77.92% AUC: 58.07%
Epoch: 103 Loss: 0.0984 valLoss: 0.2293 valAcc: 80.09% AUC: 58.18%
Epoch: 104 Loss: 0.0988 valLoss: 0.2121 valAcc: 76.98% AUC: 58.00%
Epoch: 105 Loss: 0.0978 valLoss: 0.2421 valAcc: 81.36% AUC: 57.89%
Epoch: 106 Loss: 0.0986 valLoss: 0.2089 valAcc: 76.44% AUC: 57.85%
Restoring model weights from the best epoch: 26 with the best VAL_AUC: 63.21%
Training time: 0:15:13.910311
## Testing ../data/prepared_epitope_testing.csv at: 12-06-2023#13:52:37 ##
Shape featuresTable: (56, 1) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 21)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:00.520528
The best cut-off value is: 0.60
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[14989  1136]
 [ 1136   249]]
ValAcc: 87.02% specScore: 92.96% presScore: 17.98% recallScore: 17.98% F1Score: 17.98% MCC: 10.93% AUC: 67.56% AP: 14.66%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 12-06-2023#14:31:13 @@
## tf-version: 2.12.0|| keras-version: 2.12.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 21)]        0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          9408      
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (BatchN  (None, 1170, 64)         256       
 ormalization)                                                   
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Batc  (None, 1170, 64)         256       
 hNormalization)                                                 
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Batc  (None, 1170, 32)         128       
 hNormalization)                                                 
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDistr  (None, 1170, 1)          33        
 ibuted)                                                         
                                                                 
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________
Shape featuresTable: (343, 1) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 21)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1716 valLoss: 0.1333 valAcc: 11.32% AUC: 54.07%
Epoch: 01 Loss: 0.1605 valLoss: 0.1329 valAcc: 11.32% AUC: 54.35%
Epoch: 02 Loss: 0.1512 valLoss: 0.1334 valAcc: 11.32% AUC: 54.69%
Epoch: 03 Loss: 0.1461 valLoss: 0.1340 valAcc: 11.32% AUC: 55.16%
Epoch: 04 Loss: 0.1426 valLoss: 0.1337 valAcc: 11.32% AUC: 55.77%
Epoch: 05 Loss: 0.1406 valLoss: 0.1329 valAcc: 11.33% AUC: 56.36%
Epoch: 06 Loss: 0.1378 valLoss: 0.1325 valAcc: 11.33% AUC: 56.78%
Epoch: 07 Loss: 0.1363 valLoss: 0.1321 valAcc: 11.33% AUC: 57.20%
Epoch: 08 Loss: 0.1346 valLoss: 0.1316 valAcc: 11.35% AUC: 57.92%
Epoch: 09 Loss: 0.1335 valLoss: 0.1314 valAcc: 11.35% AUC: 58.53%
Epoch: 10 Loss: 0.1335 valLoss: 0.1311 valAcc: 11.33% AUC: 59.36%
Epoch: 11 Loss: 0.1328 valLoss: 0.1309 valAcc: 11.33% AUC: 60.00%
Epoch: 12 Loss: 0.1328 valLoss: 0.1308 valAcc: 11.33% AUC: 60.42%
Epoch: 13 Loss: 0.1319 valLoss: 0.1306 valAcc: 11.34% AUC: 60.79%
Epoch: 14 Loss: 0.1317 valLoss: 0.1305 valAcc: 11.36% AUC: 60.94%
Epoch: 15 Loss: 0.1317 valLoss: 0.1304 valAcc: 11.37% AUC: 61.15%
Epoch: 16 Loss: 0.1310 valLoss: 0.1304 valAcc: 11.44% AUC: 61.35%
Epoch: 17 Loss: 0.1306 valLoss: 0.1303 valAcc: 11.56% AUC: 61.51%
Epoch: 18 Loss: 0.1304 valLoss: 0.1303 valAcc: 11.73% AUC: 61.52%
Epoch: 19 Loss: 0.1303 valLoss: 0.1303 valAcc: 11.78% AUC: 61.52%
Epoch: 20 Loss: 0.1301 valLoss: 0.1303 valAcc: 11.77% AUC: 61.65%
Epoch: 21 Loss: 0.1299 valLoss: 0.1303 valAcc: 12.07% AUC: 61.75%
Epoch: 22 Loss: 0.1291 valLoss: 0.1303 valAcc: 12.09% AUC: 61.72%
Epoch: 23 Loss: 0.1295 valLoss: 0.1303 valAcc: 12.32% AUC: 61.72%
Epoch: 24 Loss: 0.1290 valLoss: 0.1303 valAcc: 12.50% AUC: 61.85%
Epoch: 25 Loss: 0.1290 valLoss: 0.1302 valAcc: 12.58% AUC: 61.95%
Epoch: 26 Loss: 0.1288 valLoss: 0.1302 valAcc: 12.69% AUC: 61.88%
Epoch: 27 Loss: 0.1284 valLoss: 0.1301 valAcc: 12.86% AUC: 61.97%
Epoch: 28 Loss: 0.1287 valLoss: 0.1301 valAcc: 13.06% AUC: 61.94%
Epoch: 29 Loss: 0.1284 valLoss: 0.1301 valAcc: 13.12% AUC: 61.99%
Epoch: 30 Loss: 0.1279 valLoss: 0.1301 valAcc: 13.37% AUC: 61.98%
Epoch: 31 Loss: 0.1278 valLoss: 0.1301 valAcc: 13.72% AUC: 62.20%
Epoch: 32 Loss: 0.1278 valLoss: 0.1301 valAcc: 14.30% AUC: 62.22%
Epoch: 33 Loss: 0.1274 valLoss: 0.1300 valAcc: 14.63% AUC: 62.37%
Epoch: 34 Loss: 0.1274 valLoss: 0.1301 valAcc: 14.79% AUC: 62.27%
Epoch: 35 Loss: 0.1270 valLoss: 0.1301 valAcc: 14.74% AUC: 62.20%
Epoch: 36 Loss: 0.1268 valLoss: 0.1301 valAcc: 15.05% AUC: 62.10%
Epoch: 37 Loss: 0.1269 valLoss: 0.1299 valAcc: 15.37% AUC: 62.22%
Epoch: 38 Loss: 0.1263 valLoss: 0.1298 valAcc: 16.41% AUC: 62.37%
Epoch: 39 Loss: 0.1263 valLoss: 0.1299 valAcc: 17.73% AUC: 62.43%
Epoch: 40 Loss: 0.1264 valLoss: 0.1300 valAcc: 17.94% AUC: 62.15%
Epoch: 41 Loss: 0.1259 valLoss: 0.1302 valAcc: 18.56% AUC: 62.04%
Epoch: 42 Loss: 0.1258 valLoss: 0.1301 valAcc: 19.03% AUC: 62.09%
Epoch: 43 Loss: 0.1254 valLoss: 0.1301 valAcc: 19.89% AUC: 62.11%
Epoch: 44 Loss: 0.1256 valLoss: 0.1301 valAcc: 19.51% AUC: 62.09%
Epoch: 45 Loss: 0.1252 valLoss: 0.1301 valAcc: 20.69% AUC: 62.04%
Epoch: 46 Loss: 0.1252 valLoss: 0.1301 valAcc: 20.91% AUC: 61.93%
Epoch: 47 Loss: 0.1251 valLoss: 0.1301 valAcc: 22.15% AUC: 61.93%
Epoch: 48 Loss: 0.1246 valLoss: 0.1304 valAcc: 24.24% AUC: 61.76%
Epoch: 49 Loss: 0.1241 valLoss: 0.1303 valAcc: 23.86% AUC: 61.73%
Epoch: 50 Loss: 0.1241 valLoss: 0.1305 valAcc: 27.42% AUC: 61.80%
Epoch: 51 Loss: 0.1242 valLoss: 0.1301 valAcc: 25.51% AUC: 61.86%
Epoch: 52 Loss: 0.1237 valLoss: 0.1303 valAcc: 26.56% AUC: 61.80%
Epoch: 53 Loss: 0.1231 valLoss: 0.1307 valAcc: 29.60% AUC: 61.71%
Epoch: 54 Loss: 0.1233 valLoss: 0.1304 valAcc: 27.95% AUC: 61.59%
Epoch: 55 Loss: 0.1231 valLoss: 0.1306 valAcc: 30.88% AUC: 61.80%
Epoch: 56 Loss: 0.1227 valLoss: 0.1306 valAcc: 32.22% AUC: 61.78%
Epoch: 57 Loss: 0.1225 valLoss: 0.1306 valAcc: 32.57% AUC: 61.82%
Epoch: 58 Loss: 0.1223 valLoss: 0.1309 valAcc: 33.94% AUC: 61.68%
Epoch: 59 Loss: 0.1218 valLoss: 0.1311 valAcc: 35.58% AUC: 61.62%
Epoch: 60 Loss: 0.1213 valLoss: 0.1310 valAcc: 35.76% AUC: 61.64%
Epoch: 61 Loss: 0.1215 valLoss: 0.1313 valAcc: 38.74% AUC: 61.67%
Epoch: 62 Loss: 0.1212 valLoss: 0.1311 valAcc: 38.68% AUC: 61.81%
Epoch: 63 Loss: 0.1208 valLoss: 0.1316 valAcc: 41.28% AUC: 61.69%
Epoch: 64 Loss: 0.1204 valLoss: 0.1321 valAcc: 43.34% AUC: 61.70%
Epoch: 65 Loss: 0.1196 valLoss: 0.1319 valAcc: 43.30% AUC: 61.63%
Epoch: 66 Loss: 0.1197 valLoss: 0.1322 valAcc: 44.41% AUC: 61.51%
Epoch: 67 Loss: 0.1192 valLoss: 0.1322 valAcc: 43.65% AUC: 61.24%
Epoch: 68 Loss: 0.1191 valLoss: 0.1327 valAcc: 46.64% AUC: 61.33%
Epoch: 69 Loss: 0.1185 valLoss: 0.1332 valAcc: 48.22% AUC: 61.23%
Epoch: 70 Loss: 0.1182 valLoss: 0.1343 valAcc: 51.52% AUC: 61.28%
Epoch: 71 Loss: 0.1175 valLoss: 0.1355 valAcc: 53.57% AUC: 60.95%
Epoch: 72 Loss: 0.1182 valLoss: 0.1338 valAcc: 48.79% AUC: 61.04%
Epoch: 73 Loss: 0.1173 valLoss: 0.1338 valAcc: 48.37% AUC: 60.80%
Epoch: 74 Loss: 0.1168 valLoss: 0.1358 valAcc: 53.24% AUC: 60.69%
Epoch: 75 Loss: 0.1167 valLoss: 0.1371 valAcc: 55.58% AUC: 60.56%
Epoch: 76 Loss: 0.1155 valLoss: 0.1364 valAcc: 52.93% AUC: 60.41%
Epoch: 77 Loss: 0.1157 valLoss: 0.1354 valAcc: 51.46% AUC: 60.71%
Epoch: 78 Loss: 0.1153 valLoss: 0.1363 valAcc: 53.66% AUC: 60.69%
Epoch: 79 Loss: 0.1145 valLoss: 0.1360 valAcc: 53.50% AUC: 60.70%
Epoch: 80 Loss: 0.1146 valLoss: 0.1356 valAcc: 52.13% AUC: 60.65%
Epoch: 81 Loss: 0.1137 valLoss: 0.1384 valAcc: 56.36% AUC: 60.18%
Epoch: 82 Loss: 0.1135 valLoss: 0.1379 valAcc: 55.70% AUC: 60.39%
Epoch: 83 Loss: 0.1131 valLoss: 0.1386 valAcc: 55.16% AUC: 60.15%
Epoch: 84 Loss: 0.1117 valLoss: 0.1416 valAcc: 59.58% AUC: 60.10%
Epoch: 85 Loss: 0.1123 valLoss: 0.1388 valAcc: 55.56% AUC: 60.33%
Epoch: 86 Loss: 0.1112 valLoss: 0.1381 valAcc: 54.85% AUC: 60.38%
Epoch: 87 Loss: 0.1107 valLoss: 0.1444 valAcc: 63.07% AUC: 60.26%
Epoch: 88 Loss: 0.1104 valLoss: 0.1406 valAcc: 57.61% AUC: 60.17%
Epoch: 89 Loss: 0.1107 valLoss: 0.1406 valAcc: 57.95% AUC: 60.26%
Epoch: 90 Loss: 0.1083 valLoss: 0.1496 valAcc: 65.78% AUC: 59.91%
Epoch: 91 Loss: 0.1092 valLoss: 0.1420 valAcc: 59.11% AUC: 59.99%
Epoch: 92 Loss: 0.1086 valLoss: 0.1495 valAcc: 65.54% AUC: 59.94%
Epoch: 93 Loss: 0.1081 valLoss: 0.1434 valAcc: 59.82% AUC: 60.16%
Epoch: 94 Loss: 0.1071 valLoss: 0.1506 valAcc: 65.25% AUC: 59.79%
Epoch: 95 Loss: 0.1066 valLoss: 0.1460 valAcc: 60.62% AUC: 59.95%
Epoch: 96 Loss: 0.1062 valLoss: 0.1488 valAcc: 63.47% AUC: 59.90%
Epoch: 97 Loss: 0.1064 valLoss: 0.1490 valAcc: 63.80% AUC: 59.78%
Epoch: 98 Loss: 0.1049 valLoss: 0.1564 valAcc: 68.19% AUC: 59.80%
Epoch: 99 Loss: 0.1054 valLoss: 0.1496 valAcc: 62.54% AUC: 59.51%
Epoch: 100 Loss: 0.1046 valLoss: 0.1527 valAcc: 63.69% AUC: 59.65%
Epoch: 101 Loss: 0.1035 valLoss: 0.1569 valAcc: 66.22% AUC: 59.65%
Epoch: 102 Loss: 0.1027 valLoss: 0.1600 valAcc: 68.18% AUC: 59.59%
Epoch: 103 Loss: 0.1023 valLoss: 0.1593 valAcc: 67.21% AUC: 59.53%
Epoch: 104 Loss: 0.1019 valLoss: 0.1690 valAcc: 71.52% AUC: 59.79%
Epoch: 105 Loss: 0.1011 valLoss: 0.1681 valAcc: 71.09% AUC: 59.83%
Epoch: 106 Loss: 0.1003 valLoss: 0.1669 valAcc: 70.15% AUC: 59.91%
Epoch: 107 Loss: 0.1004 valLoss: 0.1719 valAcc: 72.66% AUC: 59.94%
Epoch: 108 Loss: 0.0990 valLoss: 0.1731 valAcc: 71.78% AUC: 59.69%
Epoch: 109 Loss: 0.0997 valLoss: 0.1695 valAcc: 70.17% AUC: 59.75%
Epoch: 110 Loss: 0.0981 valLoss: 0.1682 valAcc: 69.15% AUC: 59.83%
Epoch: 111 Loss: 0.0976 valLoss: 0.1703 valAcc: 70.24% AUC: 60.19%
Epoch: 112 Loss: 0.0973 valLoss: 0.1649 valAcc: 68.01% AUC: 60.02%
Epoch: 113 Loss: 0.0968 valLoss: 0.1701 valAcc: 69.86% AUC: 59.90%
Restoring model weights from the best epoch: 33 with the best VAL_AUC: 62.37%
Training time: 0:12:55.917553
## Testing ../data/prepared_epitope_testing.csv at: 12-06-2023#14:44:12 ##
Shape featuresTable: (56, 1) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 21)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:00.500023
The best cut-off value is: 0.56
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15022  1103]
 [ 1103   282]]
ValAcc: 87.40% specScore: 93.16% presScore: 20.36% recallScore: 20.36% F1Score: 20.36% MCC: 13.52% AUC: 67.68% AP: 15.40%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-07-2023#23:27:15 @@
## tf-version: 2.12.0|| keras-version: 2.12.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 21)]        0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          9408      
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (BatchN  (None, 1170, 64)         256       
 ormalization)                                                   
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Batc  (None, 1170, 64)         256       
 hNormalization)                                                 
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Batc  (None, 1170, 32)         128       
 hNormalization)                                                 
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDistr  (None, 1170, 1)          33        
 ibuted)                                                         
                                                                 
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________
Shape featuresTable: (343, 1) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 21)
Shape labelData: (343, 1170, 1)

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 27-07-2023#13:40:01 @@
## tf-version: 2.12.0|| keras-version: 2.12.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 21)]        0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          9408      
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (BatchN  (None, 1170, 64)         256       
 ormalization)                                                   
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Batc  (None, 1170, 128)        512       
 hNormalization)                                                 
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Batc  (None, 1170, 64)         256       
 hNormalization)                                                 
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Batc  (None, 1170, 32)         128       
 hNormalization)                                                 
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDistr  (None, 1170, 1)          33        
 ibuted)                                                         
                                                                 
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________
Shape featuresTable: (343, 1) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 21)
Shape labelData: (343, 1170, 1)

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 28-07-2023#12:02:31 @@
## tf-version: 2.4.1|| keras-version: 2.4.0|| float_type: float64
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 1170, 21)]        0         
_________________________________________________________________
conv1d (Conv1D)              (None, 1170, 64)          9408      
_________________________________________________________________
dropout (Dropout)            (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization (BatchNo (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu (PReLU)              (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 1170, 128)         57344     
_________________________________________________________________
dropout_1 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_1 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_1 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_2 (Conv1D)            (None, 1170, 128)         114688    
_________________________________________________________________
dropout_2 (Dropout)          (None, 1170, 128)         0         
_________________________________________________________________
batch_normalization_2 (Batch (None, 1170, 128)         512       
_________________________________________________________________
p_re_lu_2 (PReLU)            (None, 1170, 128)         149760    
_________________________________________________________________
conv1d_3 (Conv1D)            (None, 1170, 64)          57344     
_________________________________________________________________
dropout_3 (Dropout)          (None, 1170, 64)          0         
_________________________________________________________________
batch_normalization_3 (Batch (None, 1170, 64)          256       
_________________________________________________________________
p_re_lu_3 (PReLU)            (None, 1170, 64)          74880     
_________________________________________________________________
conv1d_4 (Conv1D)            (None, 1170, 32)          14336     
_________________________________________________________________
dropout_4 (Dropout)          (None, 1170, 32)          0         
_________________________________________________________________
batch_normalization_4 (Batch (None, 1170, 32)          128       
_________________________________________________________________
p_re_lu_4 (PReLU)            (None, 1170, 32)          37440     
_________________________________________________________________
time_distributed (TimeDistri (None, 1170, 1)           33        
=================================================================
Total params: 741,537
Trainable params: 740,705
Non-trainable params: 832
_________________________________________________________________
Shape featuresTable: (343, 1) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 21)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1756 valLoss: 0.1336 valAcc: 11.32% AUC: 51.78%
Epoch: 01 Loss: 0.1537 valLoss: 0.1333 valAcc: 11.32% AUC: 51.92%
Epoch: 02 Loss: 0.1436 valLoss: 0.1337 valAcc: 11.32% AUC: 50.71%
Epoch: 03 Loss: 0.1392 valLoss: 0.1339 valAcc: 11.32% AUC: 50.68%
Epoch: 04 Loss: 0.1372 valLoss: 0.1337 valAcc: 11.34% AUC: 51.63%
Epoch: 05 Loss: 0.1366 valLoss: 0.1333 valAcc: 11.35% AUC: 53.22%
Epoch: 06 Loss: 0.1359 valLoss: 0.1327 valAcc: 11.36% AUC: 55.11%
Epoch: 07 Loss: 0.1351 valLoss: 0.1321 valAcc: 11.36% AUC: 56.73%
Epoch: 08 Loss: 0.1345 valLoss: 0.1318 valAcc: 11.39% AUC: 57.87%
Epoch: 09 Loss: 0.1339 valLoss: 0.1315 valAcc: 11.38% AUC: 58.87%
Epoch: 10 Loss: 0.1336 valLoss: 0.1314 valAcc: 11.39% AUC: 59.50%
Epoch: 11 Loss: 0.1333 valLoss: 0.1312 valAcc: 11.38% AUC: 60.24%
Epoch: 12 Loss: 0.1327 valLoss: 0.1311 valAcc: 11.35% AUC: 60.62%
Epoch: 13 Loss: 0.1324 valLoss: 0.1308 valAcc: 11.33% AUC: 61.45%
Epoch: 14 Loss: 0.1319 valLoss: 0.1306 valAcc: 11.32% AUC: 61.92%
Epoch: 15 Loss: 0.1318 valLoss: 0.1305 valAcc: 11.32% AUC: 61.97%
Epoch: 16 Loss: 0.1317 valLoss: 0.1303 valAcc: 11.32% AUC: 62.20%
Epoch: 17 Loss: 0.1312 valLoss: 0.1302 valAcc: 11.32% AUC: 62.45%
Epoch: 18 Loss: 0.1309 valLoss: 0.1301 valAcc: 11.33% AUC: 62.57%
Epoch: 19 Loss: 0.1306 valLoss: 0.1299 valAcc: 11.37% AUC: 62.72%
Epoch: 20 Loss: 0.1305 valLoss: 0.1298 valAcc: 11.46% AUC: 62.78%
Epoch: 21 Loss: 0.1303 valLoss: 0.1297 valAcc: 11.50% AUC: 62.90%
Epoch: 22 Loss: 0.1297 valLoss: 0.1296 valAcc: 11.66% AUC: 62.83%
Epoch: 23 Loss: 0.1298 valLoss: 0.1295 valAcc: 11.74% AUC: 63.04%
Epoch: 24 Loss: 0.1296 valLoss: 0.1295 valAcc: 11.84% AUC: 62.95%
Epoch: 25 Loss: 0.1293 valLoss: 0.1295 valAcc: 11.98% AUC: 63.00%
Epoch: 26 Loss: 0.1292 valLoss: 0.1294 valAcc: 12.09% AUC: 63.21%
Epoch: 27 Loss: 0.1292 valLoss: 0.1293 valAcc: 12.20% AUC: 63.12%
Epoch: 28 Loss: 0.1291 valLoss: 0.1293 valAcc: 12.37% AUC: 63.12%
Epoch: 29 Loss: 0.1284 valLoss: 0.1293 valAcc: 12.69% AUC: 62.99%
Epoch: 30 Loss: 0.1283 valLoss: 0.1294 valAcc: 12.80% AUC: 62.85%
Epoch: 31 Loss: 0.1281 valLoss: 0.1294 valAcc: 13.18% AUC: 62.76%
Epoch: 32 Loss: 0.1281 valLoss: 0.1293 valAcc: 13.40% AUC: 62.82%
Epoch: 33 Loss: 0.1277 valLoss: 0.1292 valAcc: 13.03% AUC: 62.80%
Epoch: 34 Loss: 0.1276 valLoss: 0.1293 valAcc: 13.79% AUC: 62.61%
Epoch: 35 Loss: 0.1277 valLoss: 0.1293 valAcc: 14.36% AUC: 62.62%
Epoch: 36 Loss: 0.1270 valLoss: 0.1294 valAcc: 14.97% AUC: 62.49%
Epoch: 37 Loss: 0.1268 valLoss: 0.1292 valAcc: 14.72% AUC: 62.81%
Epoch: 38 Loss: 0.1265 valLoss: 0.1290 valAcc: 14.70% AUC: 62.99%
Epoch: 39 Loss: 0.1266 valLoss: 0.1292 valAcc: 15.83% AUC: 62.60%
Epoch: 40 Loss: 0.1263 valLoss: 0.1292 valAcc: 16.65% AUC: 62.50%
Epoch: 41 Loss: 0.1262 valLoss: 0.1294 valAcc: 18.65% AUC: 62.03%
Epoch: 42 Loss: 0.1260 valLoss: 0.1292 valAcc: 17.94% AUC: 62.26%
Epoch: 43 Loss: 0.1258 valLoss: 0.1294 valAcc: 17.72% AUC: 61.89%
Epoch: 44 Loss: 0.1255 valLoss: 0.1294 valAcc: 19.88% AUC: 61.83%
Epoch: 45 Loss: 0.1251 valLoss: 0.1295 valAcc: 21.48% AUC: 61.56%
Epoch: 46 Loss: 0.1249 valLoss: 0.1295 valAcc: 22.73% AUC: 61.88%
Epoch: 47 Loss: 0.1247 valLoss: 0.1294 valAcc: 21.59% AUC: 61.57%
Epoch: 48 Loss: 0.1244 valLoss: 0.1297 valAcc: 25.01% AUC: 61.43%
Epoch: 49 Loss: 0.1246 valLoss: 0.1299 valAcc: 26.54% AUC: 61.37%
Epoch: 50 Loss: 0.1239 valLoss: 0.1300 valAcc: 28.20% AUC: 61.38%
Epoch: 51 Loss: 0.1239 valLoss: 0.1301 valAcc: 26.74% AUC: 61.07%
Epoch: 52 Loss: 0.1241 valLoss: 0.1303 valAcc: 29.22% AUC: 60.99%
Epoch: 53 Loss: 0.1230 valLoss: 0.1309 valAcc: 32.82% AUC: 60.72%
Epoch: 54 Loss: 0.1227 valLoss: 0.1315 valAcc: 36.60% AUC: 60.76%
Epoch: 55 Loss: 0.1226 valLoss: 0.1307 valAcc: 31.04% AUC: 60.59%
Epoch: 56 Loss: 0.1223 valLoss: 0.1306 valAcc: 31.26% AUC: 60.54%
Epoch: 57 Loss: 0.1224 valLoss: 0.1314 valAcc: 36.10% AUC: 60.59%
Epoch: 58 Loss: 0.1221 valLoss: 0.1311 valAcc: 33.72% AUC: 60.25%
Epoch: 59 Loss: 0.1214 valLoss: 0.1318 valAcc: 38.25% AUC: 60.36%
Epoch: 60 Loss: 0.1210 valLoss: 0.1324 valAcc: 40.91% AUC: 60.28%
Epoch: 61 Loss: 0.1205 valLoss: 0.1339 valAcc: 44.50% AUC: 59.81%
Epoch: 62 Loss: 0.1208 valLoss: 0.1336 valAcc: 43.98% AUC: 59.83%
Epoch: 63 Loss: 0.1202 valLoss: 0.1343 valAcc: 45.65% AUC: 59.74%
Epoch: 64 Loss: 0.1195 valLoss: 0.1351 valAcc: 47.74% AUC: 59.67%
Epoch: 65 Loss: 0.1199 valLoss: 0.1336 valAcc: 44.07% AUC: 59.80%
Epoch: 66 Loss: 0.1188 valLoss: 0.1365 valAcc: 50.92% AUC: 59.43%
Epoch: 67 Loss: 0.1189 valLoss: 0.1359 valAcc: 49.73% AUC: 59.61%
Epoch: 68 Loss: 0.1177 valLoss: 0.1379 valAcc: 52.97% AUC: 59.19%
Epoch: 69 Loss: 0.1177 valLoss: 0.1363 valAcc: 50.52% AUC: 59.65%
Epoch: 70 Loss: 0.1175 valLoss: 0.1394 valAcc: 55.33% AUC: 59.27%
Epoch: 71 Loss: 0.1171 valLoss: 0.1402 valAcc: 57.28% AUC: 59.48%
Epoch: 72 Loss: 0.1167 valLoss: 0.1386 valAcc: 53.72% AUC: 59.24%
Epoch: 73 Loss: 0.1163 valLoss: 0.1419 valAcc: 57.99% AUC: 59.00%
Epoch: 74 Loss: 0.1159 valLoss: 0.1445 valAcc: 60.87% AUC: 58.90%
Epoch: 75 Loss: 0.1152 valLoss: 0.1422 valAcc: 58.77% AUC: 59.07%
Epoch: 76 Loss: 0.1147 valLoss: 0.1428 valAcc: 59.37% AUC: 59.17%
Epoch: 77 Loss: 0.1144 valLoss: 0.1476 valAcc: 63.74% AUC: 58.69%
Epoch: 78 Loss: 0.1136 valLoss: 0.1513 valAcc: 65.89% AUC: 58.58%
Epoch: 79 Loss: 0.1128 valLoss: 0.1465 valAcc: 62.06% AUC: 58.95%
Epoch: 80 Loss: 0.1129 valLoss: 0.1538 valAcc: 66.89% AUC: 58.36%
Epoch: 81 Loss: 0.1120 valLoss: 0.1600 valAcc: 69.75% AUC: 58.29%
Epoch: 82 Loss: 0.1121 valLoss: 0.1524 valAcc: 65.56% AUC: 58.59%
Epoch: 83 Loss: 0.1116 valLoss: 0.1539 valAcc: 65.22% AUC: 58.41%
Epoch: 84 Loss: 0.1107 valLoss: 0.1565 valAcc: 66.10% AUC: 58.21%
Epoch: 85 Loss: 0.1106 valLoss: 0.1598 valAcc: 68.64% AUC: 58.25%
Epoch: 86 Loss: 0.1094 valLoss: 0.1640 valAcc: 69.11% AUC: 58.02%
Epoch: 87 Loss: 0.1094 valLoss: 0.1699 valAcc: 72.18% AUC: 58.34%
Epoch: 88 Loss: 0.1084 valLoss: 0.1689 valAcc: 72.15% AUC: 58.47%
Epoch: 89 Loss: 0.1075 valLoss: 0.1872 valAcc: 76.67% AUC: 58.25%
Epoch: 90 Loss: 0.1076 valLoss: 0.1694 valAcc: 71.27% AUC: 58.34%
Epoch: 91 Loss: 0.1060 valLoss: 0.1952 valAcc: 77.63% AUC: 58.05%
Epoch: 92 Loss: 0.1055 valLoss: 0.1758 valAcc: 72.84% AUC: 58.01%
Epoch: 93 Loss: 0.1058 valLoss: 0.1792 valAcc: 72.92% AUC: 57.84%
Epoch: 94 Loss: 0.1052 valLoss: 0.1867 valAcc: 74.10% AUC: 57.57%
Epoch: 95 Loss: 0.1043 valLoss: 0.1878 valAcc: 74.97% AUC: 58.04%
Epoch: 96 Loss: 0.1036 valLoss: 0.1936 valAcc: 75.91% AUC: 58.09%
Epoch: 97 Loss: 0.1025 valLoss: 0.2150 valAcc: 79.66% AUC: 58.16%
Epoch: 98 Loss: 0.1025 valLoss: 0.2163 valAcc: 79.79% AUC: 58.02%
Epoch: 99 Loss: 0.1016 valLoss: 0.2008 valAcc: 76.44% AUC: 57.95%
Epoch: 100 Loss: 0.1009 valLoss: 0.2162 valAcc: 78.61% AUC: 58.12%
Epoch: 101 Loss: 0.1010 valLoss: 0.1997 valAcc: 75.36% AUC: 57.72%
Epoch: 102 Loss: 0.1003 valLoss: 0.2133 valAcc: 77.92% AUC: 58.07%
Epoch: 103 Loss: 0.0984 valLoss: 0.2293 valAcc: 80.09% AUC: 58.18%
Epoch: 104 Loss: 0.0988 valLoss: 0.2121 valAcc: 76.98% AUC: 58.00%
Epoch: 105 Loss: 0.0978 valLoss: 0.2421 valAcc: 81.36% AUC: 57.89%
Epoch: 106 Loss: 0.0986 valLoss: 0.2089 valAcc: 76.44% AUC: 57.85%
Restoring model weights from the best epoch: 26 with the best VAL_AUC: 63.21%
Training time: 0:03:55.983727
## Testing ../data/prepared_epitope_testing.csv at: 28-07-2023#12:06:33 ##
Shape featuresTable: (56, 1) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 21)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:00.778001
The best cut-off value is: 0.60
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[14989  1136]
 [ 1136   249]]
ValAcc: 87.02% specScore: 92.96% presScore: 17.98% recallScore: 17.98% F1Score: 17.98% MCC: 10.93% AUC: 67.56% AP: 14.66%
## Testing ./prepared_userds.csv at: 22-09-2023#13:52:57 ##
Shape featuresTable: (4999, 2) | Shape labelvec: (4999, 1)
Shape inputData: (4999, 1170, 1025)
Shape labelData: (4999, 1170, 1)
## Testing ./prepared_userds.csv at: 23-09-2023#12:12:02 ##
Shape featuresTable: (4, 2) | Shape labelvec: (4, 1)
## Testing ./prepared_userds.csv at: 23-09-2023#12:30:52 ##
Shape featuresTable: (4, 2) | Shape labelvec: (4, 1)
## Testing ./prepared_userds.csv at: 23-09-2023#12:32:32 ##
Shape featuresTable: (4, 2) | Shape labelvec: (4, 1)
Shape inputData: (4, 1170, 1025)
Shape labelData: (4, 1170, 1)
Testing time: 0:00:01.118296
## Testing ./prepared_userds.csv at: 23-09-2023#12:35:57 ##
## Testing ./prepared_userds.csv at: 23-09-2023#13:05:18 ##
## Testing ./prepared_userds.csv at: 23-09-2023#14:13:36 ##
Shape featuresTable: (499, 2) | Shape labelvec: (499, 1)
Shape inputData: (499, 1170, 1025)
Shape labelData: (499, 1170, 1)
Testing time: 0:00:23.062395
## Testing ./prepared_userds.csv at: 23-09-2023#15:43:43 ##
Shape featuresTable: (499, 2) | Shape labelvec: (499, 1)
Shape inputData: (499, 1170, 1025)
Shape labelData: (499, 1170, 1)
Testing time: 0:00:11.879525
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:06 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:13 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:21 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:29 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:36 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:44 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:51 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:31:59 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:32:07 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:32:14 ##
## Testing ./prepared_userds.csv at: 23-09-2023#16:32:22 ##
Shape featuresTable: (499, 2) | Shape labelvec: (499, 1)
## Testing ./prepared_userds.csv at: 23-09-2023#16:53:56 ##
Shape featuresTable: (499, 2) | Shape labelvec: (499, 1)
Shape inputData: (499, 1170, 1025)
Shape labelData: (499, 1170, 1)
Testing time: 0:00:11.834864
## Testing ./prepared_userds.csv at: 23-09-2023#17:21:41 ##
## Testing ./prepared_userds.csv at: 23-09-2023#17:37:54 ##
## Testing ./prepared_userds.csv at: 23-09-2023#18:00:00 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.896171
## Testing ./prepared_userds.csv at: 23-09-2023#18:04:25 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.846818
## Testing ./prepared_userds.csv at: 23-09-2023#18:08:48 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.856660
## Testing ./prepared_userds.csv at: 23-09-2023#18:13:11 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.896089
## Testing ./prepared_userds.csv at: 23-09-2023#18:17:34 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.845825
## Testing ./prepared_userds.csv at: 23-09-2023#18:21:59 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.840719
## Testing ./prepared_userds.csv at: 23-09-2023#18:26:24 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.801924
## Testing ./prepared_userds.csv at: 23-09-2023#18:30:48 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.846148
## Testing ./prepared_userds.csv at: 23-09-2023#18:35:11 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.884767
## Testing ./prepared_userds.csv at: 23-09-2023#18:39:37 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.881059
## Testing ./prepared_userds.csv at: 23-09-2023#18:44:01 ##
Shape featuresTable: (499, 2) | Shape labelvec: (499, 1)
Shape inputData: (499, 1170, 1025)
Shape labelData: (499, 1170, 1)
Testing time: 0:00:11.812311
## Testing ./prepared_userds.csv at: 23-09-2023#18:48:23 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.884838
## Testing ./prepared_userds.csv at: 23-09-2023#18:52:47 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.854218
## Testing ./prepared_userds.csv at: 23-09-2023#18:57:12 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
## Testing ./prepared_userds.csv at: 24-09-2023#14:43:46 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.847483
## Testing ./prepared_userds.csv at: 24-09-2023#14:48:09 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.881002
## Testing ./prepared_userds.csv at: 24-09-2023#14:52:32 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.827743
## Testing ./prepared_userds.csv at: 24-09-2023#14:56:55 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.824907
## Testing ./prepared_userds.csv at: 24-09-2023#15:01:19 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.894334
## Testing ./prepared_userds.csv at: 24-09-2023#15:05:43 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.897016
## Testing ./prepared_userds.csv at: 24-09-2023#15:10:07 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.862019
## Testing ./prepared_userds.csv at: 24-09-2023#15:14:31 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.905013
## Testing ./prepared_userds.csv at: 24-09-2023#15:18:55 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.819283
## Testing ./prepared_userds.csv at: 24-09-2023#15:23:19 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.812805
## Testing ./prepared_userds.csv at: 24-09-2023#15:27:42 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.881177
## Testing ./prepared_userds.csv at: 24-09-2023#15:32:06 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.841289
## Testing ./prepared_userds.csv at: 24-09-2023#15:36:30 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.850068
## Testing ./prepared_userds.csv at: 24-09-2023#15:40:53 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.940228
## Testing ./prepared_userds.csv at: 24-09-2023#15:45:19 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.878815
## Testing ./prepared_userds.csv at: 24-09-2023#15:49:44 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.825488
## Testing ./prepared_userds.csv at: 24-09-2023#15:54:10 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.780223
## Testing ./prepared_userds.csv at: 24-09-2023#15:58:35 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.950294
## Testing ./prepared_userds.csv at: 24-09-2023#16:03:03 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.819055
## Testing ./prepared_userds.csv at: 24-09-2023#16:07:27 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.896635
## Testing ./prepared_userds.csv at: 24-09-2023#16:11:51 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.833987
## Testing ./prepared_userds.csv at: 24-09-2023#16:16:15 ##
Shape featuresTable: (67, 2) | Shape labelvec: (67, 1)
Shape inputData: (67, 1170, 1025)
Shape labelData: (67, 1170, 1)
Testing time: 0:00:07.445549
## Testing ./prepared_userds.csv at: 24-09-2023#16:16:45 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.815364
## Testing ./prepared_userds.csv at: 24-09-2023#16:21:09 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.890406
## Testing ./prepared_userds.csv at: 24-09-2023#16:25:37 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.839481
## Testing ./prepared_userds.csv at: 24-09-2023#16:30:02 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:11.870414
## Testing ./prepared_userds.csv at: 24-09-2023#16:34:29 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:12.004875
## Testing ./prepared_userds.csv at: 24-09-2023#16:38:53 ##
Shape featuresTable: (500, 2) | Shape labelvec: (500, 1)
Shape inputData: (500, 1170, 1025)
Shape labelData: (500, 1170, 1)
Testing time: 0:00:12.034942
## Testing ./prepared_userds.csv at: 28-09-2023#09:09:05 ##
Shape featuresTable: (97, 2) | Shape labelvec: (97, 1)
Shape inputData: (104, 1170, 1025)
Shape labelData: (104, 1170, 1)
Testing time: 0:00:02.722158
## Testing ./prepared_userds.csv at: 05-10-2023#10:38:27 ##
Shape featuresTable: (49, 2) | Shape labelvec: (49, 1)
Shape inputData: (51, 1170, 1025)
Shape labelData: (51, 1170, 1)
Testing time: 0:00:01.390921
## Testing ./prepared_userds.csv at: 12-10-2023#11:01:05 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.186122
## Testing ./prepared_userds.csv at: 27-10-2023#17:31:33 ##
Shape featuresTable: (896, 2) | Shape labelvec: (896, 1)
Shape inputData: (910, 1170, 1025)
Shape labelData: (910, 1170, 1)
Testing time: 0:00:09.710770
## Testing ./prepared_userds.csv at: 03-11-2023#17:00:08 ##
Shape featuresTable: (423, 2) | Shape labelvec: (423, 1)
Shape inputData: (423, 1170, 1025)
Shape labelData: (423, 1170, 1)
Testing time: 0:00:05.070156
## Testing ./prepared_userds.csv at: 03-11-2023#17:06:22 ##
Shape featuresTable: (543, 2) | Shape labelvec: (543, 1)
Shape inputData: (565, 1170, 1025)
Shape labelData: (565, 1170, 1)
Testing time: 0:00:07.729541

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 14-12-2023#15:16:19 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 14-12-2023#15:20:33 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 14-12-2023#20:25:17 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1700 valLoss: 0.1342 valAcc: 11.32% AUC: 50.36%
Epoch: 01 Loss: 0.1469 valLoss: 0.1354 valAcc: 13.15% AUC: 53.33%
Epoch: 02 Loss: 0.1391 valLoss: 0.1359 valAcc: 15.64% AUC: 52.18%
Epoch: 03 Loss: 0.1348 valLoss: 0.1345 valAcc: 14.01% AUC: 53.73%
Epoch: 04 Loss: 0.1329 valLoss: 0.1333 valAcc: 13.45% AUC: 55.86%
Epoch: 05 Loss: 0.1306 valLoss: 0.1321 valAcc: 12.74% AUC: 59.52%
Epoch: 06 Loss: 0.1287 valLoss: 0.1312 valAcc: 14.16% AUC: 61.61%
Epoch: 07 Loss: 0.1261 valLoss: 0.1307 valAcc: 22.79% AUC: 63.26%
Epoch: 08 Loss: 0.1238 valLoss: 0.1296 valAcc: 32.45% AUC: 65.11%
Epoch: 09 Loss: 0.1219 valLoss: 0.1283 valAcc: 35.77% AUC: 66.09%
Epoch: 10 Loss: 0.1196 valLoss: 0.1277 valAcc: 43.57% AUC: 66.95%
Epoch: 11 Loss: 0.1166 valLoss: 0.1301 valAcc: 52.21% AUC: 67.38%
Epoch: 12 Loss: 0.1157 valLoss: 0.1287 valAcc: 48.92% AUC: 67.44%
Epoch: 13 Loss: 0.1117 valLoss: 0.1351 valAcc: 60.44% AUC: 67.75%
Epoch: 14 Loss: 0.1091 valLoss: 0.1405 valAcc: 62.79% AUC: 67.35%
Epoch: 15 Loss: 0.1061 valLoss: 0.1500 valAcc: 67.12% AUC: 67.74%
Epoch: 16 Loss: 0.1024 valLoss: 0.1593 valAcc: 68.86% AUC: 67.65%
Epoch: 17 Loss: 0.0990 valLoss: 0.1785 valAcc: 73.14% AUC: 67.27%
Epoch: 18 Loss: 0.0958 valLoss: 0.1973 valAcc: 74.30% AUC: 67.05%
Epoch: 19 Loss: 0.0927 valLoss: 0.2311 valAcc: 78.44% AUC: 66.53%
Epoch: 20 Loss: 0.0889 valLoss: 0.2126 valAcc: 73.62% AUC: 66.89%
Epoch: 21 Loss: 0.0859 valLoss: 0.2622 valAcc: 80.22% AUC: 66.92%
Epoch: 22 Loss: 0.0834 valLoss: 0.2827 valAcc: 80.73% AUC: 66.44%
Epoch: 23 Loss: 0.0803 valLoss: 0.3072 valAcc: 81.82% AUC: 66.72%
Epoch: 24 Loss: 0.0781 valLoss: 0.3358 valAcc: 82.28% AUC: 66.23%
Epoch: 25 Loss: 0.0768 valLoss: 0.2422 valAcc: 72.09% AUC: 66.00%
Epoch: 26 Loss: 0.0738 valLoss: 0.2991 valAcc: 79.24% AUC: 66.36%
Epoch: 27 Loss: 0.0719 valLoss: 0.3007 valAcc: 78.11% AUC: 66.12%
Epoch: 28 Loss: 0.0699 valLoss: 0.3196 valAcc: 79.65% AUC: 66.49%
Epoch: 29 Loss: 0.0661 valLoss: 0.3570 valAcc: 81.45% AUC: 66.32%
Epoch: 30 Loss: 0.0644 valLoss: 0.2961 valAcc: 77.28% AUC: 66.09%
Epoch: 31 Loss: 0.0615 valLoss: 0.3541 valAcc: 80.64% AUC: 66.37%
Epoch: 32 Loss: 0.0603 valLoss: 0.3495 valAcc: 80.31% AUC: 66.78%
Epoch: 33 Loss: 0.0583 valLoss: 0.3433 valAcc: 80.15% AUC: 66.40%
Epoch: 34 Loss: 0.0562 valLoss: 0.3511 valAcc: 79.61% AUC: 66.79%
Epoch: 35 Loss: 0.0543 valLoss: 0.3576 valAcc: 80.79% AUC: 66.84%
Epoch: 36 Loss: 0.0530 valLoss: 0.3434 valAcc: 79.24% AUC: 66.99%
Epoch: 37 Loss: 0.0505 valLoss: 0.3830 valAcc: 81.48% AUC: 67.18%
Epoch: 38 Loss: 0.0492 valLoss: 0.4208 valAcc: 83.33% AUC: 66.96%
Epoch: 39 Loss: 0.0473 valLoss: 0.4274 valAcc: 83.17% AUC: 67.17%
Epoch: 40 Loss: 0.0463 valLoss: 0.4076 valAcc: 82.54% AUC: 67.05%
Epoch: 41 Loss: 0.0441 valLoss: 0.4231 valAcc: 83.44% AUC: 67.07%
Epoch: 42 Loss: 0.0434 valLoss: 0.4454 valAcc: 84.04% AUC: 66.83%
Epoch: 43 Loss: 0.0415 valLoss: 0.4475 valAcc: 84.16% AUC: 67.13%
Epoch: 44 Loss: 0.0411 valLoss: 0.4676 valAcc: 83.94% AUC: 66.64%
Epoch: 45 Loss: 0.0399 valLoss: 0.4641 valAcc: 84.32% AUC: 66.82%
Epoch: 46 Loss: 0.0379 valLoss: 0.4581 valAcc: 83.92% AUC: 66.79%
Epoch: 47 Loss: 0.0365 valLoss: 0.4855 valAcc: 83.83% AUC: 66.35%
Epoch: 48 Loss: 0.0350 valLoss: 0.5053 valAcc: 84.77% AUC: 66.71%
Epoch: 49 Loss: 0.0359 valLoss: 0.4565 valAcc: 83.57% AUC: 66.78%
Epoch: 50 Loss: 0.0350 valLoss: 0.4800 valAcc: 84.04% AUC: 66.47%
Epoch: 51 Loss: 0.0337 valLoss: 0.4756 valAcc: 83.64% AUC: 66.46%
Epoch: 52 Loss: 0.0325 valLoss: 0.4485 valAcc: 82.89% AUC: 66.78%
Epoch: 53 Loss: 0.0308 valLoss: 0.5055 valAcc: 84.14% AUC: 66.18%
Epoch: 54 Loss: 0.0305 valLoss: 0.4668 valAcc: 82.94% AUC: 66.34%
Epoch: 55 Loss: 0.0288 valLoss: 0.5028 valAcc: 84.10% AUC: 66.52%
Epoch: 56 Loss: 0.0281 valLoss: 0.5007 valAcc: 83.58% AUC: 66.54%
Epoch: 57 Loss: 0.0277 valLoss: 0.4705 valAcc: 83.11% AUC: 66.57%
Epoch: 58 Loss: 0.0266 valLoss: 0.4764 valAcc: 83.20% AUC: 66.09%
Epoch: 59 Loss: 0.0263 valLoss: 0.4701 valAcc: 82.63% AUC: 66.33%
Epoch: 60 Loss: 0.0250 valLoss: 0.4921 valAcc: 83.16% AUC: 66.20%
Epoch: 61 Loss: 0.0253 valLoss: 0.5303 valAcc: 83.94% AUC: 65.95%
Epoch: 62 Loss: 0.0241 valLoss: 0.5166 valAcc: 83.73% AUC: 66.04%
Epoch: 63 Loss: 0.0231 valLoss: 0.5037 valAcc: 83.00% AUC: 65.48%
Epoch: 64 Loss: 0.0224 valLoss: 0.5187 valAcc: 83.51% AUC: 65.64%
Epoch: 65 Loss: 0.0217 valLoss: 0.5207 valAcc: 83.49% AUC: 66.10%
Epoch: 66 Loss: 0.0207 valLoss: 0.5264 valAcc: 83.52% AUC: 65.97%
Epoch: 67 Loss: 0.0221 valLoss: 0.5093 valAcc: 83.55% AUC: 65.83%
Epoch: 68 Loss: 0.0202 valLoss: 0.5083 valAcc: 83.28% AUC: 66.23%
Epoch: 69 Loss: 0.0203 valLoss: 0.5560 valAcc: 84.71% AUC: 66.17%
Epoch: 70 Loss: 0.0195 valLoss: 0.5359 valAcc: 83.85% AUC: 66.04%
Epoch: 71 Loss: 0.0195 valLoss: 0.5382 valAcc: 84.34% AUC: 66.61%
Epoch: 72 Loss: 0.0190 valLoss: 0.5726 valAcc: 84.76% AUC: 65.94%
Epoch: 73 Loss: 0.0188 valLoss: 0.5474 valAcc: 83.84% AUC: 66.02%
Epoch: 74 Loss: 0.0180 valLoss: 0.5754 valAcc: 84.61% AUC: 66.05%
Epoch: 75 Loss: 0.0185 valLoss: 0.5418 valAcc: 83.49% AUC: 66.06%
Epoch: 76 Loss: 0.0167 valLoss: 0.5801 valAcc: 84.65% AUC: 65.94%
Epoch: 77 Loss: 0.0163 valLoss: 0.6262 valAcc: 85.49% AUC: 66.07%
Epoch: 78 Loss: 0.0170 valLoss: 0.5880 valAcc: 84.63% AUC: 66.08%
Epoch: 79 Loss: 0.0158 valLoss: 0.5849 valAcc: 84.24% AUC: 65.93%
Epoch: 80 Loss: 0.0159 valLoss: 0.5889 valAcc: 84.00% AUC: 65.77%
Epoch: 81 Loss: 0.0159 valLoss: 0.5824 valAcc: 84.22% AUC: 65.65%
Epoch: 82 Loss: 0.0141 valLoss: 0.5854 valAcc: 84.00% AUC: 65.70%
Epoch: 83 Loss: 0.0139 valLoss: 0.5853 valAcc: 84.06% AUC: 65.73%
Epoch: 84 Loss: 0.0135 valLoss: 0.5595 valAcc: 83.33% AUC: 66.23%
Epoch: 85 Loss: 0.0135 valLoss: 0.6179 valAcc: 84.70% AUC: 66.07%
Epoch: 86 Loss: 0.0125 valLoss: 0.6022 valAcc: 84.03% AUC: 65.60%
Epoch: 87 Loss: 0.0132 valLoss: 0.5760 valAcc: 83.70% AUC: 66.15%
Epoch: 88 Loss: 0.0126 valLoss: 0.5845 valAcc: 83.76% AUC: 65.79%
Epoch: 89 Loss: 0.0121 valLoss: 0.5952 valAcc: 83.85% AUC: 65.95%
Epoch: 90 Loss: 0.0129 valLoss: 0.5643 valAcc: 82.33% AUC: 65.43%
Epoch: 91 Loss: 0.0122 valLoss: 0.5841 valAcc: 82.99% AUC: 65.53%
Epoch: 92 Loss: 0.0123 valLoss: 0.6578 valAcc: 84.76% AUC: 64.64%
Epoch: 93 Loss: 0.0113 valLoss: 0.6123 valAcc: 84.13% AUC: 65.71%
Restoring model weights from the best epoch: 13 with the best VAL_AUC: 67.75%
Training time: 0:28:11.185565
## Testing ../data/prepared_epitope_testing.csv at: 14-12-2023#20:54:51 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.211495
The best cut-off value is: 0.57
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15061  1064]
 [ 1064   321]]
ValAcc: 87.85% specScore: 93.40% presScore: 23.18% recallScore: 23.18% F1Score: 23.18% MCC: 16.58% AUC: 73.74% AP: 19.81%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 19-12-2023#13:39:17 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1714 valLoss: 0.1337 valAcc: 11.32% AUC: 52.88%
Epoch: 01 Loss: 0.1459 valLoss: 0.1351 valAcc: 12.82% AUC: 54.57%
Epoch: 02 Loss: 0.1388 valLoss: 0.1359 valAcc: 22.81% AUC: 55.86%
Epoch: 03 Loss: 0.1354 valLoss: 0.1366 valAcc: 30.92% AUC: 56.03%
Epoch: 04 Loss: 0.1325 valLoss: 0.1342 valAcc: 27.79% AUC: 59.01%
Epoch: 05 Loss: 0.1297 valLoss: 0.1323 valAcc: 35.14% AUC: 62.08%
Epoch: 06 Loss: 0.1272 valLoss: 0.1304 valAcc: 40.99% AUC: 64.34%
Epoch: 07 Loss: 0.1252 valLoss: 0.1305 valAcc: 48.65% AUC: 64.49%
Epoch: 08 Loss: 0.1228 valLoss: 0.1288 valAcc: 48.41% AUC: 65.80%
Epoch: 09 Loss: 0.1214 valLoss: 0.1303 valAcc: 56.79% AUC: 66.21%
Epoch: 10 Loss: 0.1193 valLoss: 0.1284 valAcc: 53.96% AUC: 67.03%
Epoch: 11 Loss: 0.1168 valLoss: 0.1294 valAcc: 57.81% AUC: 67.48%
Epoch: 12 Loss: 0.1141 valLoss: 0.1326 valAcc: 63.39% AUC: 67.93%
Epoch: 13 Loss: 0.1119 valLoss: 0.1323 valAcc: 61.69% AUC: 67.81%
Epoch: 14 Loss: 0.1099 valLoss: 0.1356 valAcc: 63.30% AUC: 68.11%
Epoch: 15 Loss: 0.1078 valLoss: 0.1457 valAcc: 69.07% AUC: 68.05%
Epoch: 16 Loss: 0.1037 valLoss: 0.1501 valAcc: 68.31% AUC: 68.20%
Epoch: 17 Loss: 0.1013 valLoss: 0.1511 valAcc: 66.29% AUC: 67.72%
Epoch: 18 Loss: 0.0984 valLoss: 0.1641 valAcc: 69.07% AUC: 67.77%
Epoch: 19 Loss: 0.0959 valLoss: 0.1758 valAcc: 71.91% AUC: 67.54%
Epoch: 20 Loss: 0.0919 valLoss: 0.1957 valAcc: 75.13% AUC: 67.88%
Epoch: 21 Loss: 0.0896 valLoss: 0.2029 valAcc: 76.62% AUC: 67.74%
Epoch: 22 Loss: 0.0859 valLoss: 0.2047 valAcc: 75.69% AUC: 67.96%
Epoch: 23 Loss: 0.0839 valLoss: 0.2336 valAcc: 78.62% AUC: 67.29%
Epoch: 24 Loss: 0.0806 valLoss: 0.2406 valAcc: 78.05% AUC: 67.33%
Epoch: 25 Loss: 0.0784 valLoss: 0.2460 valAcc: 79.15% AUC: 67.61%
Epoch: 26 Loss: 0.0765 valLoss: 0.2532 valAcc: 77.98% AUC: 67.19%
Epoch: 27 Loss: 0.0741 valLoss: 0.2699 valAcc: 79.76% AUC: 67.60%
Epoch: 28 Loss: 0.0705 valLoss: 0.2697 valAcc: 78.81% AUC: 67.38%
Epoch: 29 Loss: 0.0687 valLoss: 0.2836 valAcc: 79.44% AUC: 67.14%
Epoch: 30 Loss: 0.0661 valLoss: 0.2955 valAcc: 79.97% AUC: 66.90%
Epoch: 31 Loss: 0.0648 valLoss: 0.3057 valAcc: 80.54% AUC: 67.23%
Epoch: 32 Loss: 0.0621 valLoss: 0.3002 valAcc: 79.91% AUC: 67.37%
Epoch: 33 Loss: 0.0589 valLoss: 0.3137 valAcc: 80.04% AUC: 67.47%
Epoch: 34 Loss: 0.0574 valLoss: 0.2860 valAcc: 77.42% AUC: 67.49%
epitope_training.csv at: 19-12-2023#13:49:59 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
                                                                                                                                                                                                                                                                        Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1698 valLoss: 0.1327 valAcc: 11.32% AUC: 57.04%
Epoch: 01 Loss: 0.1469 valLoss: 0.1322 valAcc: 11.40% AUC: 57.54%
Epoch: 02 Loss: 0.1394 valLoss: 0.1316 valAcc: 12.27% AUC: 58.88%
Epoch: 03 Loss: 0.1354 valLoss: 0.1309 valAcc: 15.31% AUC: 60.41%
Epoch: 04 Loss: 0.1332 valLoss: 0.1299 valAcc: 17.91% AUC: 61.81%
Epoch: 05 Loss: 0.1291 valLoss: 0.1290 valAcc: 25.73% AUC: 63.37%
Epoch: 06 Loss: 0.1273 valLoss: 0.1279 valAcc: 27.87% AUC: 64.31%
Epoch: 07 Loss: 0.1255 valLoss: 0.1273 valAcc: 28.27% AUC: 64.86%
Epoch: 08 Loss: 0.1232 valLoss: 0.1267 valAcc: 32.63% AUC: 65.70%
Epoch: 09 Loss: 0.1217 valLoss: 0.1261 valAcc: 33.07% AUC: 66.05%
Epoch: 10 Loss: 0.1194 valLoss: 0.1261 valAcc: 37.99% AUC: 66.77%
Epoch: 11 Loss: 0.1175 valLoss: 0.1276 valAcc: 43.06% AUC: 66.73%
Epoch: 12 Loss: 0.1141 valLoss: 0.1290 valAcc: 45.04% AUC: 66.71%
Epoch: 13 Loss: 0.1118 valLoss: 0.1321 valAcc: 52.19% AUC: 66.80%
Epoch: 14 Loss: 0.1097 valLoss: 0.1321 valAcc: 51.24% AUC: 67.16%
Epoch: 15 Loss: 0.1075 valLoss: 0.1379 valAcc: 55.91% AUC: 66.76%
Epoch: 16 Loss: 0.1046 valLoss: 0.1443 valAcc: 57.27% AUC: 66.58%
                                                                  Epoch: 17 Loss: 0.1013 valLoss: 0.1492 valAcc: 59.29% AUC: 67.01%
Epoch: 18 Loss: 0.0975 valLoss: 0.1674 valAcc: 65.58% AUC: 66.43%
Epoch: 19 Loss: 0.0954 valLoss: 0.2051 valAcc: 74.16% AUC: 66.29%
Epoch: 20 Loss: 0.0932 valLoss: 0.1981 valAcc: 72.30% AUC: 66.63%
Epoch: 21 Loss: 0.0892 valLoss: 0.2024 valAcc: 72.10% AUC: 66.57%
Epoch: 22 Loss: 0.0882 valLoss: 0.2278 valAcc: 75.07% AUC: 66.04%
Epoch: 23 Loss: 0.0845 valLoss: 0.2243 valAcc: 73.26% AUC: 65.90%
Epoch: 24 Loss: 0.0814 valLoss: 0.2178 valAcc: 72.13% AUC: 66.09%
Epoch: 25 Loss: 0.0793 valLoss: 0.2430 valAcc: 74.65% AUC: 66.28%
Epoch: 26 Loss: 0.0777 valLoss: 0.2299 valAcc: 71.40% AUC: 66.22%
Epoch: 27 Loss: 0.0749 valLoss: 0.2385 valAcc: 71.71% AUC: 66.07%
Epoch: 28 Loss: 0.0732 valLoss: 0.2337 valAcc: 72.02% AUC: 66.17%
Epoch: 29 Loss: 0.0707 valLoss: 0.2158 valAcc: 66.94% AUC: 66.04%
Epoch: 30 Loss: 0.0685 valLoss: 0.2340 valAcc: 70.05% AUC: 66.31%
Epoch: 31 Loss: 0.0680 valLoss: 0.2108 valAcc: 65.72% AUC: 65.72%
Epoch: 32 Loss: 0.0657 valLoss: 0.2797 valAcc: 74.64% AUC: 66.27%
Epoch: 33 Loss: 0.0634 valLoss: 0.2882 valAcc: 75.77% AUC: 66.47%
Epoch: 34 Loss: 0.0610 valLoss: 0.3127 valAcc: 77.31% AUC: 66.04%
Epoch: 35 Loss: 0.0578 valLoss: 0.3497 valAcc: 79.44% AUC: 66.39%
Epoch: 36 Loss: 0.0560 valLoss: 0.3448 valAcc: 79.17% AUC: 66.89%
Epoch: 78 Loss: 0.0159 valLoss: 0.5377 valAcc: 83.74% AUC: 66.42%
Epoch: 38 Loss: 0.0524 valLoss: 0.3368 valAcc: 77.83% AUC: 66.04%
Epoch: 39 Loss: 0.0502 valLoss: 0.3520 valAcc: 78.91% AUC: 66.13%
Epoch: 40 Loss: 0.0487 valLoss: 0.3744 valAcc: 80.15% AUC: 66.14%
Epoch: 41 Loss: 0.0478 valLoss: 0.3342 valAcc: 77.89% AUC: 66.71%
Epoch: 42 Loss: 0.0460 valLoss: 0.3722 valAcc: 79.84% AUC: 66.47%
Epoch: 43 Loss: 0.0441 valLoss: 0.3809 valAcc: 80.20% AUC: 65.85%
Epoch: 44 Loss: 0.0423 valLoss: 0.3831 valAcc: 80.24% AUC: 66.36%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 19-12-2023#14:03:06 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Epoch: 45 Loss: 0.0413 valLoss: 0.4173 valAcc: 81.42% AUC: 66.23%
Epoch: 46 Loss: 0.0394 valLoss: 0.4007 valAcc: 79.93% AUC: 66.57%
Epoch: 47 Loss: 0.0385 valLoss: 0.4026 valAcc: 80.71% AUC: 66.66%
Epoch: 48 Loss: 0.0369 valLoss: 0.3896 valAcc: 80.10% AUC: 66.52%
Epoch: 49 Loss: 0.0356 valLoss: 0.4099 valAcc: 80.63% AUC: 66.68%
Epoch: 50 Loss: 0.0344 valLoss: 0.4381 valAcc: 81.79% AUC: 66.72%
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 51 Loss: 0.0333 valLoss: 0.4143 valAcc: 81.09% AUC: 67.37%
Epoch: 00 Loss: 0.1676 valLoss: 0.1338 valAcc: 11.32% AUC: 53.14%
Epoch: 52 Loss: 0.0339 valLoss: 0.4070 valAcc: 80.83% AUC: 67.07%
Epoch: 01 Loss: 0.1449 valLoss: 0.1348 valAcc: 12.62% AUC: 53.19%
Epoch: 53 Loss: 0.0315 valLoss: 0.3746 valAcc: 79.22% AUC: 67.12%
Epoch: 02 Loss: 0.1380 valLoss: 0.1346 valAcc: 15.64% AUC: 54.63%
Epoch: 54 Loss: 0.0306 valLoss: 0.4114 valAcc: 80.78% AUC: 66.99%
Epoch: 03 Loss: 0.1356 valLoss: 0.1328 valAcc: 19.77% AUC: 58.96%
Epoch: 55 Loss: 0.0297 valLoss: 0.3951 valAcc: 80.20% AUC: 67.18%
Epoch: 04 Loss: 0.1315 valLoss: 0.1309 valAcc: 29.67% AUC: 62.74%
Epoch: 56 Loss: 0.0295 valLoss: 0.4381 valAcc: 81.63% AUC: 67.08%
Epoch: 05 Loss: 0.1289 valLoss: 0.1319 valAcc: 50.03% AUC: 63.95%
Epoch: 57 Loss: 0.0280 valLoss: 0.4346 valAcc: 81.19% AUC: 67.30%
Epoch: 06 Loss: 0.1271 valLoss: 0.1309 valAcc: 54.43% AUC: 65.25%
Epoch: 58 Loss: 0.0273 valLoss: 0.4456 valAcc: 81.84% AUC: 66.99%
Epoch: 07 Loss: 0.1244 valLoss: 0.1301 valAcc: 54.78% AUC: 65.73%
Epoch: 59 Loss: 0.0262 valLoss: 0.4029 valAcc: 80.05% AUC: 67.00%
Epoch: 08 Loss: 0.1228 valLoss: 0.1334 valAcc: 59.49% AUC: 65.65%
Epoch: 60 Loss: 0.0264 valLoss: 0.4458 valAcc: 81.91% AUC: 67.55%
Epoch: 09 Loss: 0.1205 valLoss: 0.1323 valAcc: 56.32% AUC: 66.02%
Epoch: 61 Loss: 0.0249 valLoss: 0.4309 valAcc: 80.66% AUC: 67.30%
Epoch: 10 Loss: 0.1178 valLoss: 0.1337 valAcc: 59.22% AUC: 66.82%
Epoch: 62 Loss: 0.0240 valLoss: 0.4271 valAcc: 80.58% AUC: 67.27%
Epoch: 11 Loss: 0.1158 valLoss: 0.1331 valAcc: 57.97% AUC: 67.22%
Epoch: 63 Loss: 0.0239 valLoss: 0.4355 valAcc: 80.60% AUC: 67.20%
Epoch: 12 Loss: 0.1128 valLoss: 0.1407 valAcc: 64.15% AUC: 67.72%
Epoch: 64 Loss: 0.0231 valLoss: 0.4757 valAcc: 82.29% AUC: 67.04%
Epoch: 13 Loss: 0.1102 valLoss: 0.1491 valAcc: 67.66% AUC: 67.49%
Epoch: 65 Loss: 0.0222 valLoss: 0.4878 valAcc: 81.94% AUC: 67.16%
Epoch: 14 Loss: 0.1077 valLoss: 0.1603 valAcc: 70.71% AUC: 67.71%
Epoch: 66 Loss: 0.0230 valLoss: 0.4328 valAcc: 80.32% AUC: 67.10%
Epoch: 15 Loss: 0.1053 valLoss: 0.1655 valAcc: 70.94% AUC: 67.69%
Epoch: 67 Loss: 0.0226 valLoss: 0.4609 valAcc: 81.84% AUC: 66.68%
Epoch: 16 Loss: 0.1018 valLoss: 0.1832 valAcc: 73.25% AUC: 67.49%
Epoch: 68 Loss: 0.0215 valLoss: 0.4780 valAcc: 81.74% AUC: 66.19%
Epoch: 17 Loss: 0.0991 valLoss: 0.2025 valAcc: 76.03% AUC: 67.63%
Epoch: 69 Loss: 0.0204 valLoss: 0.4691 valAcc: 81.40% AUC: 66.73%
Epoch: 18 Loss: 0.0957 valLoss: 0.2051 valAcc: 74.75% AUC: 67.51%
Epoch: 70 Loss: 0.0203 valLoss: 0.4838 valAcc: 81.50% AUC: 66.69%
Epoch: 19 Loss: 0.0940 valLoss: 0.2212 valAcc: 76.82% AUC: 67.61%
Epoch: 71 Loss: 0.0193 valLoss: 0.4754 valAcc: 81.12% AUC: 66.72%
Epoch: 20 Loss: 0.0907 valLoss: 0.2422 valAcc: 77.67% AUC: 67.05%
Epoch: 72 Loss: 0.0192 valLoss: 0.5063 valAcc: 82.41% AUC: 66.61%
Epoch: 21 Loss: 0.0882 valLoss: 0.2944 valAcc: 82.80% AUC: 67.38%
Epoch: 73 Loss: 0.0178 valLoss: 0.4776 valAcc: 80.97% AUC: 66.98%
Epoch: 22 Loss: 0.0840 valLoss: 0.2897 valAcc: 81.23% AUC: 66.96%
Epoch: 23 Loss: 0.0824 valLoss: 0.2920 valAcc: 80.05% AUC: 66.74%
Epoch: 74 Loss: 0.0178 valLoss: 0.4843 valAcc: 81.58% AUC: 66.82%
Epoch: 116 Loss: 0.0080 valLoss: 0.5950 valAcc: 83.25% AUC: 67.38%
poch: 75 Loss: 0.0182 valLoss: 0.5141 valAcc: 82.69% AUC: 66.51%
Epoch: 25 Loss: 0.0756 valLoss: 0.3195 valAcc: 81.69% AUC: 67.17%
Epoch: 76 Loss: 0.0175 valLoss: 0.5092 valAcc: 82.14% AUC: 66.40%
Epoch: 26 Loss: 0.0740 valLoss: 0.3544 valAcc: 83.59% AUC: 66.59%
Epoch: 77 Loss: 0.0169 valLoss: 0.4938 valAcc: 81.98% AUC: 66.79%
                                                                   Epoch: 27 Loss: 0.0710 valLoss: 0.3120 valAcc: 79.96% AUC: 66.75%
Epoch: 78 Loss: 0.0167 valLoss: 0.5457 valAcc: 83.13% AUC: 66.63%
Epoch: 28 Loss: 0.0714 valLoss: 0.3008 valAcc: 78.82% AUC: 67.07%
Epoch: 79 Loss: 0.0164 valLoss: 0.5450 valAcc: 83.63% AUC: 67.14%
                                                                   Epoch: 29 Loss: 0.0670 valLoss: 0.3675 valAcc: 81.84% AUC: 66.66%
Epoch: 80 Loss: 0.0154 valLoss: 0.5235 valAcc: 82.51% AUC: 66.99%
Epoch: 30 Loss: 0.0650 valLoss: 0.3754 valAcc: 83.09% AUC: 67.06%
Epoch: 81 Loss: 0.0157 valLoss: 0.5434 valAcc: 83.24% AUC: 66.82%
  Epoch: 31 Loss: 0.0626 valLoss: 0.3744 valAcc: 81.84% AUC: 66.63%
Epoch: 82 Loss: 0.0155 valLoss: 0.5606 valAcc: 83.19% AUC: 66.57%
Epoch: 32 Loss: 0.0616 valLoss: 0.3447 valAcc: 80.00% AUC: 66.79%
Epoch: 83 Loss: 0.0159 valLoss: 0.5887 valAcc: 84.10% AUC: 66.61%
 Epoch: 33 Loss: 0.0609 valLoss: 0.3151 valAcc: 78.18% AUC: 66.63%
Epoch: 84 Loss: 0.0149 valLoss: 0.5412 valAcc: 83.31% AUC: 66.89%
Epoch: 34 Loss: 0.0588 valLoss: 0.2776 valAcc: 74.03% AUC: 66.66%
Epoch: 85 Loss: 0.0152 valLoss: 0.5339 valAcc: 83.00% AUC: 66.79%
 Epoch: 35 Loss: 0.0592 valLoss: 0.2808 valAcc: 74.94% AUC: 66.56%
Epoch: 86 Loss: 0.0140 valLoss: 0.5432 valAcc: 81.95% AUC: 66.75%
Epoch: 36 Loss: 0.0575 valLoss: 0.3001 valAcc: 76.77% AUC: 67.13%
Epoch: 87 Loss: 0.0135 valLoss: 0.5657 valAcc: 82.22% AUC: 66.40%
 Epoch: 37 Loss: 0.0543 valLoss: 0.3654 valAcc: 81.46% AUC: 67.58%
Epoch: 88 Loss: 0.0129 valLoss: 0.5391 valAcc: 81.47% AUC: 66.61%
Epoch: 38 Loss: 0.0503 valLoss: 0.3856 valAcc: 81.49% AUC: 66.92%
Epoch: 89 Loss: 0.0131 valLoss: 0.5826 valAcc: 82.92% AUC: 66.32%
                                                                   Epoch: 39 Loss: 0.0481 valLoss: 0.4102 valAcc: 83.08% AUC: 66.82%
Epoch: 90 Loss: 0.0124 valLoss: 0.5711 valAcc: 82.90% AUC: 66.64%
Epoch: 40 Loss: 0.0460 valLoss: 0.4397 valAcc: 84.20% AUC: 67.02%
Epoch: 91 Loss: 0.0122 valLoss: 0.5444 valAcc: 81.93% AUC: 66.79%
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Epoch: 41 Loss: 0.0448 valLoss: 0.4148 valAcc: 82.71% AUC: 67.15%
Epoch: 92 Loss: 0.0123 valLoss: 0.5591 valAcc: 82.33% AUC: 66.52%
Epoch: 42 Loss: 0.0435 valLoss: 0.4270 valAcc: 83.18% AUC: 66.81%
Epoch: 93 Loss: 0.0114 valLoss: 0.5757 valAcc: 82.58% AUC: 66.37%
Epoch: 43 Loss: 0.0413 valLoss: 0.4379 valAcc: 83.94% AUC: 67.29%
Epoch: 94 Loss: 0.0119 valLoss: 0.5386 valAcc: 81.50% AUC: 66.14%
Epoch: 44 Loss: 0.0404 valLoss: 0.4258 valAcc: 83.76% AUC: 67.68%
Epoch: 95 Loss: 0.0118 valLoss: 0.5065 valAcc: 80.78% AUC: 66.68%
Epoch: 45 Loss: 0.0387 valLoss: 0.4225 valAcc: 83.32% AUC: 67.35%
Epoch: 96 Loss: 0.0114 valLoss: 0.5434 valAcc: 81.75% AUC: 66.30%
Epoch: 46 Loss: 0.0389 valLoss: 0.4497 valAcc: 84.29% AUC: 67.77%
Epoch: 97 Loss: 0.0114 valLoss: 0.5551 valAcc: 81.91% AUC: 66.28%
Epoch: 47 Loss: 0.0372 valLoss: 0.4884 valAcc: 85.30% AUC: 67.06%
Epoch: 98 Loss: 0.0107 valLoss: 0.5372 valAcc: 81.71% AUC: 66.49%
Epoch: 48 Loss: 0.0357 valLoss: 0.4643 valAcc: 84.75% AUC: 67.47%
Epoch: 99 Loss: 0.0117 valLoss: 0.5302 valAcc: 81.02% AUC: 66.12%
Epoch: 49 Loss: 0.0344 valLoss: 0.4376 valAcc: 83.26% AUC: 67.56%
Epoch: 100 Loss: 0.0104 valLoss: 0.5367 valAcc: 81.72% AUC: 66.79%
Epoch: 50 Loss: 0.0331 valLoss: 0.4647 valAcc: 84.55% AUC: 67.00%
Epoch: 101 Loss: 0.0102 valLoss: 0.5193 valAcc: 81.41% AUC: 67.03%
Epoch: 51 Loss: 0.0326 valLoss: 0.4643 valAcc: 84.00% AUC: 67.28%
Epoch: 102 Loss: 0.0096 valLoss: 0.5466 valAcc: 81.83% AUC: 66.34%
Epoch: 52 Loss: 0.0315 valLoss: 0.5106 valAcc: 85.22% AUC: 66.89%
Epoch: 103 Loss: 0.0101 valLoss: 0.5960 valAcc: 82.95% AUC: 66.32%
Epoch: 53 Loss: 0.0307 valLoss: 0.4846 valAcc: 84.81% AUC: 67.56%
Epoch: 104 Loss: 0.0100 valLoss: 0.5822 valAcc: 82.21% AUC: 65.97%
Epoch: 54 Loss: 0.0297 valLoss: 0.4668 valAcc: 84.18% AUC: 67.37%
Epoch: 105 Loss: 0.0099 valLoss: 0.5922 valAcc: 82.58% AUC: 66.29%
Epoch: 55 Loss: 0.0295 valLoss: 0.4697 valAcc: 84.30% AUC: 67.58%
Epoch: 106 Loss: 0.0097 valLoss: 0.6106 valAcc: 83.46% AUC: 66.47%
Epoch: 56 Loss: 0.0284 valLoss: 0.4673 valAcc: 84.37% AUC: 67.61%
Epoch: 107 Loss: 0.0099 valLoss: 0.5479 valAcc: 82.17% AUC: 66.52%
Epoch: 57 Loss: 0.0275 valLoss: 0.4718 valAcc: 84.28% AUC: 67.83%
Epoch: 108 Loss: 0.0089 valLoss: 0.5505 valAcc: 82.30% AUC: 66.55%
Epoch: 58 Loss: 0.0273 valLoss: 0.4890 valAcc: 84.61% AUC: 67.68%
Epoch: 109 Loss: 0.0090 valLoss: 0.5851 valAcc: 82.75% AUC: 66.31%
Epoch: 59 Loss: 0.0278 valLoss: 0.4696 valAcc: 84.32% AUC: 68.15%
Epoch: 110 Loss: 0.0086 valLoss: 0.5929 valAcc: 82.98% AUC: 66.27%
Epoch: 60 Loss: 0.0263 valLoss: 0.5413 valAcc: 85.69% AUC: 67.81%
Epoch: 111 Loss: 0.0084 valLoss: 0.6148 valAcc: 83.69% AUC: 66.40%
Epoch: 61 Loss: 0.0247 valLoss: 0.5652 valAcc: 86.36% AUC: 68.03%
Epoch: 112 Loss: 0.0086 valLoss: 0.6239 valAcc: 83.93% AUC: 66.45%
Epoch: 62 Loss: 0.0242 valLoss: 0.5265 valAcc: 85.00% AUC: 67.70%
Epoch: 113 Loss: 0.0088 valLoss: 0.5932 valAcc: 83.40% AUC: 66.40%
Epoch: 63 Loss: 0.0238 valLoss: 0.5700 valAcc: 86.37% AUC: 67.74%
Epoch: 114 Loss: 0.0087 valLoss: 0.6013 valAcc: 83.05% AUC: 66.42%
Epoch: 64 Loss: 0.0232 valLoss: 0.5677 valAcc: 85.80% AUC: 67.79%
Epoch: 115 Loss: 0.0087 valLoss: 0.6261 valAcc: 83.80% AUC: 66.38%
Epoch: 65 Loss: 0.0229 valLoss: 0.5172 valAcc: 84.21% AUC: 67.60%
Epoch: 116 Loss: 0.0082 valLoss: 0.5953 valAcc: 82.90% AUC: 65.98%
Epoch: 66 Loss: 0.0223 valLoss: 0.5160 valAcc: 84.62% AUC: 67.36%
Epoch: 117 Loss: 0.0074 valLoss: 0.6127 valAcc: 83.31% AUC: 66.23%
Epoch: 67 Loss: 0.0213 valLoss: 0.5089 valAcc: 84.37% AUC: 67.86%
Epoch: 118 Loss: 0.0075 valLoss: 0.6010 valAcc: 82.80% AUC: 66.46%
Epoch: 68 Loss: 0.0215 valLoss: 0.5484 valAcc: 85.36% AUC: 67.70%
Epoch: 119 Loss: 0.0068 valLoss: 0.6046 valAcc: 82.65% AUC: 66.32%
Epoch: 69 Loss: 0.0205 valLoss: 0.5308 valAcc: 85.25% AUC: 68.28%
Epoch: 120 Loss: 0.0070 valLoss: 0.6543 valAcc: 83.63% AUC: 65.80%
Epoch: 70 Loss: 0.0206 valLoss: 0.5442 valAcc: 85.06% AUC: 68.03%
Epoch: 121 Loss: 0.0066 valLoss: 0.6310 valAcc: 82.99% AUC: 66.15%
Epoch: 71 Loss: 0.0200 valLoss: 0.5965 valAcc: 86.10% AUC: 67.55%
Epoch: 72 Loss: 0.0184 valLoss: 0.5722 valAcc: 85.81% AUC: 67.76%
Epoch: 122 Loss: 0.0070 valLoss: 0.6254 valAcc: 83.18% AUC: 65.88%
Epoch: 73 Loss: 0.0190 valLoss: 0.5999 valAcc: 86.30% AUC: 67.49%
Epoch: 123 Loss: 0.0061 valLoss: 0.6405 valAcc: 83.48% AUC: 65.56%
Epoch: 74 Loss: 0.0178 valLoss: 0.5416 valAcc: 84.76% AUC: 67.44%
Epoch: 124 Loss: 0.0063 valLoss: 0.6243 valAcc: 82.85% AUC: 65.89%
Epoch: 75 Loss: 0.0173 valLoss: 0.5731 valAcc: 85.58% AUC: 67.69%
Epoch: 125 Loss: 0.0061 valLoss: 0.6098 valAcc: 82.88% AUC: 66.04%
Epoch: 76 Loss: 0.0173 valLoss: 0.5805 valAcc: 85.86% AUC: 68.05%
Epoch: 126 Loss: 0.0061 valLoss: 0.6285 valAcc: 82.81% AUC: 66.02%
Epoch: 77 Loss: 0.0171 valLoss: 0.5634 valAcc: 85.28% AUC: 68.25%
Epoch: 127 Loss: 0.0058 valLoss: 0.5950 valAcc: 82.07% AUC: 65.48%
Epoch: 78 Loss: 0.0162 valLoss: 0.5549 valAcc: 85.37% AUC: 67.72%
Epoch: 128 Loss: 0.0062 valLoss: 0.6496 valAcc: 83.45% AUC: 65.60%
Epoch: 79 Loss: 0.0159 valLoss: 0.5704 valAcc: 85.41% AUC: 67.40%
Epoch: 129 Loss: 0.0058 valLoss: 0.6453 valAcc: 83.28% AUC: 65.91%
Epoch: 80 Loss: 0.0156 valLoss: 0.5500 valAcc: 85.00% AUC: 67.92%
Epoch: 130 Loss: 0.0065 valLoss: 0.6106 valAcc: 82.16% AUC: 65.85%
Epoch: 81 Loss: 0.0156 valLoss: 0.5589 valAcc: 85.00% AUC: 67.50%
Epoch: 131 Loss: 0.0065 valLoss: 0.6191 valAcc: 82.62% AUC: 65.77%
Epoch: 82 Loss: 0.0155 valLoss: 0.6012 valAcc: 85.60% AUC: 67.61%
Epoch: 132 Loss: 0.0060 valLoss: 0.6240 valAcc: 82.41% AUC: 65.86%
Epoch: 83 Loss: 0.0152 valLoss: 0.5845 valAcc: 85.21% AUC: 67.54%
Epoch: 133 Loss: 0.0058 valLoss: 0.6319 valAcc: 82.74% AUC: 65.66%
Epoch: 84 Loss: 0.0138 valLoss: 0.5994 valAcc: 85.74% AUC: 67.58%
Epoch: 134 Loss: 0.0054 valLoss: 0.6490 valAcc: 83.05% AUC: 65.85%
Epoch: 85 Loss: 0.0143 valLoss: 0.5369 valAcc: 83.94% AUC: 67.50%
Epoch: 135 Loss: 0.0061 valLoss: 0.6621 valAcc: 83.25% AUC: 65.66%
Epoch: 86 Loss: 0.0141 valLoss: 0.5773 valAcc: 84.92% AUC: 67.86%
Epoch: 136 Loss: 0.0057 valLoss: 0.6668 valAcc: 83.30% AUC: 65.88%
Epoch: 87 Loss: 0.0135 valLoss: 0.5799 valAcc: 85.14% AUC: 67.40%
Epoch: 137 Loss: 0.0052 valLoss: 0.6466 valAcc: 82.76% AUC: 65.95%
Epoch: 88 Loss: 0.0126 valLoss: 0.6031 valAcc: 85.49% AUC: 67.49%
Epoch: 138 Loss: 0.0058 valLoss: 0.6265 valAcc: 82.19% AUC: 66.01%
Epoch: 89 Loss: 0.0127 valLoss: 0.6089 valAcc: 85.44% AUC: 67.31%
Epoch: 139 Loss: 0.0059 valLoss: 0.6646 valAcc: 84.09% AUC: 66.25%
Epoch: 90 Loss: 0.0128 valLoss: 0.5955 valAcc: 84.96% AUC: 67.29%
Epoch: 140 Loss: 0.0054 valLoss: 0.6791 valAcc: 83.69% AUC: 66.10%
Restoring model weights from the best epoch: 60 with the best VAL_AUC: 67.55%
Training time: 0:36:08.116324
## Testing ../data/prepared_epitope_testing.csv at: 19-12-2023#14:27:30 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.107234
The best cut-off value is: 0.45
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15043  1082]
 [ 1082   303]]
ValAcc: 87.64% specScore: 93.29% presScore: 21.88% recallScore: 21.88% F1Score: 21.88% MCC: 15.17% AUC: 68.90% AP: 16.78%
Epoch: 91 Loss: 0.0121 valLoss: 0.6208 valAcc: 85.46% AUC: 67.02%
Epoch: 92 Loss: 0.0126 valLoss: 0.6271 valAcc: 85.68% AUC: 67.51%
Epoch: 93 Loss: 0.0124 valLoss: 0.6026 valAcc: 85.34% AUC: 67.43%
Epoch: 94 Loss: 0.0115 valLoss: 0.6333 valAcc: 85.66% AUC: 67.54%
Epoch: 95 Loss: 0.0115 valLoss: 0.6006 valAcc: 84.80% AUC: 67.33%
Epoch: 96 Loss: 0.0118 valLoss: 0.6116 valAcc: 84.84% AUC: 66.88%
Epoch: 97 Loss: 0.0112 valLoss: 0.6172 valAcc: 85.47% AUC: 67.52%
Epoch: 98 Loss: 0.0111 valLoss: 0.6005 valAcc: 84.67% AUC: 67.65%
Epoch: 99 Loss: 0.0118 valLoss: 0.5981 valAcc: 84.68% AUC: 67.24%
Epoch: 100 Loss: 0.0113 valLoss: 0.5731 valAcc: 83.69% AUC: 66.91%
Epoch: 101 Loss: 0.0104 valLoss: 0.6415 valAcc: 85.56% AUC: 67.45%
Epoch: 102 Loss: 0.0104 valLoss: 0.6272 valAcc: 85.37% AUC: 67.24%
Epoch: 103 Loss: 0.0105 valLoss: 0.6013 valAcc: 84.43% AUC: 67.05%
Epoch: 104 Loss: 0.0106 valLoss: 0.6195 valAcc: 85.04% AUC: 66.93%
Epoch: 105 Loss: 0.0109 valLoss: 0.5695 valAcc: 83.17% AUC: 66.59%
Epoch: 106 Loss: 0.0109 valLoss: 0.5989 valAcc: 84.50% AUC: 67.16%
Epoch: 107 Loss: 0.0101 valLoss: 0.5924 valAcc: 84.07% AUC: 67.28%
Epoch: 108 Loss: 0.0101 valLoss: 0.5945 valAcc: 84.65% AUC: 67.70%
Epoch: 109 Loss: 0.0105 valLoss: 0.6195 valAcc: 84.43% AUC: 66.75%
Epoch: 110 Loss: 0.0107 valLoss: 0.6162 valAcc: 84.62% AUC: 67.53%
Epoch: 111 Loss: 0.0107 valLoss: 0.6842 valAcc: 85.74% AUC: 67.74%
Epoch: 112 Loss: 0.0103 valLoss: 0.6527 valAcc: 85.10% AUC: 67.54%
Epoch: 113 Loss: 0.0088 valLoss: 0.6676 valAcc: 85.40% AUC: 67.96%
Epoch: 114 Loss: 0.0084 valLoss: 0.6509 valAcc: 84.98% AUC: 67.81%
Epoch: 115 Loss: 0.0080 valLoss: 0.6562 valAcc: 85.10% AUC: 67.92%
Epoch: 116 Loss: 0.0074 valLoss: 0.6914 valAcc: 85.49% AUC: 67.92%
Epoch: 117 Loss: 0.0072 valLoss: 0.6114 valAcc: 83.84% AUC: 67.58%
Epoch: 118 Loss: 0.0075 valLoss: 0.6255 valAcc: 84.53% AUC: 67.61%
Epoch: 119 Loss: 0.0077 valLoss: 0.6018 valAcc: 83.99% AUC: 67.92%
Epoch: 120 Loss: 0.0070 valLoss: 0.6555 valAcc: 85.29% AUC: 67.80%
Epoch: 121 Loss: 0.0068 valLoss: 0.6798 valAcc: 85.63% AUC: 67.34%
Epoch: 122 Loss: 0.0071 valLoss: 0.6656 valAcc: 85.51% AUC: 67.56%
Epoch: 123 Loss: 0.0068 valLoss: 0.6664 valAcc: 85.14% AUC: 67.32%
Epoch: 124 Loss: 0.0065 valLoss: 0.6871 valAcc: 85.56% AUC: 67.45%
Epoch: 125 Loss: 0.0066 valLoss: 0.6989 valAcc: 85.53% AUC: 67.26%
Epoch: 126 Loss: 0.0065 valLoss: 0.6759 valAcc: 84.96% AUC: 67.19%
Epoch: 127 Loss: 0.0064 valLoss: 0.6768 valAcc: 85.28% AUC: 67.13%
Epoch: 128 Loss: 0.0060 valLoss: 0.6751 valAcc: 85.18% AUC: 67.60%
Epoch: 129 Loss: 0.0062 valLoss: 0.6660 valAcc: 84.95% AUC: 67.50%
Epoch: 130 Loss: 0.0066 valLoss: 0.6667 valAcc: 84.52% AUC: 67.48%
Epoch: 131 Loss: 0.0064 valLoss: 0.6934 valAcc: 85.28% AUC: 67.80%
Epoch: 132 Loss: 0.0063 valLoss: 0.6987 valAcc: 85.16% AUC: 67.47%
Epoch: 133 Loss: 0.0061 valLoss: 0.6582 valAcc: 84.78% AUC: 67.50%
Epoch: 134 Loss: 0.0059 valLoss: 0.6780 valAcc: 85.10% AUC: 67.27%
Epoch: 135 Loss: 0.0058 valLoss: 0.6680 valAcc: 84.56% AUC: 67.20%
Epoch: 136 Loss: 0.0054 valLoss: 0.6908 valAcc: 85.05% AUC: 67.27%
Epoch: 137 Loss: 0.0058 valLoss: 0.6828 valAcc: 84.99% AUC: 67.38%
Epoch: 138 Loss: 0.0053 valLoss: 0.7067 valAcc: 85.28% AUC: 66.97%
Epoch: 139 Loss: 0.0054 valLoss: 0.6841 valAcc: 84.80% AUC: 67.00%
Epoch: 140 Loss: 0.0056 valLoss: 0.6783 valAcc: 84.74% AUC: 67.03%
Epoch: 141 Loss: 0.0057 valLoss: 0.6456 valAcc: 83.87% AUC: 66.87%
Epoch: 142 Loss: 0.0061 valLoss: 0.6937 valAcc: 84.82% AUC: 66.47%
Epoch: 143 Loss: 0.0062 valLoss: 0.6393 valAcc: 83.45% AUC: 66.61%
Epoch: 144 Loss: 0.0054 valLoss: 0.6878 valAcc: 84.67% AUC: 66.88%
Epoch: 145 Loss: 0.0061 valLoss: 0.6650 valAcc: 84.30% AUC: 66.72%
Epoch: 146 Loss: 0.0063 valLoss: 0.6986 valAcc: 84.72% AUC: 66.75%
Epoch: 147 Loss: 0.0068 valLoss: 0.7202 valAcc: 85.24% AUC: 67.11%
Epoch: 148 Loss: 0.0052 valLoss: 0.7142 valAcc: 84.90% AUC: 66.84%
Epoch: 149 Loss: 0.0052 valLoss: 0.7005 valAcc: 84.91% AUC: 67.27%
Restoring model weights from the best epoch: 69 with the best VAL_AUC: 68.28%
Training time: 0:37:55.150612
## Testing ../data/prepared_epitope_testing.csv at: 19-12-2023#14:42:23 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.138494
The best cut-off value is: 0.08
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15029  1096]
 [ 1096   289]]
ValAcc: 87.48% specScore: 93.20% presScore: 20.87% recallScore: 20.87% F1Score: 20.87% MCC: 14.07% AUC: 69.28% AP: 16.13%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 20-12-2023#12:17:18 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1578 valLoss: 0.1332 valAcc: 11.32% AUC: 55.43%
Epoch: 01 Loss: 0.1447 valLoss: 0.1327 valAcc: 12.14% AUC: 59.09%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 20-12-2023#12:21:14 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
                                                                                                                                                                                                               Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
                                                                  Epoch: 00 Loss: 0.1623 valLoss: 0.1331 valAcc: 11.32% AUC: 54.89%
                                                                  Epoch: 01 Loss: 0.1429 valLoss: 0.1326 valAcc: 11.35% AUC: 56.86%
                                                                  Epoch: 02 Loss: 0.1368 valLoss: 0.1321 valAcc: 11.47% AUC: 58.21%
Epoch: 03 Loss: 0.1338 valLoss: 0.1310 valAcc: 12.77% AUC: 60.85%
Epoch: 04 Loss: 0.1310 valLoss: 0.1295 valAcc: 21.50% AUC: 63.60%
                                                                  Epoch: 05 Loss: 0.1280 valLoss: 0.1284 valAcc: 36.20% AUC: 65.61%
                                                                  Epoch: 06 Loss: 0.1255 valLoss: 0.1285 valAcc: 46.15% AUC: 66.63%
Epoch: 07 Loss: 0.1230 valLoss: 0.1280 valAcc: 51.23% AUC: 67.48%
Epoch: 08 Loss: 0.1202 valLoss: 0.1270 valAcc: 50.40% AUC: 67.90%
Epoch: 09 Loss: 0.1176 valLoss: 0.1291 valAcc: 55.36% AUC: 68.17%
                                                                  Epoch: 10 Loss: 0.1151 valLoss: 0.1315 valAcc: 57.43% AUC: 68.20%
                                                                  Epoch: 11 Loss: 0.1128 valLoss: 0.1356 valAcc: 61.07% AUC: 68.39%
                                                                  Epoch: 12 Loss: 0.1095 valLoss: 0.1463 valAcc: 66.87% AUC: 68.40%
Epoch: 13 Loss: 0.1069 valLoss: 0.1557 valAcc: 68.73% AUC: 68.39%
Epoch: 14 Loss: 0.1039 valLoss: 0.1678 valAcc: 70.86% AUC: 68.10%
Epoch: 15 Loss: 0.1007 valLoss: 0.1820 valAcc: 73.77% AUC: 68.31%
Epoch: 19 Loss: 0.0899 valLoss: 0.2128 valAcc: 77.78% AUC: 67.21%
                                                                  Epoch: 20 Loss: 0.0858 valLoss: 0.2484 valAcc: 79.69% AUC: 67.15%
Epoch: 21 Loss: 0.0837 valLoss: 0.2212 valAcc: 76.81% AUC: 67.28%
                                                                  Epoch: 22 Loss: 0.0805 valLoss: 0.2365 valAcc: 77.97% AUC: 67.22%
                                                                  Epoch: 23 Loss: 0.0777 valLoss: 0.2522 valAcc: 79.15% AUC: 67.17%
                                                                  Epoch: 24 Loss: 0.0747 valLoss: 0.2619 valAcc: 79.07% AUC: 67.20%
                                                                  Epoch: 25 Loss: 0.0725 valLoss: 0.2758 valAcc: 79.36% AUC: 66.80%
                                                                  Epoch: 26 Loss: 0.0699 valLoss: 0.2495 valAcc: 77.37% AUC: 67.56%
                                                                  Epoch: 27 Loss: 0.0676 valLoss: 0.2495 valAcc: 76.26% AUC: 67.33%
                                                                  Epoch: 28 Loss: 0.0659 valLoss: 0.2965 valAcc: 80.38% AUC: 67.25%
                                                                  Epoch: 29 Loss: 0.0633 valLoss: 0.2866 valAcc: 79.37% AUC: 66.98%
                                                                  Epoch: 30 Loss: 0.0605 valLoss: 0.3103 valAcc: 80.90% AUC: 67.27%
                                                                  Epoch: 31 Loss: 0.0583 valLoss: 0.2966 valAcc: 79.45% AUC: 67.70%
                                                                  Epoch: 32 Loss: 0.0549 valLoss: 0.3245 valAcc: 80.73% AUC: 67.44%
                                                                  Epoch: 33 Loss: 0.0549 valLoss: 0.3142 valAcc: 80.34% AUC: 67.83%
Epoch: 34 Loss: 0.0516 valLoss: 0.3251 valAcc: 80.43% AUC: 67.57%
                                                                  Epoch: 35 Loss: 0.0511 valLoss: 0.3315 valAcc: 80.43% AUC: 67.79%
                                                                  Epoch: 36 Loss: 0.0487 valLoss: 0.3566 valAcc: 81.97% AUC: 67.73%
                                                                  Epoch: 37 Loss: 0.0469 valLoss: 0.3751 valAcc: 82.47% AUC: 67.38%
                                                                  Epoch: 38 Loss: 0.0447 valLoss: 0.3996 valAcc: 83.10% AUC: 67.42%
                                                                  Epoch: 39 Loss: 0.0441 valLoss: 0.3540 valAcc: 80.85% AUC: 67.51%
                                                                  Epoch: 40 Loss: 0.0422 valLoss: 0.3697 valAcc: 81.92% AUC: 67.68%
                                                                  Epoch: 41 Loss: 0.0417 valLoss: 0.3377 valAcc: 79.53% AUC: 67.58%
Epoch: 43 Loss: 0.0407 valLoss: 0.3838 valAcc: 80.04% AUC: 66.21%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 20-12-2023#12:33:46 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
                                                                  Epoch: 44 Loss: 0.0398 valLoss: 0.3811 valAcc: 79.83% AUC: 66.06%
Epoch: 45 Loss: 0.0382 valLoss: 0.4048 valAcc: 80.81% AUC: 66.09%
Epoch: 46 Loss: 0.0387 valLoss: 0.3825 valAcc: 79.44% AUC: 65.92%
Epoch: 47 Loss: 0.0373 valLoss: 0.4061 valAcc: 80.81% AUC: 65.79%
                                                                  Epoch: 48 Loss: 0.0351 valLoss: 0.4061 valAcc: 80.94% AUC: 66.14%
                                                                  Epoch: 49 Loss: 0.0347 valLoss: 0.3911 valAcc: 79.73% AUC: 66.29%
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
                                                                  Epoch: 50 Loss: 0.0333 valLoss: 0.4268 valAcc: 81.72% AUC: 66.35%
Epoch: 00 Loss: 0.1619 valLoss: 0.1323 valAcc: 11.32% AUC: 57.43%
                                                                  Epoch: 51 Loss: 0.0333 valLoss: 0.4005 valAcc: 80.86% AUC: 66.46%
Epoch: 01 Loss: 0.1427 valLoss: 0.1318 valAcc: 11.32% AUC: 60.57%
Epoch: 52 Loss: 0.0320 valLoss: 0.4349 valAcc: 82.58% AUC: 66.55%
Epoch: 02 Loss: 0.1374 valLoss: 0.1318 valAcc: 11.35% AUC: 61.83%
Epoch: 53 Loss: 0.0314 valLoss: 0.4229 valAcc: 81.54% AUC: 66.46%
                                                                  Epoch: 03 Loss: 0.1344 valLoss: 0.1304 valAcc: 11.81% AUC: 62.52%
Epoch: 54 Loss: 0.0295 valLoss: 0.4479 valAcc: 83.21% AUC: 66.62%
                                                                  Epoch: 04 Loss: 0.1315 valLoss: 0.1296 valAcc: 13.56% AUC: 62.97%
Epoch: 55 Loss: 0.0295 valLoss: 0.4937 valAcc: 84.06% AUC: 66.85%
                                                                  Epoch: 56 Loss: 0.0286 valLoss: 0.4991 valAcc: 84.35% AUC: 66.49%
Epoch: 05 Loss: 0.1289 valLoss: 0.1283 valAcc: 23.64% AUC: 63.68%
                                                                  Epoch: 57 Loss: 0.0294 valLoss: 0.4818 valAcc: 83.92% AUC: 66.81%
Epoch: 06 Loss: 0.1264 valLoss: 0.1278 valAcc: 28.00% AUC: 64.26%
Epoch: 58 Loss: 0.0284 valLoss: 0.4454 valAcc: 81.80% AUC: 66.46%
Epoch: 07 Loss: 0.1236 valLoss: 0.1270 valAcc: 31.48% AUC: 64.85%
Epoch: 59 Loss: 0.0261 valLoss: 0.4689 valAcc: 82.35% AUC: 65.93%
Epoch: 08 Loss: 0.1219 valLoss: 0.1260 valAcc: 32.46% AUC: 65.94%
Epoch: 60 Loss: 0.0252 valLoss: 0.4476 valAcc: 81.56% AUC: 66.51%
Epoch: 09 Loss: 0.1186 valLoss: 0.1248 valAcc: 35.31% AUC: 66.88%
Epoch: 61 Loss: 0.0236 valLoss: 0.4749 valAcc: 82.29% AUC: 66.05%
Epoch: 10 Loss: 0.1166 valLoss: 0.1244 valAcc: 39.74% AUC: 67.29%
Epoch: 62 Loss: 0.0238 valLoss: 0.4557 valAcc: 82.22% AUC: 66.71%
Epoch: 11 Loss: 0.1138 valLoss: 0.1246 valAcc: 44.00% AUC: 67.50%
                                                                  Epoch: 63 Loss: 0.0221 valLoss: 0.4723 valAcc: 82.27% AUC: 66.21%
Epoch: 12 Loss: 0.1110 valLoss: 0.1256 valAcc: 49.95% AUC: 67.99%
                                                                  Epoch: 64 Loss: 0.0219 valLoss: 0.4647 valAcc: 82.10% AUC: 66.38%
Epoch: 13 Loss: 0.1072 valLoss: 0.1259 valAcc: 49.85% AUC: 68.28%
Epoch: 65 Loss: 0.0218 valLoss: 0.4437 valAcc: 81.44% AUC: 66.50%
Epoch: 14 Loss: 0.1053 valLoss: 0.1278 valAcc: 48.96% AUC: 67.78%
Epoch: 66 Loss: 0.0208 valLoss: 0.4822 valAcc: 82.50% AUC: 66.14%
Epoch: 15 Loss: 0.1012 valLoss: 0.1310 valAcc: 54.12% AUC: 68.12%
Epoch: 67 Loss: 0.0201 valLoss: 0.4593 valAcc: 81.50% AUC: 65.96%
Epoch: 16 Loss: 0.0984 valLoss: 0.1445 valAcc: 64.24% AUC: 67.75%
Epoch: 68 Loss: 0.0196 valLoss: 0.4804 valAcc: 82.35% AUC: 66.39%
Epoch: 17 Loss: 0.0951 valLoss: 0.1497 valAcc: 64.98% AUC: 67.66%
Epoch: 69 Loss: 0.0185 valLoss: 0.4976 valAcc: 82.98% AUC: 66.42%
Epoch: 18 Loss: 0.0922 valLoss: 0.1634 valAcc: 68.94% AUC: 67.37%
                                                                  Epoch: 70 Loss: 0.0186 valLoss: 0.4863 valAcc: 82.13% AUC: 66.23%
Epoch: 19 Loss: 0.0893 valLoss: 0.1675 valAcc: 67.22% AUC: 67.04%
                                                                  Epoch: 71 Loss: 0.0180 valLoss: 0.5026 valAcc: 82.80% AUC: 66.26%
Epoch: 20 Loss: 0.0862 valLoss: 0.1719 valAcc: 67.99% AUC: 67.18%
Epoch: 72 Loss: 0.0176 valLoss: 0.5075 valAcc: 83.03% AUC: 65.78%
Epoch: 21 Loss: 0.0829 valLoss: 0.1836 valAcc: 70.59% AUC: 66.82%
Epoch: 73 Loss: 0.0180 valLoss: 0.4838 valAcc: 82.29% AUC: 66.07%
Epoch: 22 Loss: 0.0802 valLoss: 0.1804 valAcc: 68.54% AUC: 67.12%
Epoch: 74 Loss: 0.0170 valLoss: 0.4826 valAcc: 82.17% AUC: 66.06%
Epoch: 23 Loss: 0.0774 valLoss: 0.1660 valAcc: 63.40% AUC: 67.21%
Epoch: 75 Loss: 0.0174 valLoss: 0.4932 valAcc: 82.46% AUC: 66.15%
Epoch: 24 Loss: 0.0753 valLoss: 0.1910 valAcc: 70.52% AUC: 66.68%
Epoch: 76 Loss: 0.0154 valLoss: 0.5283 valAcc: 83.19% AUC: 65.87%
Epoch: 25 Loss: 0.0719 valLoss: 0.1918 valAcc: 69.87% AUC: 67.02%
Epoch: 77 Loss: 0.0160 valLoss: 0.5039 valAcc: 82.11% AUC: 66.07%
Epoch: 26 Loss: 0.0689 valLoss: 0.2094 valAcc: 73.67% AUC: 67.01%
Epoch: 78 Loss: 0.0155 valLoss: 0.5491 valAcc: 83.50% AUC: 65.77%
Epoch: 27 Loss: 0.0664 valLoss: 0.1923 valAcc: 68.26% AUC: 66.93%
Epoch: 79 Loss: 0.0152 valLoss: 0.5126 valAcc: 82.33% AUC: 66.03%
Epoch: 28 Loss: 0.0637 valLoss: 0.1938 valAcc: 69.25% AUC: 66.62%
Epoch: 80 Loss: 0.0145 valLoss: 0.5023 valAcc: 82.08% AUC: 66.38%
                                                                  Epoch: 29 Loss: 0.0619 valLoss: 0.1976 valAcc: 67.71% AUC: 67.07%
Epoch: 81 Loss: 0.0143 valLoss: 0.5441 valAcc: 83.31% AUC: 66.20%
                                                                  Epoch: 30 Loss: 0.0596 valLoss: 0.1978 valAcc: 69.14% AUC: 66.66%
Epoch: 82 Loss: 0.0140 valLoss: 0.5565 valAcc: 83.85% AUC: 66.11%
                                                                  Epoch: 31 Loss: 0.0580 valLoss: 0.1903 valAcc: 66.18% AUC: 66.80%
Epoch: 83 Loss: 0.0135 valLoss: 0.5416 valAcc: 83.42% AUC: 66.03%
Epoch: 32 Loss: 0.0561 valLoss: 0.1988 valAcc: 67.43% AUC: 66.83%
Epoch: 84 Loss: 0.0143 valLoss: 0.5330 valAcc: 83.25% AUC: 65.94%
Epoch: 33 Loss: 0.0536 valLoss: 0.2066 valAcc: 69.35% AUC: 66.80%
Epoch: 85 Loss: 0.0137 valLoss: 0.5349 valAcc: 83.06% AUC: 65.95%
Epoch: 34 Loss: 0.0522 valLoss: 0.2091 valAcc: 67.93% AUC: 66.09%
Epoch: 86 Loss: 0.0135 valLoss: 0.5275 valAcc: 82.79% AUC: 65.91%
Epoch: 35 Loss: 0.0498 valLoss: 0.2300 valAcc: 72.35% AUC: 66.30%
Epoch: 87 Loss: 0.0133 valLoss: 0.5336 valAcc: 82.55% AUC: 66.21%
Epoch: 36 Loss: 0.0484 valLoss: 0.2117 valAcc: 68.85% AUC: 66.80%
Epoch: 88 Loss: 0.0128 valLoss: 0.5644 valAcc: 83.69% AUC: 65.80%
Epoch: 37 Loss: 0.0455 valLoss: 0.2288 valAcc: 72.25% AUC: 66.34%
Epoch: 89 Loss: 0.0126 valLoss: 0.5755 valAcc: 83.49% AUC: 65.00%
                                                                  Epoch: 38 Loss: 0.0450 valLoss: 0.2248 valAcc: 70.09% AUC: 66.82%
Epoch: 90 Loss: 0.0128 valLoss: 0.5295 valAcc: 82.34% AUC: 65.93%
                                                                  Epoch: 39 Loss: 0.0438 valLoss: 0.2400 valAcc: 74.29% AUC: 66.95%
Epoch: 91 Loss: 0.0121 valLoss: 0.5847 valAcc: 83.73% AUC: 65.78%
Restoring model weights from the best epoch: 11 with the best VAL_AUC: 68.39%
Training time: 0:23:18.469968
## Testing ../data/prepared_epitope_testing.csv at: 20-12-2023#12:45:54 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.126638
The best cut-off value is: 0.61
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15054  1071]
 [ 1071   314]]
ValAcc: 87.77% specScore: 93.36% presScore: 22.67% recallScore: 22.67% F1Score: 22.67% MCC: 16.03% AUC: 72.45% AP: 17.73%
Epoch: 40 Loss: 0.0414 valLoss: 0.2499 valAcc: 75.42% AUC: 66.79%
Epoch: 41 Loss: 0.0403 valLoss: 0.2419 valAcc: 73.46% AUC: 66.43%
Epoch: 42 Loss: 0.0386 valLoss: 0.2630 valAcc: 76.42% AUC: 66.18%
                                                                  Epoch: 43 Loss: 0.0375 valLoss: 0.2499 valAcc: 74.79% AUC: 66.94%
                                                                  Epoch: 44 Loss: 0.0359 valLoss: 0.2615 valAcc: 76.37% AUC: 66.36%
                                                                  Epoch: 45 Loss: 0.0347 valLoss: 0.2666 valAcc: 76.71% AUC: 66.40%
                                                                  Epoch: 46 Loss: 0.0338 valLoss: 0.2717 valAcc: 77.04% AUC: 66.39%
                                                                  Epoch: 47 Loss: 0.0324 valLoss: 0.2748 valAcc: 76.73% AUC: 65.95%
                                                                  Epoch: 48 Loss: 0.0320 valLoss: 0.2674 valAcc: 75.75% AUC: 65.96%
Epoch: 49 Loss: 0.0306 valLoss: 0.2765 valAcc: 76.74% AUC: 66.23%
Epoch: 50 Loss: 0.0300 valLoss: 0.2763 valAcc: 76.02% AUC: 66.25%
Epoch: 51 Loss: 0.0288 valLoss: 0.2868 valAcc: 76.62% AUC: 66.28%
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Epoch: 52 Loss: 0.0280 valLoss: 0.2939 valAcc: 77.10% AUC: 65.73%
Epoch: 53 Loss: 0.0264 valLoss: 0.3022 valAcc: 78.25% AUC: 66.10%
Epoch: 54 Loss: 0.0259 valLoss: 0.3062 valAcc: 78.12% AUC: 65.95%
Epoch: 55 Loss: 0.0251 valLoss: 0.3013 valAcc: 77.11% AUC: 66.43%
Epoch: 56 Loss: 0.0245 valLoss: 0.3174 valAcc: 78.67% AUC: 66.22%
Epoch: 57 Loss: 0.0242 valLoss: 0.3066 valAcc: 77.68% AUC: 65.76%
Epoch: 58 Loss: 0.0236 valLoss: 0.3198 valAcc: 78.88% AUC: 65.68%
Epoch: 59 Loss: 0.0224 valLoss: 0.3299 valAcc: 79.31% AUC: 65.44%
Epoch: 60 Loss: 0.0220 valLoss: 0.3191 valAcc: 77.90% AUC: 65.42%
Epoch: 61 Loss: 0.0209 valLoss: 0.3079 valAcc: 76.90% AUC: 65.40%
Epoch: 62 Loss: 0.0211 valLoss: 0.3215 valAcc: 78.42% AUC: 65.43%
Epoch: 63 Loss: 0.0205 valLoss: 0.3274 valAcc: 77.36% AUC: 64.89%
Epoch: 64 Loss: 0.0190 valLoss: 0.3484 valAcc: 79.51% AUC: 65.27%
Epoch: 65 Loss: 0.0191 valLoss: 0.3526 valAcc: 78.68% AUC: 65.06%
Epoch: 66 Loss: 0.0185 valLoss: 0.3572 valAcc: 79.38% AUC: 64.91%
Epoch: 67 Loss: 0.0177 valLoss: 0.3495 valAcc: 78.78% AUC: 65.18%
Epoch: 68 Loss: 0.0173 valLoss: 0.3452 valAcc: 77.34% AUC: 64.83%
Epoch: 69 Loss: 0.0173 valLoss: 0.3671 valAcc: 78.83% AUC: 64.33%
Epoch: 70 Loss: 0.0167 valLoss: 0.3666 valAcc: 78.64% AUC: 64.66%
Epoch: 71 Loss: 0.0163 valLoss: 0.3661 valAcc: 78.42% AUC: 64.73%
Epoch: 72 Loss: 0.0163 valLoss: 0.3705 valAcc: 78.00% AUC: 64.71%
Epoch: 73 Loss: 0.0157 valLoss: 0.3785 valAcc: 79.41% AUC: 65.02%
Epoch: 74 Loss: 0.0154 valLoss: 0.3609 valAcc: 77.86% AUC: 65.10%
Epoch: 75 Loss: 0.0152 valLoss: 0.3875 valAcc: 80.21% AUC: 65.05%
Epoch: 76 Loss: 0.0147 valLoss: 0.3607 valAcc: 78.48% AUC: 65.13%
Epoch: 77 Loss: 0.0147 valLoss: 0.3458 valAcc: 76.77% AUC: 65.31%
Epoch: 78 Loss: 0.0145 valLoss: 0.3504 valAcc: 75.57% AUC: 64.29%
Epoch: 79 Loss: 0.0148 valLoss: 0.3569 valAcc: 77.66% AUC: 65.78%
Epoch: 80 Loss: 0.0148 valLoss: 0.3645 valAcc: 76.90% AUC: 64.64%
Epoch: 81 Loss: 0.0142 valLoss: 0.3841 valAcc: 78.40% AUC: 64.47%
Epoch: 82 Loss: 0.0141 valLoss: 0.3525 valAcc: 75.03% AUC: 65.19%
Epoch: 83 Loss: 0.0139 valLoss: 0.3821 valAcc: 77.00% AUC: 64.51%
Epoch: 84 Loss: 0.0140 valLoss: 0.3648 valAcc: 76.14% AUC: 64.82%
Epoch: 85 Loss: 0.0148 valLoss: 0.3752 valAcc: 77.27% AUC: 65.15%
Epoch: 86 Loss: 0.0132 valLoss: 0.3913 valAcc: 78.26% AUC: 65.43%
Epoch: 87 Loss: 0.0118 valLoss: 0.4121 valAcc: 79.68% AUC: 65.09%
Epoch: 88 Loss: 0.0119 valLoss: 0.4066 valAcc: 79.53% AUC: 65.10%
Epoch: 89 Loss: 0.0106 valLoss: 0.4451 valAcc: 81.92% AUC: 65.48%
Epoch: 90 Loss: 0.0103 valLoss: 0.4092 valAcc: 79.97% AUC: 65.61%
Epoch: 91 Loss: 0.0102 valLoss: 0.4278 valAcc: 80.49% AUC: 65.16%
Epoch: 92 Loss: 0.0097 valLoss: 0.4356 valAcc: 80.70% AUC: 64.81%
Epoch: 93 Loss: 0.0088 valLoss: 0.4220 valAcc: 79.98% AUC: 65.33%
Restoring model weights from the best epoch: 13 with the best VAL_AUC: 68.28%
Training time: 0:24:25.252765
## Testing ../data/prepared_epitope_testing.csv at: 20-12-2023#12:59:41 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.100789
The best cut-off value is: 0.69
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15070  1055]
 [ 1055   330]]
ValAcc: 87.95% specScore: 93.46% presScore: 23.83% recallScore: 23.83% F1Score: 23.83% MCC: 17.28% AUC: 72.08% AP: 18.38%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 20-12-2023#14:07:53 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1651 valLoss: 0.1325 valAcc: 11.32% AUC: 56.09%
Epoch: 01 Loss: 0.1458 valLoss: 0.1323 valAcc: 11.32% AUC: 57.11%
Epoch: 02 Loss: 0.1398 valLoss: 0.1322 valAcc: 11.33% AUC: 58.73%
Epoch: 03 Loss: 0.1352 valLoss: 0.1319 valAcc: 11.44% AUC: 60.05%
Epoch: 04 Loss: 0.1322 valLoss: 0.1313 valAcc: 11.51% AUC: 61.54%
Epoch: 05 Loss: 0.1291 valLoss: 0.1302 valAcc: 12.22% AUC: 63.72%
Epoch: 06 Loss: 0.1267 valLoss: 0.1293 valAcc: 13.64% AUC: 64.85%
Epoch: 07 Loss: 0.1242 valLoss: 0.1292 valAcc: 15.29% AUC: 65.53%
Epoch: 08 Loss: 0.1219 valLoss: 0.1291 valAcc: 16.99% AUC: 66.15%
Epoch: 09 Loss: 0.1198 valLoss: 0.1273 valAcc: 22.48% AUC: 66.63%
Epoch: 10 Loss: 0.1179 valLoss: 0.1259 valAcc: 26.45% AUC: 66.94%
Epoch: 11 Loss: 0.1157 valLoss: 0.1253 valAcc: 30.82% AUC: 67.35%
Epoch: 12 Loss: 0.1138 valLoss: 0.1260 valAcc: 30.22% AUC: 66.89%
Epoch: 13 Loss: 0.1100 valLoss: 0.1260 valAcc: 39.14% AUC: 67.18%
Epoch: 14 Loss: 0.1079 valLoss: 0.1260 valAcc: 41.18% AUC: 67.58%
Epoch: 15 Loss: 0.1053 valLoss: 0.1264 valAcc: 38.93% AUC: 67.49%
Epoch: 16 Loss: 0.1022 valLoss: 0.1289 valAcc: 44.35% AUC: 67.33%
Epoch: 17 Loss: 0.0987 valLoss: 0.1356 valAcc: 55.47% AUC: 68.02%
Epoch: 18 Loss: 0.0960 valLoss: 0.1334 valAcc: 49.63% AUC: 67.46%
Epoch: 19 Loss: 0.0939 valLoss: 0.1410 valAcc: 56.13% AUC: 67.59%
Epoch: 20 Loss: 0.0906 valLoss: 0.1400 valAcc: 51.28% AUC: 67.15%
Epoch: 21 Loss: 0.0876 valLoss: 0.1490 valAcc: 57.02% AUC: 67.27%
Epoch: 22 Loss: 0.0842 valLoss: 0.1677 valAcc: 65.09% AUC: 67.93%
Epoch: 23 Loss: 0.0812 valLoss: 0.1651 valAcc: 63.25% AUC: 67.38%
Epoch: 24 Loss: 0.0783 valLoss: 0.1973 valAcc: 70.51% AUC: 67.86%
Epoch: 25 Loss: 0.0756 valLoss: 0.1944 valAcc: 69.29% AUC: 67.29%
Epoch: 26 Loss: 0.0740 valLoss: 0.1802 valAcc: 65.10% AUC: 67.41%
Epoch: 27 Loss: 0.0695 valLoss: 0.1978 valAcc: 69.55% AUC: 67.80%
Epoch: 28 Loss: 0.0678 valLoss: 0.2139 valAcc: 71.68% AUC: 67.65%
Epoch: 29 Loss: 0.0667 valLoss: 0.2027 valAcc: 70.52% AUC: 67.86%
Epoch: 30 Loss: 0.0631 valLoss: 0.2068 valAcc: 69.64% AUC: 67.70%
Epoch: 31 Loss: 0.0611 valLoss: 0.2232 valAcc: 72.30% AUC: 67.67%
Epoch: 32 Loss: 0.0582 valLoss: 0.2294 valAcc: 72.74% AUC: 67.49%
Epoch: 33 Loss: 0.0569 valLoss: 0.2189 valAcc: 71.42% AUC: 67.86%
Epoch: 34 Loss: 0.0554 valLoss: 0.2285 valAcc: 73.16% AUC: 67.83%
Epoch: 35 Loss: 0.0525 valLoss: 0.2652 valAcc: 75.79% AUC: 67.65%
Epoch: 36 Loss: 0.0504 valLoss: 0.2614 valAcc: 75.56% AUC: 68.02%
Epoch: 37 Loss: 0.0490 valLoss: 0.2616 valAcc: 74.33% AUC: 67.59%
Epoch: 38 Loss: 0.0483 valLoss: 0.2604 valAcc: 73.96% AUC: 67.70%
Epoch: 39 Loss: 0.0465 valLoss: 0.2434 valAcc: 72.03% AUC: 67.75%
Epoch: 40 Loss: 0.0439 valLoss: 0.2258 valAcc: 70.46% AUC: 67.99%
Epoch: 41 Loss: 0.0430 valLoss: 0.2358 valAcc: 70.81% AUC: 67.93%
Epoch: 42 Loss: 0.0418 valLoss: 0.2509 valAcc: 72.02% AUC: 67.85%
Epoch: 43 Loss: 0.0405 valLoss: 0.2783 valAcc: 73.70% AUC: 67.37%
Epoch: 44 Loss: 0.0404 valLoss: 0.2328 valAcc: 69.00% AUC: 67.84%
Epoch: 45 Loss: 0.0411 valLoss: 0.2219 valAcc: 68.33% AUC: 68.01%
Epoch: 46 Loss: 0.0411 valLoss: 0.2154 valAcc: 66.43% AUC: 67.43%
Epoch: 47 Loss: 0.0403 valLoss: 0.2766 valAcc: 73.73% AUC: 67.82%
Epoch: 48 Loss: 0.0360 valLoss: 0.3093 valAcc: 76.36% AUC: 68.10%
Epoch: 49 Loss: 0.0323 valLoss: 0.3439 valAcc: 79.32% AUC: 68.30%
Epoch: 50 Loss: 0.0316 valLoss: 0.3133 valAcc: 76.65% AUC: 68.08%
Epoch: 51 Loss: 0.0302 valLoss: 0.2998 valAcc: 75.96% AUC: 67.89%
Epoch: 52 Loss: 0.0292 valLoss: 0.3070 valAcc: 76.17% AUC: 68.17%
Epoch: 53 Loss: 0.0293 valLoss: 0.3002 valAcc: 74.85% AUC: 67.84%
Epoch: 54 Loss: 0.0280 valLoss: 0.3312 valAcc: 77.76% AUC: 67.51%
Epoch: 55 Loss: 0.0267 valLoss: 0.3157 valAcc: 76.66% AUC: 67.59%
Epoch: 56 Loss: 0.0260 valLoss: 0.3479 valAcc: 78.76% AUC: 67.64%
Epoch: 57 Loss: 0.0246 valLoss: 0.3447 valAcc: 78.32% AUC: 67.52%
Epoch: 58 Loss: 0.0244 valLoss: 0.3468 valAcc: 78.12% AUC: 67.53%
Epoch: 59 Loss: 0.0237 valLoss: 0.3539 valAcc: 78.23% AUC: 67.15%
Epoch: 60 Loss: 0.0240 valLoss: 0.3579 valAcc: 78.41% AUC: 66.99%
Epoch: 61 Loss: 0.0228 valLoss: 0.3418 valAcc: 76.93% AUC: 67.42%
Epoch: 62 Loss: 0.0224 valLoss: 0.3638 valAcc: 78.89% AUC: 67.66%
Epoch: 63 Loss: 0.0212 valLoss: 0.3738 valAcc: 80.00% AUC: 67.70%
Epoch: 64 Loss: 0.0213 valLoss: 0.4053 valAcc: 80.82% AUC: 67.21%
Epoch: 65 Loss: 0.0200 valLoss: 0.3853 valAcc: 78.99% AUC: 67.13%
Epoch: 66 Loss: 0.0202 valLoss: 0.3900 valAcc: 79.89% AUC: 67.49%
Epoch: 67 Loss: 0.0192 valLoss: 0.3813 valAcc: 79.35% AUC: 67.33%
Epoch: 68 Loss: 0.0187 valLoss: 0.4030 valAcc: 80.36% AUC: 67.38%
Epoch: 69 Loss: 0.0177 valLoss: 0.4079 valAcc: 80.30% AUC: 67.54%
Epoch: 70 Loss: 0.0181 valLoss: 0.3891 valAcc: 78.79% AUC: 67.17%
Epoch: 71 Loss: 0.0170 valLoss: 0.3989 valAcc: 79.57% AUC: 67.46%
Epoch: 72 Loss: 0.0172 valLoss: 0.3812 valAcc: 78.60% AUC: 67.11%
Epoch: 73 Loss: 0.0167 valLoss: 0.3894 valAcc: 78.91% AUC: 67.11%
Epoch: 74 Loss: 0.0155 valLoss: 0.4185 valAcc: 80.05% AUC: 66.89%
Epoch: 75 Loss: 0.0158 valLoss: 0.4230 valAcc: 79.94% AUC: 66.63%
Epoch: 76 Loss: 0.0157 valLoss: 0.4055 valAcc: 78.92% AUC: 67.05%
Epoch: 77 Loss: 0.0156 valLoss: 0.4016 valAcc: 78.30% AUC: 66.96%
Epoch: 78 Loss: 0.0152 valLoss: 0.4192 valAcc: 79.37% AUC: 66.80%
Epoch: 79 Loss: 0.0153 valLoss: 0.3918 valAcc: 77.96% AUC: 67.01%
Epoch: 80 Loss: 0.0151 valLoss: 0.3843 valAcc: 76.90% AUC: 66.77%
Epoch: 81 Loss: 0.0146 valLoss: 0.4040 valAcc: 78.34% AUC: 67.05%
Epoch: 82 Loss: 0.0150 valLoss: 0.3895 valAcc: 77.46% AUC: 67.00%
Epoch: 83 Loss: 0.0145 valLoss: 0.3761 valAcc: 75.71% AUC: 66.66%
Epoch: 84 Loss: 0.0146 valLoss: 0.4164 valAcc: 78.41% AUC: 66.97%
Epoch: 85 Loss: 0.0130 valLoss: 0.4206 valAcc: 78.86% AUC: 67.16%
Epoch: 86 Loss: 0.0120 valLoss: 0.4166 valAcc: 78.68% AUC: 67.44%
Epoch: 87 Loss: 0.0114 valLoss: 0.4424 valAcc: 79.94% AUC: 66.70%
Epoch: 88 Loss: 0.0113 valLoss: 0.4109 valAcc: 78.72% AUC: 67.53%
Epoch: 89 Loss: 0.0110 valLoss: 0.4329 valAcc: 79.75% AUC: 67.32%
Epoch: 90 Loss: 0.0111 valLoss: 0.4508 valAcc: 80.30% AUC: 67.25%
Epoch: 91 Loss: 0.0101 valLoss: 0.4524 valAcc: 80.33% AUC: 67.02%
Epoch: 92 Loss: 0.0101 valLoss: 0.4628 valAcc: 80.87% AUC: 67.21%
Epoch: 93 Loss: 0.0103 valLoss: 0.4271 valAcc: 78.65% AUC: 67.09%
Epoch: 94 Loss: 0.0106 valLoss: 0.4294 valAcc: 79.02% AUC: 67.39%
Epoch: 95 Loss: 0.0101 valLoss: 0.4607 valAcc: 80.30% AUC: 66.94%
Epoch: 96 Loss: 0.0095 valLoss: 0.4467 valAcc: 80.05% AUC: 67.11%
Epoch: 97 Loss: 0.0095 valLoss: 0.4640 valAcc: 80.71% AUC: 66.78%
Epoch: 98 Loss: 0.0091 valLoss: 0.4437 valAcc: 79.11% AUC: 66.65%
Epoch: 99 Loss: 0.0090 valLoss: 0.4564 valAcc: 79.83% AUC: 66.98%
Epoch: 100 Loss: 0.0092 valLoss: 0.4718 valAcc: 80.64% AUC: 66.83%
Epoch: 101 Loss: 0.0090 valLoss: 0.4919 valAcc: 80.94% AUC: 66.82%
Epoch: 102 Loss: 0.0091 valLoss: 0.4700 valAcc: 80.34% AUC: 66.60%
Epoch: 103 Loss: 0.0095 valLoss: 0.4885 valAcc: 81.25% AUC: 66.91%
Epoch: 104 Loss: 0.0088 valLoss: 0.4992 valAcc: 81.24% AUC: 66.56%
Epoch: 105 Loss: 0.0092 valLoss: 0.4788 valAcc: 80.29% AUC: 66.90%
Epoch: 106 Loss: 0.0081 valLoss: 0.4728 valAcc: 80.02% AUC: 66.54%
Epoch: 107 Loss: 0.0081 valLoss: 0.4652 valAcc: 79.80% AUC: 66.34%
Epoch: 108 Loss: 0.0083 valLoss: 0.4686 valAcc: 79.55% AUC: 66.45%
Epoch: 109 Loss: 0.0079 valLoss: 0.4728 valAcc: 79.71% AUC: 66.58%
Epoch: 110 Loss: 0.0077 valLoss: 0.4798 valAcc: 79.75% AUC: 66.44%
Epoch: 111 Loss: 0.0071 valLoss: 0.5010 valAcc: 80.21% AUC: 66.28%
Epoch: 112 Loss: 0.0072 valLoss: 0.4875 valAcc: 79.55% AUC: 66.19%
Epoch: 113 Loss: 0.0068 valLoss: 0.5260 valAcc: 81.32% AUC: 66.32%
Epoch: 114 Loss: 0.0066 valLoss: 0.5012 valAcc: 80.50% AUC: 66.54%
Epoch: 115 Loss: 0.0064 valLoss: 0.5094 valAcc: 80.72% AUC: 66.46%
Epoch: 116 Loss: 0.0062 valLoss: 0.4915 valAcc: 79.87% AUC: 66.75%
Epoch: 117 Loss: 0.0063 valLoss: 0.5068 valAcc: 80.88% AUC: 66.41%
Epoch: 118 Loss: 0.0063 valLoss: 0.5241 valAcc: 81.54% AUC: 66.21%
Epoch: 119 Loss: 0.0057 valLoss: 0.5296 valAcc: 81.65% AUC: 66.62%
Epoch: 120 Loss: 0.0068 valLoss: 0.5265 valAcc: 81.36% AUC: 66.59%
Epoch: 121 Loss: 0.0062 valLoss: 0.5280 valAcc: 80.83% AUC: 66.12%
Epoch: 122 Loss: 0.0065 valLoss: 0.5292 valAcc: 81.38% AUC: 66.17%
Epoch: 123 Loss: 0.0058 valLoss: 0.5382 valAcc: 81.47% AUC: 66.60%
Epoch: 124 Loss: 0.0054 valLoss: 0.5629 valAcc: 81.95% AUC: 65.89%
Epoch: 125 Loss: 0.0057 valLoss: 0.5381 valAcc: 81.10% AUC: 66.13%
Epoch: 126 Loss: 0.0052 valLoss: 0.5452 valAcc: 81.39% AUC: 66.11%
Epoch: 127 Loss: 0.0061 valLoss: 0.5480 valAcc: 81.43% AUC: 66.00%
Epoch: 128 Loss: 0.0061 valLoss: 0.4907 valAcc: 79.35% AUC: 66.27%
Epoch: 129 Loss: 0.0057 valLoss: 0.5319 valAcc: 81.43% AUC: 65.97%
Restoring model weights from the best epoch: 49 with the best VAL_AUC: 68.30%
Training time: 0:32:28.575630
## Testing ../data/prepared_epitope_testing.csv at: 20-12-2023#14:41:42 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.098363
The best cut-off value is: 0.52
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15021  1104]
 [ 1104   281]]
ValAcc: 87.39% specScore: 93.15% presScore: 20.29% recallScore: 20.29% F1Score: 20.29% MCC: 13.44% AUC: 70.77% AP: 16.64%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-12-2023#10:44:13 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.2576 valLoss: 0.1201 valAcc: 8.29% AUC: 52.71%
Epoch: 01 Loss: 0.1987 valLoss: 0.1178 valAcc: 8.29% AUC: 56.59%
Epoch: 02 Loss: 0.1683 valLoss: 0.1166 valAcc: 8.32% AUC: 58.12%
Epoch: 03 Loss: 0.1499 valLoss: 0.1162 valAcc: 8.36% AUC: 58.02%
Epoch: 04 Loss: 0.1379 valLoss: 0.1159 valAcc: 8.43% AUC: 57.63%
Epoch: 05 Loss: 0.1278 valLoss: 0.1159 valAcc: 8.60% AUC: 57.15%
Epoch: 06 Loss: 0.1273 valLoss: 0.1159 valAcc: 8.77% AUC: 56.67%
Epoch: 07 Loss: 0.1229 valLoss: 0.1159 valAcc: 9.02% AUC: 56.58%
Epoch: 08 Loss: 0.1239 valLoss: 0.1159 valAcc: 9.19% AUC: 56.67%
Epoch: 09 Loss: 0.1237 valLoss: 0.1158 valAcc: 9.29% AUC: 56.79%
Epoch: 10 Loss: 0.1216 valLoss: 0.1158 valAcc: 9.36% AUC: 56.98%
Epoch: 11 Loss: 0.1146 valLoss: 0.1157 valAcc: 9.50% AUC: 57.29%
Epoch: 12 Loss: 0.1162 valLoss: 0.1156 valAcc: 9.40% AUC: 57.43%
Epoch: 13 Loss: 0.1108 valLoss: 0.1155 valAcc: 9.43% AUC: 57.60%
Epoch: 14 Loss: 0.1148 valLoss: 0.1154 valAcc: 9.29% AUC: 57.89%
Epoch: 15 Loss: 0.1078 valLoss: 0.1152 valAcc: 9.22% AUC: 58.23%
Epoch: 16 Loss: 0.1088 valLoss: 0.1151 valAcc: 9.15% AUC: 58.49%
Epoch: 17 Loss: 0.1069 valLoss: 0.1150 valAcc: 9.15% AUC: 58.89%
Epoch: 18 Loss: 0.1036 valLoss: 0.1150 valAcc: 9.12% AUC: 59.16%
Epoch: 19 Loss: 0.1016 valLoss: 0.1149 valAcc: 9.08% AUC: 59.36%
Epoch: 20 Loss: 0.1024 valLoss: 0.1149 valAcc: 9.08% AUC: 59.35%
Epoch: 21 Loss: 0.1001 valLoss: 0.1148 valAcc: 9.22% AUC: 59.31%
Epoch: 22 Loss: 0.0965 valLoss: 0.1148 valAcc: 9.40% AUC: 59.29%
Epoch: 23 Loss: 0.1015 valLoss: 0.1148 valAcc: 9.60% AUC: 59.27%
Epoch: 24 Loss: 0.0943 valLoss: 0.1147 valAcc: 9.91% AUC: 59.23%
Epoch: 25 Loss: 0.0921 valLoss: 0.1147 valAcc: 10.36% AUC: 59.25%
Epoch: 26 Loss: 0.0889 valLoss: 0.1146 valAcc: 10.98% AUC: 59.27%
Epoch: 27 Loss: 0.0918 valLoss: 0.1146 valAcc: 11.74% AUC: 59.31%
Epoch: 28 Loss: 0.0861 valLoss: 0.1145 valAcc: 12.50% AUC: 59.46%
Epoch: 29 Loss: 0.0887 valLoss: 0.1144 valAcc: 12.95% AUC: 59.61%
Epoch: 30 Loss: 0.0853 valLoss: 0.1143 valAcc: 13.54% AUC: 59.75%
Epoch: 31 Loss: 0.0857 valLoss: 0.1143 valAcc: 14.13% AUC: 59.93%
Epoch: 32 Loss: 0.0845 valLoss: 0.1142 valAcc: 15.34% AUC: 60.06%
Epoch: 33 Loss: 0.0823 valLoss: 0.1141 valAcc: 16.27% AUC: 60.17%
Epoch: 34 Loss: 0.0824 valLoss: 0.1140 valAcc: 17.10% AUC: 60.32%
Epoch: 35 Loss: 0.0828 valLoss: 0.1140 valAcc: 18.00% AUC: 60.43%
Epoch: 36 Loss: 0.0764 valLoss: 0.1139 valAcc: 19.00% AUC: 60.42%
Epoch: 37 Loss: 0.0738 valLoss: 0.1139 valAcc: 20.00% AUC: 60.42%
Epoch: 38 Loss: 0.0720 valLoss: 0.1139 valAcc: 20.90% AUC: 60.37%
Epoch: 39 Loss: 0.0739 valLoss: 0.1138 valAcc: 21.90% AUC: 60.40%
Epoch: 40 Loss: 0.0730 valLoss: 0.1138 valAcc: 22.97% AUC: 60.42%
Epoch: 41 Loss: 0.0680 valLoss: 0.1138 valAcc: 24.49% AUC: 60.42%
Epoch: 42 Loss: 0.0712 valLoss: 0.1138 valAcc: 25.63% AUC: 60.36%
Epoch: 43 Loss: 0.0705 valLoss: 0.1137 valAcc: 26.53% AUC: 60.38%
Epoch: 44 Loss: 0.0662 valLoss: 0.1137 valAcc: 27.98% AUC: 60.40%
Epoch: 45 Loss: 0.0673 valLoss: 0.1137 valAcc: 30.09% AUC: 60.41%
Epoch: 46 Loss: 0.0642 valLoss: 0.1136 valAcc: 31.19% AUC: 60.44%
Epoch: 47 Loss: 0.0634 valLoss: 0.1135 valAcc: 32.78% AUC: 60.51%
Epoch: 48 Loss: 0.0593 valLoss: 0.1135 valAcc: 34.09% AUC: 60.59%
Epoch: 49 Loss: 0.0614 valLoss: 0.1135 valAcc: 35.03% AUC: 60.59%
Epoch: 50 Loss: 0.0549 valLoss: 0.1135 valAcc: 36.37% AUC: 60.51%
Epoch: 51 Loss: 0.0622 valLoss: 0.1136 valAcc: 36.99% AUC: 60.47%
Epoch: 52 Loss: 0.0562 valLoss: 0.1136 valAcc: 37.58% AUC: 60.46%
Epoch: 53 Loss: 0.0538 valLoss: 0.1136 valAcc: 38.17% AUC: 60.47%
Epoch: 54 Loss: 0.0585 valLoss: 0.1136 valAcc: 38.86% AUC: 60.50%
Epoch: 55 Loss: 0.0578 valLoss: 0.1136 valAcc: 39.79% AUC: 60.47%
Epoch: 56 Loss: 0.0515 valLoss: 0.1136 valAcc: 40.55% AUC: 60.46%
Epoch: 57 Loss: 0.0498 valLoss: 0.1137 valAcc: 41.62% AUC: 60.50%
Epoch: 58 Loss: 0.0516 valLoss: 0.1137 valAcc: 42.45% AUC: 60.55%
Epoch: 59 Loss: 0.0515 valLoss: 0.1137 valAcc: 43.25% AUC: 60.53%
Epoch: 60 Loss: 0.0479 valLoss: 0.1138 valAcc: 44.46% AUC: 60.50%
Epoch: 61 Loss: 0.0483 valLoss: 0.1138 valAcc: 45.25% AUC: 60.49%
Epoch: 62 Loss: 0.0497 valLoss: 0.1138 valAcc: 46.04% AUC: 60.51%
Epoch: 63 Loss: 0.0460 valLoss: 0.1139 valAcc: 46.46% AUC: 60.45%
Epoch: 64 Loss: 0.0449 valLoss: 0.1140 valAcc: 47.22% AUC: 60.33%
Epoch: 65 Loss: 0.0454 valLoss: 0.1140 valAcc: 48.15% AUC: 60.29%
Epoch: 66 Loss: 0.0431 valLoss: 0.1141 valAcc: 48.91% AUC: 60.31%
Epoch: 67 Loss: 0.0431 valLoss: 0.1141 valAcc: 49.60% AUC: 60.33%
Epoch: 68 Loss: 0.0438 valLoss: 0.1141 valAcc: 50.16% AUC: 60.43%
Epoch: 69 Loss: 0.0414 valLoss: 0.1140 valAcc: 50.85% AUC: 60.49%
Epoch: 70 Loss: 0.0401 valLoss: 0.1140 valAcc: 51.40% AUC: 60.55%
Epoch: 71 Loss: 0.0404 valLoss: 0.1140 valAcc: 52.64% AUC: 60.66%
Epoch: 72 Loss: 0.0395 valLoss: 0.1140 valAcc: 54.37% AUC: 60.81%
Epoch: 73 Loss: 0.0381 valLoss: 0.1141 valAcc: 56.23% AUC: 60.96%
Epoch: 74 Loss: 0.0384 valLoss: 0.1141 valAcc: 57.31% AUC: 61.11%
Epoch: 75 Loss: 0.0386 valLoss: 0.1141 valAcc: 58.17% AUC: 61.22%
Epoch: 76 Loss: 0.0363 valLoss: 0.1142 valAcc: 58.96% AUC: 61.33%
Epoch: 77 Loss: 0.0363 valLoss: 0.1142 valAcc: 59.48% AUC: 61.41%
Epoch: 78 Loss: 0.0366 valLoss: 0.1142 valAcc: 59.72% AUC: 61.43%
Epoch: 79 Loss: 0.0344 valLoss: 0.1143 valAcc: 59.97% AUC: 61.47%
Epoch: 80 Loss: 0.0338 valLoss: 0.1144 valAcc: 60.79% AUC: 61.50%
Epoch: 81 Loss: 0.0348 valLoss: 0.1145 valAcc: 61.69% AUC: 61.53%
Epoch: 82 Loss: 0.0313 valLoss: 0.1146 valAcc: 63.04% AUC: 61.63%
Epoch: 83 Loss: 0.0321 valLoss: 0.1147 valAcc: 64.25% AUC: 61.74%
Epoch: 84 Loss: 0.0322 valLoss: 0.1147 valAcc: 65.42% AUC: 61.90%
Epoch: 85 Loss: 0.0315 valLoss: 0.1148 valAcc: 66.77% AUC: 62.00%
Epoch: 86 Loss: 0.0282 valLoss: 0.1151 valAcc: 67.88% AUC: 62.01%
Epoch: 87 Loss: 0.0294 valLoss: 0.1154 valAcc: 68.64% AUC: 61.98%
Epoch: 88 Loss: 0.0299 valLoss: 0.1155 valAcc: 68.84% AUC: 61.93%
Epoch: 89 Loss: 0.0259 valLoss: 0.1158 valAcc: 69.81% AUC: 61.92%
Epoch: 90 Loss: 0.0291 valLoss: 0.1159 valAcc: 70.71% AUC: 61.97%
Epoch: 91 Loss: 0.0286 valLoss: 0.1161 valAcc: 71.16% AUC: 61.96%
Epoch: 92 Loss: 0.0285 valLoss: 0.1163 valAcc: 71.50% AUC: 61.93%
Epoch: 93 Loss: 0.0273 valLoss: 0.1165 valAcc: 71.88% AUC: 61.86%
Epoch: 94 Loss: 0.0258 valLoss: 0.1168 valAcc: 72.64% AUC: 61.80%
Epoch: 95 Loss: 0.0247 valLoss: 0.1172 valAcc: 73.51% AUC: 61.86%
Epoch: 96 Loss: 0.0269 valLoss: 0.1173 valAcc: 74.44% AUC: 62.06%
Epoch: 97 Loss: 0.0245 valLoss: 0.1173 valAcc: 75.13% AUC: 62.33%
Epoch: 98 Loss: 0.0242 valLoss: 0.1174 valAcc: 75.82% AUC: 62.48%
Epoch: 99 Loss: 0.0248 valLoss: 0.1177 valAcc: 76.37% AUC: 62.58%
Epoch: 100 Loss: 0.0228 valLoss: 0.1180 valAcc: 77.13% AUC: 62.71%
Epoch: 101 Loss: 0.0228 valLoss: 0.1182 valAcc: 77.48% AUC: 62.83%
Epoch: 102 Loss: 0.0238 valLoss: 0.1184 valAcc: 77.96% AUC: 62.88%
Epoch: 103 Loss: 0.0225 valLoss: 0.1188 valAcc: 78.45% AUC: 62.89%
Epoch: 104 Loss: 0.0222 valLoss: 0.1191 valAcc: 78.89% AUC: 62.86%
Epoch: 105 Loss: 0.0223 valLoss: 0.1194 valAcc: 79.27% AUC: 62.89%
Epoch: 106 Loss: 0.0198 valLoss: 0.1197 valAcc: 79.76% AUC: 62.89%
Epoch: 107 Loss: 0.0202 valLoss: 0.1202 valAcc: 80.10% AUC: 62.86%
Epoch: 108 Loss: 0.0200 valLoss: 0.1204 valAcc: 80.45% AUC: 62.96%
Epoch: 109 Loss: 0.0201 valLoss: 0.1208 valAcc: 80.90% AUC: 63.03%
Epoch: 110 Loss: 0.0197 valLoss: 0.1214 valAcc: 81.35% AUC: 63.02%
Epoch: 111 Loss: 0.0197 valLoss: 0.1219 valAcc: 81.59% AUC: 62.97%
Epoch: 112 Loss: 0.0208 valLoss: 0.1223 valAcc: 81.80% AUC: 62.86%
Epoch: 113 Loss: 0.0170 valLoss: 0.1229 valAcc: 82.11% AUC: 62.75%
Epoch: 114 Loss: 0.0180 valLoss: 0.1233 valAcc: 82.56% AUC: 62.74%
Epoch: 115 Loss: 0.0175 valLoss: 0.1239 valAcc: 83.11% AUC: 62.72%
Epoch: 116 Loss: 0.0181 valLoss: 0.1244 valAcc: 83.56% AUC: 62.73%
Epoch: 117 Loss: 0.0169 valLoss: 0.1249 valAcc: 83.90% AUC: 62.76%
Epoch: 118 Loss: 0.0172 valLoss: 0.1255 valAcc: 84.35% AUC: 62.73%
Epoch: 119 Loss: 0.0163 valLoss: 0.1259 valAcc: 84.53% AUC: 62.69%
Epoch: 120 Loss: 0.0171 valLoss: 0.1262 valAcc: 84.73% AUC: 62.66%
Epoch: 121 Loss: 0.0167 valLoss: 0.1264 valAcc: 84.56% AUC: 62.60%
Epoch: 122 Loss: 0.0152 valLoss: 0.1266 valAcc: 84.59% AUC: 62.64%
Epoch: 123 Loss: 0.0158 valLoss: 0.1268 valAcc: 84.66% AUC: 62.71%
Epoch: 124 Loss: 0.0158 valLoss: 0.1272 valAcc: 84.84% AUC: 62.84%
Epoch: 125 Loss: 0.0144 valLoss: 0.1279 valAcc: 85.39% AUC: 63.01%
Epoch: 126 Loss: 0.0141 valLoss: 0.1285 valAcc: 85.70% AUC: 63.12%
Epoch: 127 Loss: 0.0141 valLoss: 0.1291 valAcc: 85.80% AUC: 63.19%
Epoch: 128 Loss: 0.0163 valLoss: 0.1295 valAcc: 86.08% AUC: 63.23%
Epoch: 129 Loss: 0.0140 valLoss: 0.1298 valAcc: 86.18% AUC: 63.28%
Epoch: 130 Loss: 0.0131 valLoss: 0.1302 valAcc: 86.15% AUC: 63.31%
Epoch: 131 Loss: 0.0140 valLoss: 0.1306 valAcc: 86.36% AUC: 63.32%
Epoch: 132 Loss: 0.0140 valLoss: 0.1310 valAcc: 86.46% AUC: 63.35%
Epoch: 133 Loss: 0.0140 valLoss: 0.1313 valAcc: 86.60% AUC: 63.29%
Epoch: 134 Loss: 0.0131 valLoss: 0.1318 valAcc: 86.60% AUC: 63.26%
Epoch: 135 Loss: 0.0150 valLoss: 0.1322 valAcc: 86.56% AUC: 63.30%
Epoch: 136 Loss: 0.0135 valLoss: 0.1326 valAcc: 86.60% AUC: 63.37%
Epoch: 137 Loss: 0.0125 valLoss: 0.1333 valAcc: 86.77% AUC: 63.39%
Epoch: 138 Loss: 0.0132 valLoss: 0.1341 valAcc: 87.15% AUC: 63.38%
Epoch: 139 Loss: 0.0133 valLoss: 0.1349 valAcc: 87.32% AUC: 63.38%
Epoch: 140 Loss: 0.0128 valLoss: 0.1357 valAcc: 87.63% AUC: 63.36%
Epoch: 141 Loss: 0.0130 valLoss: 0.1361 valAcc: 87.67% AUC: 63.35%
Epoch: 142 Loss: 0.0115 valLoss: 0.1367 valAcc: 87.81% AUC: 63.33%
Epoch: 143 Loss: 0.0114 valLoss: 0.1373 valAcc: 87.98% AUC: 63.38%
Epoch: 144 Loss: 0.0115 valLoss: 0.1378 valAcc: 88.36% AUC: 63.46%
Epoch: 145 Loss: 0.0131 valLoss: 0.1381 valAcc: 88.29% AUC: 63.51%
Epoch: 146 Loss: 0.0107 valLoss: 0.1384 valAcc: 88.32% AUC: 63.58%
Epoch: 147 Loss: 0.0108 valLoss: 0.1388 valAcc: 88.32% AUC: 63.66%
Epoch: 148 Loss: 0.0111 valLoss: 0.1392 valAcc: 88.36% AUC: 63.69%
Epoch: 149 Loss: 0.0110 valLoss: 0.1399 valAcc: 88.36% AUC: 63.66%
Epoch: 150 Loss: 0.0108 valLoss: 0.1403 valAcc: 88.50% AUC: 63.62%
Epoch: 151 Loss: 0.0109 valLoss: 0.1408 valAcc: 88.50% AUC: 63.52%
Epoch: 152 Loss: 0.0107 valLoss: 0.1416 valAcc: 88.57% AUC: 63.40%
Epoch: 153 Loss: 0.0104 valLoss: 0.1424 valAcc: 88.64% AUC: 63.35%
Epoch: 154 Loss: 0.0095 valLoss: 0.1432 valAcc: 88.70% AUC: 63.39%
Epoch: 155 Loss: 0.0102 valLoss: 0.1440 valAcc: 88.74% AUC: 63.42%
Epoch: 156 Loss: 0.0097 valLoss: 0.1447 valAcc: 88.88% AUC: 63.49%
Epoch: 157 Loss: 0.0102 valLoss: 0.1454 valAcc: 89.05% AUC: 63.53%
Epoch: 158 Loss: 0.0093 valLoss: 0.1462 valAcc: 89.08% AUC: 63.57%
Epoch: 159 Loss: 0.0097 valLoss: 0.1471 valAcc: 89.19% AUC: 63.63%
Epoch: 160 Loss: 0.0108 valLoss: 0.1479 valAcc: 89.33% AUC: 63.66%
Epoch: 161 Loss: 0.0090 valLoss: 0.1488 valAcc: 89.40% AUC: 63.67%
Epoch: 162 Loss: 0.0090 valLoss: 0.1496 valAcc: 89.53% AUC: 63.65%
Epoch: 163 Loss: 0.0098 valLoss: 0.1504 valAcc: 89.64% AUC: 63.58%
Epoch: 164 Loss: 0.0103 valLoss: 0.1508 valAcc: 89.57% AUC: 63.58%
Epoch: 165 Loss: 0.0093 valLoss: 0.1515 valAcc: 89.67% AUC: 63.49%
Epoch: 166 Loss: 0.0092 valLoss: 0.1522 valAcc: 89.60% AUC: 63.45%
Epoch: 167 Loss: 0.0087 valLoss: 0.1528 valAcc: 89.74% AUC: 63.46%
Epoch: 168 Loss: 0.0079 valLoss: 0.1536 valAcc: 89.88% AUC: 63.50%
Epoch: 169 Loss: 0.0075 valLoss: 0.1544 valAcc: 89.91% AUC: 63.53%
Epoch: 170 Loss: 0.0099 valLoss: 0.1551 valAcc: 89.98% AUC: 63.61%
Epoch: 171 Loss: 0.0090 valLoss: 0.1557 valAcc: 89.88% AUC: 63.64%
Epoch: 172 Loss: 0.0081 valLoss: 0.1563 valAcc: 89.95% AUC: 63.66%
Epoch: 173 Loss: 0.0085 valLoss: 0.1570 valAcc: 89.98% AUC: 63.67%
Epoch: 174 Loss: 0.0085 valLoss: 0.1575 valAcc: 89.95% AUC: 63.64%
Epoch: 175 Loss: 0.0078 valLoss: 0.1579 valAcc: 89.88% AUC: 63.63%
Epoch: 176 Loss: 0.0071 valLoss: 0.1582 valAcc: 89.91% AUC: 63.63%
Epoch: 177 Loss: 0.0074 valLoss: 0.1585 valAcc: 89.91% AUC: 63.64%
Epoch: 178 Loss: 0.0072 valLoss: 0.1590 valAcc: 89.91% AUC: 63.65%
Epoch: 179 Loss: 0.0085 valLoss: 0.1594 valAcc: 89.88% AUC: 63.60%
Epoch: 180 Loss: 0.0079 valLoss: 0.1600 valAcc: 89.84% AUC: 63.51%
Epoch: 181 Loss: 0.0069 valLoss: 0.1609 valAcc: 89.95% AUC: 63.41%
Epoch: 182 Loss: 0.0073 valLoss: 0.1618 valAcc: 89.88% AUC: 63.35%
Epoch: 183 Loss: 0.0072 valLoss: 0.1626 valAcc: 89.88% AUC: 63.35%
Epoch: 184 Loss: 0.0068 valLoss: 0.1636 valAcc: 89.98% AUC: 63.39%
Epoch: 185 Loss: 0.0071 valLoss: 0.1646 valAcc: 89.95% AUC: 63.43%
Epoch: 186 Loss: 0.0076 valLoss: 0.1658 valAcc: 90.09% AUC: 63.37%
Epoch: 187 Loss: 0.0069 valLoss: 0.1669 valAcc: 90.12% AUC: 63.30%
Epoch: 188 Loss: 0.0071 valLoss: 0.1679 valAcc: 90.22% AUC: 63.26%
Epoch: 189 Loss: 0.0067 valLoss: 0.1687 valAcc: 90.22% AUC: 63.20%
Epoch: 190 Loss: 0.0069 valLoss: 0.1692 valAcc: 90.22% AUC: 63.18%
Epoch: 191 Loss: 0.0067 valLoss: 0.1696 valAcc: 90.22% AUC: 63.17%
Epoch: 192 Loss: 0.0061 valLoss: 0.1699 valAcc: 90.12% AUC: 63.20%
Epoch: 193 Loss: 0.0067 valLoss: 0.1703 valAcc: 90.12% AUC: 63.21%
Epoch: 194 Loss: 0.0064 valLoss: 0.1704 valAcc: 90.19% AUC: 63.30%
Epoch: 195 Loss: 0.0061 valLoss: 0.1709 valAcc: 90.16% AUC: 63.36%
Epoch: 196 Loss: 0.0068 valLoss: 0.1713 valAcc: 90.22% AUC: 63.39%
Epoch: 197 Loss: 0.0062 valLoss: 0.1718 valAcc: 90.19% AUC: 63.35%
Epoch: 198 Loss: 0.0066 valLoss: 0.1720 valAcc: 90.22% AUC: 63.33%
Epoch: 199 Loss: 0.0059 valLoss: 0.1722 valAcc: 90.22% AUC: 63.31%
Epoch: 200 Loss: 0.0064 valLoss: 0.1725 valAcc: 90.22% AUC: 63.28%
Epoch: 201 Loss: 0.0063 valLoss: 0.1725 valAcc: 90.05% AUC: 63.28%
Epoch: 202 Loss: 0.0059 valLoss: 0.1727 valAcc: 89.98% AUC: 63.25%
Epoch: 203 Loss: 0.0060 valLoss: 0.1733 valAcc: 89.98% AUC: 63.22%
Epoch: 204 Loss: 0.0070 valLoss: 0.1741 valAcc: 90.02% AUC: 63.19%
Epoch: 205 Loss: 0.0053 valLoss: 0.1749 valAcc: 90.12% AUC: 63.17%
Epoch: 206 Loss: 0.0055 valLoss: 0.1757 valAcc: 90.16% AUC: 63.14%
Epoch: 207 Loss: 0.0058 valLoss: 0.1762 valAcc: 90.19% AUC: 63.13%
Epoch: 208 Loss: 0.0052 valLoss: 0.1771 valAcc: 90.22% AUC: 63.16%
Epoch: 209 Loss: 0.0056 valLoss: 0.1779 valAcc: 90.29% AUC: 63.19%
Epoch: 210 Loss: 0.0059 valLoss: 0.1787 valAcc: 90.33% AUC: 63.17%
Epoch: 211 Loss: 0.0050 valLoss: 0.1796 valAcc: 90.26% AUC: 63.10%
Epoch: 212 Loss: 0.0054 valLoss: 0.1807 valAcc: 90.29% AUC: 63.02%
Epoch: 213 Loss: 0.0057 valLoss: 0.1817 valAcc: 90.47% AUC: 62.96%
Epoch: 214 Loss: 0.0049 valLoss: 0.1825 valAcc: 90.50% AUC: 62.92%
Epoch: 215 Loss: 0.0049 valLoss: 0.1835 valAcc: 90.57% AUC: 62.91%
Epoch: 216 Loss: 0.0050 valLoss: 0.1846 valAcc: 90.60% AUC: 62.88%
Epoch: 217 Loss: 0.0057 valLoss: 0.1856 valAcc: 90.67% AUC: 62.82%
Epoch: 218 Loss: 0.0050 valLoss: 0.1864 valAcc: 90.64% AUC: 62.81%
Epoch: 219 Loss: 0.0055 valLoss: 0.1870 valAcc: 90.67% AUC: 62.73%
Epoch: 220 Loss: 0.0051 valLoss: 0.1875 valAcc: 90.67% AUC: 62.65%
Epoch: 221 Loss: 0.0048 valLoss: 0.1879 valAcc: 90.64% AUC: 62.62%
Epoch: 222 Loss: 0.0052 valLoss: 0.1884 valAcc: 90.67% AUC: 62.62%
Epoch: 223 Loss: 0.0049 valLoss: 0.1892 valAcc: 90.64% AUC: 62.63%
Epoch: 224 Loss: 0.0047 valLoss: 0.1898 valAcc: 90.67% AUC: 62.66%
Epoch: 225 Loss: 0.0052 valLoss: 0.1901 valAcc: 90.60% AUC: 62.71%
Epoch: 226 Loss: 0.0046 valLoss: 0.1904 valAcc: 90.60% AUC: 62.75%
Epoch: 227 Loss: 0.0043 valLoss: 0.1908 valAcc: 90.57% AUC: 62.81%
Epoch: 228 Loss: 0.0044 valLoss: 0.1913 valAcc: 90.54% AUC: 62.88%
Restoring model weights from the best epoch: 148 with the best VAL_AUC: 63.69%
Training time: 0:04:18.308619

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-12-2023#10:58:02 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1705 valLoss: 0.1189 valAcc: 8.29% AUC: 53.96%
Epoch: 01 Loss: 0.1603 valLoss: 0.1182 valAcc: 8.29% AUC: 55.24%
Epoch: 02 Loss: 0.1587 valLoss: 0.1177 valAcc: 8.29% AUC: 55.76%
Epoch: 03 Loss: 0.1449 valLoss: 0.1172 valAcc: 8.29% AUC: 56.24%
Epoch: 04 Loss: 0.1426 valLoss: 0.1168 valAcc: 8.32% AUC: 56.24%
Epoch: 05 Loss: 0.1339 valLoss: 0.1165 valAcc: 8.32% AUC: 56.62%
Epoch: 06 Loss: 0.1314 valLoss: 0.1163 valAcc: 8.32% AUC: 56.94%
Epoch: 07 Loss: 0.1239 valLoss: 0.1160 valAcc: 8.32% AUC: 57.57%
Epoch: 08 Loss: 0.1221 valLoss: 0.1159 valAcc: 8.53% AUC: 57.94%
Epoch: 09 Loss: 0.1173 valLoss: 0.1158 valAcc: 8.60% AUC: 57.75%
Epoch: 10 Loss: 0.1174 valLoss: 0.1158 valAcc: 8.70% AUC: 57.72%
Epoch: 11 Loss: 0.1159 valLoss: 0.1157 valAcc: 8.84% AUC: 57.96%
Epoch: 12 Loss: 0.1084 valLoss: 0.1156 valAcc: 9.02% AUC: 58.15%
Epoch: 13 Loss: 0.1073 valLoss: 0.1156 valAcc: 9.40% AUC: 57.88%
Epoch: 14 Loss: 0.1015 valLoss: 0.1156 valAcc: 9.78% AUC: 57.40%
Epoch: 15 Loss: 0.1036 valLoss: 0.1157 valAcc: 10.16% AUC: 57.01%
Epoch: 16 Loss: 0.0978 valLoss: 0.1157 valAcc: 10.71% AUC: 56.66%
Epoch: 17 Loss: 0.0939 valLoss: 0.1158 valAcc: 11.50% AUC: 56.12%
Epoch: 18 Loss: 0.0933 valLoss: 0.1158 valAcc: 12.57% AUC: 55.96%
Epoch: 19 Loss: 0.0857 valLoss: 0.1157 valAcc: 13.82% AUC: 56.06%
Epoch: 20 Loss: 0.0875 valLoss: 0.1157 valAcc: 15.44% AUC: 56.14%
Epoch: 21 Loss: 0.0853 valLoss: 0.1157 valAcc: 17.13% AUC: 56.36%
Epoch: 22 Loss: 0.0846 valLoss: 0.1157 valAcc: 19.27% AUC: 56.61%
Epoch: 23 Loss: 0.0811 valLoss: 0.1157 valAcc: 21.55% AUC: 56.87%
Epoch: 24 Loss: 0.0722 valLoss: 0.1157 valAcc: 24.39% AUC: 56.97%
Epoch: 25 Loss: 0.0850 valLoss: 0.1157 valAcc: 26.56% AUC: 57.02%
Epoch: 26 Loss: 0.0771 valLoss: 0.1157 valAcc: 28.19% AUC: 57.10%
Epoch: 27 Loss: 0.0745 valLoss: 0.1157 valAcc: 30.05% AUC: 57.06%
Epoch: 28 Loss: 0.0726 valLoss: 0.1157 valAcc: 31.64% AUC: 57.05%
Epoch: 29 Loss: 0.0794 valLoss: 0.1157 valAcc: 33.23% AUC: 57.26%
Epoch: 30 Loss: 0.0731 valLoss: 0.1156 valAcc: 35.13% AUC: 57.50%
Epoch: 31 Loss: 0.0676 valLoss: 0.1156 valAcc: 36.86% AUC: 57.80%
Epoch: 32 Loss: 0.0669 valLoss: 0.1155 valAcc: 39.52% AUC: 58.18%
Epoch: 33 Loss: 0.0643 valLoss: 0.1153 valAcc: 41.93% AUC: 58.72%
Epoch: 34 Loss: 0.0692 valLoss: 0.1153 valAcc: 42.94% AUC: 58.86%
Epoch: 35 Loss: 0.0566 valLoss: 0.1152 valAcc: 44.04% AUC: 59.09%
Epoch: 36 Loss: 0.0632 valLoss: 0.1152 valAcc: 45.56% AUC: 59.20%
Epoch: 37 Loss: 0.0601 valLoss: 0.1152 valAcc: 47.70% AUC: 59.33%
Epoch: 38 Loss: 0.0578 valLoss: 0.1151 valAcc: 49.84% AUC: 59.60%
Epoch: 39 Loss: 0.0614 valLoss: 0.1151 valAcc: 51.81% AUC: 59.85%
Epoch: 40 Loss: 0.0504 valLoss: 0.1151 valAcc: 54.61% AUC: 60.05%
Epoch: 41 Loss: 0.0529 valLoss: 0.1151 valAcc: 55.65% AUC: 60.25%
Epoch: 42 Loss: 0.0480 valLoss: 0.1152 valAcc: 57.55% AUC: 60.30%
Epoch: 43 Loss: 0.0522 valLoss: 0.1153 valAcc: 58.48% AUC: 60.28%
Epoch: 44 Loss: 0.0499 valLoss: 0.1154 valAcc: 59.34% AUC: 60.23%
Epoch: 45 Loss: 0.0518 valLoss: 0.1155 valAcc: 60.28% AUC: 60.12%
Epoch: 46 Loss: 0.0468 valLoss: 0.1156 valAcc: 61.00% AUC: 60.11%
Epoch: 47 Loss: 0.0480 valLoss: 0.1156 valAcc: 61.69% AUC: 60.11%
Epoch: 48 Loss: 0.0470 valLoss: 0.1156 valAcc: 62.31% AUC: 60.25%
Epoch: 49 Loss: 0.0426 valLoss: 0.1155 valAcc: 63.49% AUC: 60.46%
Epoch: 50 Loss: 0.0465 valLoss: 0.1155 valAcc: 64.35% AUC: 60.68%
Epoch: 51 Loss: 0.0463 valLoss: 0.1154 valAcc: 64.87% AUC: 60.83%
Epoch: 52 Loss: 0.0454 valLoss: 0.1155 valAcc: 65.32% AUC: 60.83%
Epoch: 53 Loss: 0.0409 valLoss: 0.1156 valAcc: 66.22% AUC: 60.86%
Epoch: 54 Loss: 0.0437 valLoss: 0.1156 valAcc: 66.87% AUC: 61.00%
Epoch: 55 Loss: 0.0433 valLoss: 0.1156 valAcc: 67.53% AUC: 61.18%
Epoch: 56 Loss: 0.0401 valLoss: 0.1156 valAcc: 67.98% AUC: 61.45%
Epoch: 57 Loss: 0.0410 valLoss: 0.1155 valAcc: 68.36% AUC: 61.72%
Epoch: 58 Loss: 0.0412 valLoss: 0.1154 valAcc: 68.84% AUC: 61.99%
Epoch: 59 Loss: 0.0412 valLoss: 0.1153 valAcc: 69.08% AUC: 62.16%
Epoch: 60 Loss: 0.0375 valLoss: 0.1153 valAcc: 69.60% AUC: 62.28%
Epoch: 61 Loss: 0.0376 valLoss: 0.1152 valAcc: 69.74% AUC: 62.40%
Epoch: 62 Loss: 0.0367 valLoss: 0.1155 valAcc: 70.26% AUC: 62.33%
Epoch: 63 Loss: 0.0344 valLoss: 0.1157 valAcc: 71.09% AUC: 62.31%
Epoch: 64 Loss: 0.0357 valLoss: 0.1157 valAcc: 71.71% AUC: 62.44%
Epoch: 65 Loss: 0.0314 valLoss: 0.1158 valAcc: 72.47% AUC: 62.65%
Epoch: 66 Loss: 0.0314 valLoss: 0.1159 valAcc: 73.13% AUC: 62.86%
Epoch: 67 Loss: 0.0306 valLoss: 0.1161 valAcc: 73.68% AUC: 62.90%
Epoch: 68 Loss: 0.0350 valLoss: 0.1162 valAcc: 73.58% AUC: 62.83%
Epoch: 69 Loss: 0.0314 valLoss: 0.1164 valAcc: 73.71% AUC: 62.69%
Epoch: 70 Loss: 0.0300 valLoss: 0.1165 valAcc: 73.54% AUC: 62.56%
Epoch: 71 Loss: 0.0303 valLoss: 0.1166 valAcc: 73.64% AUC: 62.35%
Epoch: 72 Loss: 0.0306 valLoss: 0.1168 valAcc: 74.02% AUC: 62.29%
Epoch: 73 Loss: 0.0271 valLoss: 0.1172 valAcc: 74.96% AUC: 62.28%
Epoch: 74 Loss: 0.0279 valLoss: 0.1173 valAcc: 75.61% AUC: 62.50%
Epoch: 75 Loss: 0.0269 valLoss: 0.1174 valAcc: 75.96% AUC: 62.71%
Epoch: 76 Loss: 0.0261 valLoss: 0.1176 valAcc: 76.44% AUC: 62.80%
Epoch: 77 Loss: 0.0269 valLoss: 0.1176 valAcc: 76.27% AUC: 62.86%
Epoch: 78 Loss: 0.0265 valLoss: 0.1175 valAcc: 75.96% AUC: 62.98%
Epoch: 79 Loss: 0.0247 valLoss: 0.1176 valAcc: 76.48% AUC: 63.10%
Epoch: 80 Loss: 0.0251 valLoss: 0.1178 valAcc: 77.03% AUC: 63.21%
Epoch: 81 Loss: 0.0271 valLoss: 0.1182 valAcc: 77.55% AUC: 63.30%
Epoch: 82 Loss: 0.0235 valLoss: 0.1187 valAcc: 78.34% AUC: 63.39%
Epoch: 83 Loss: 0.0232 valLoss: 0.1192 valAcc: 79.45% AUC: 63.40%
Epoch: 84 Loss: 0.0238 valLoss: 0.1196 valAcc: 79.83% AUC: 63.36%
Epoch: 85 Loss: 0.0259 valLoss: 0.1200 valAcc: 79.86% AUC: 63.17%
Epoch: 86 Loss: 0.0219 valLoss: 0.1203 valAcc: 80.17% AUC: 63.04%
Epoch: 87 Loss: 0.0241 valLoss: 0.1207 valAcc: 80.38% AUC: 62.91%
Epoch: 88 Loss: 0.0223 valLoss: 0.1212 valAcc: 80.76% AUC: 62.87%
Epoch: 89 Loss: 0.0190 valLoss: 0.1217 valAcc: 81.28% AUC: 62.86%
Epoch: 90 Loss: 0.0220 valLoss: 0.1223 valAcc: 82.00% AUC: 62.89%
Epoch: 91 Loss: 0.0210 valLoss: 0.1229 valAcc: 82.69% AUC: 62.88%
Epoch: 92 Loss: 0.0209 valLoss: 0.1233 valAcc: 83.11% AUC: 62.92%
Epoch: 93 Loss: 0.0211 valLoss: 0.1234 valAcc: 82.97% AUC: 62.97%
Epoch: 94 Loss: 0.0212 valLoss: 0.1236 valAcc: 83.01% AUC: 62.94%
Epoch: 95 Loss: 0.0182 valLoss: 0.1238 valAcc: 83.07% AUC: 62.97%
Epoch: 96 Loss: 0.0180 valLoss: 0.1241 valAcc: 83.25% AUC: 63.07%
Epoch: 97 Loss: 0.0181 valLoss: 0.1245 valAcc: 83.52% AUC: 63.05%
Epoch: 98 Loss: 0.0206 valLoss: 0.1249 valAcc: 83.83% AUC: 62.98%
Epoch: 99 Loss: 0.0175 valLoss: 0.1251 valAcc: 83.94% AUC: 62.94%
Epoch: 100 Loss: 0.0207 valLoss: 0.1248 valAcc: 83.39% AUC: 63.11%
Epoch: 101 Loss: 0.0191 valLoss: 0.1245 valAcc: 83.04% AUC: 63.19%
Epoch: 102 Loss: 0.0160 valLoss: 0.1247 valAcc: 83.21% AUC: 63.26%
Epoch: 103 Loss: 0.0167 valLoss: 0.1249 valAcc: 83.14% AUC: 63.48%
Epoch: 104 Loss: 0.0200 valLoss: 0.1251 valAcc: 83.28% AUC: 63.55%
Epoch: 105 Loss: 0.0135 valLoss: 0.1258 valAcc: 83.73% AUC: 63.58%
Epoch: 106 Loss: 0.0170 valLoss: 0.1264 valAcc: 83.83% AUC: 63.58%
Epoch: 107 Loss: 0.0157 valLoss: 0.1266 valAcc: 83.90% AUC: 63.49%
Epoch: 108 Loss: 0.0184 valLoss: 0.1266 valAcc: 83.77% AUC: 63.36%
Epoch: 109 Loss: 0.0149 valLoss: 0.1270 valAcc: 83.83% AUC: 63.26%
Epoch: 110 Loss: 0.0156 valLoss: 0.1272 valAcc: 83.56% AUC: 63.27%
Epoch: 111 Loss: 0.0146 valLoss: 0.1275 valAcc: 83.56% AUC: 63.33%
Epoch: 112 Loss: 0.0158 valLoss: 0.1275 valAcc: 83.32% AUC: 63.49%
Epoch: 113 Loss: 0.0140 valLoss: 0.1274 valAcc: 83.07% AUC: 63.66%
Epoch: 114 Loss: 0.0133 valLoss: 0.1274 valAcc: 82.87% AUC: 63.74%
Epoch: 115 Loss: 0.0157 valLoss: 0.1277 valAcc: 83.11% AUC: 63.75%
Epoch: 116 Loss: 0.0151 valLoss: 0.1283 valAcc: 83.28% AUC: 63.68%
Epoch: 117 Loss: 0.0140 valLoss: 0.1288 valAcc: 83.42% AUC: 63.56%
Epoch: 118 Loss: 0.0138 valLoss: 0.1292 valAcc: 83.83% AUC: 63.45%
Epoch: 119 Loss: 0.0123 valLoss: 0.1297 valAcc: 84.01% AUC: 63.35%
Epoch: 120 Loss: 0.0141 valLoss: 0.1305 valAcc: 84.25% AUC: 63.22%
Epoch: 121 Loss: 0.0146 valLoss: 0.1307 valAcc: 84.28% AUC: 63.25%
Epoch: 122 Loss: 0.0144 valLoss: 0.1305 valAcc: 83.94% AUC: 63.34%
Epoch: 123 Loss: 0.0125 valLoss: 0.1304 valAcc: 83.83% AUC: 63.49%
Epoch: 124 Loss: 0.0150 valLoss: 0.1300 valAcc: 83.83% AUC: 63.56%
Epoch: 125 Loss: 0.0139 valLoss: 0.1297 valAcc: 83.70% AUC: 63.65%
Epoch: 126 Loss: 0.0130 valLoss: 0.1298 valAcc: 83.70% AUC: 63.76%
Epoch: 127 Loss: 0.0112 valLoss: 0.1302 valAcc: 83.87% AUC: 63.88%
Epoch: 128 Loss: 0.0120 valLoss: 0.1307 valAcc: 83.97% AUC: 64.02%
Epoch: 129 Loss: 0.0147 valLoss: 0.1310 valAcc: 83.97% AUC: 64.07%
Epoch: 130 Loss: 0.0114 valLoss: 0.1314 valAcc: 83.90% AUC: 63.98%
Epoch: 131 Loss: 0.0107 valLoss: 0.1320 valAcc: 84.28% AUC: 63.88%
Epoch: 132 Loss: 0.0113 valLoss: 0.1325 valAcc: 84.32% AUC: 63.82%
Epoch: 133 Loss: 0.0093 valLoss: 0.1333 valAcc: 84.42% AUC: 63.82%
Epoch: 134 Loss: 0.0109 valLoss: 0.1340 valAcc: 84.77% AUC: 63.90%
Epoch: 135 Loss: 0.0108 valLoss: 0.1343 valAcc: 84.91% AUC: 63.92%
Epoch: 136 Loss: 0.0099 valLoss: 0.1349 valAcc: 84.97% AUC: 63.89%
Epoch: 137 Loss: 0.0105 valLoss: 0.1356 valAcc: 85.32% AUC: 63.82%
Epoch: 138 Loss: 0.0086 valLoss: 0.1367 valAcc: 85.66% AUC: 63.77%
Epoch: 139 Loss: 0.0097 valLoss: 0.1376 valAcc: 85.70% AUC: 63.80%
Epoch: 140 Loss: 0.0101 valLoss: 0.1382 valAcc: 85.80% AUC: 63.84%
Epoch: 141 Loss: 0.0096 valLoss: 0.1388 valAcc: 85.70% AUC: 63.90%
Epoch: 142 Loss: 0.0097 valLoss: 0.1396 valAcc: 85.91% AUC: 63.88%
Epoch: 143 Loss: 0.0101 valLoss: 0.1406 valAcc: 86.11% AUC: 63.75%
Epoch: 144 Loss: 0.0092 valLoss: 0.1416 valAcc: 86.46% AUC: 63.60%
Epoch: 145 Loss: 0.0073 valLoss: 0.1427 valAcc: 86.53% AUC: 63.54%
Epoch: 146 Loss: 0.0096 valLoss: 0.1436 valAcc: 86.63% AUC: 63.54%
Epoch: 147 Loss: 0.0082 valLoss: 0.1447 valAcc: 86.84% AUC: 63.60%
Epoch: 148 Loss: 0.0083 valLoss: 0.1454 valAcc: 86.87% AUC: 63.67%
Epoch: 149 Loss: 0.0081 valLoss: 0.1461 valAcc: 87.08% AUC: 63.71%
Epoch: 150 Loss: 0.0094 valLoss: 0.1464 valAcc: 87.05% AUC: 63.73%
Epoch: 151 Loss: 0.0082 valLoss: 0.1469 valAcc: 87.05% AUC: 63.76%
Epoch: 152 Loss: 0.0089 valLoss: 0.1471 valAcc: 86.87% AUC: 63.76%
Epoch: 153 Loss: 0.0079 valLoss: 0.1475 valAcc: 86.94% AUC: 63.67%
Epoch: 154 Loss: 0.0073 valLoss: 0.1481 valAcc: 86.87% AUC: 63.61%
Epoch: 155 Loss: 0.0081 valLoss: 0.1490 valAcc: 86.98% AUC: 63.62%
Epoch: 156 Loss: 0.0080 valLoss: 0.1499 valAcc: 87.05% AUC: 63.68%
Epoch: 157 Loss: 0.0079 valLoss: 0.1509 valAcc: 87.08% AUC: 63.69%
Epoch: 158 Loss: 0.0087 valLoss: 0.1522 valAcc: 87.36% AUC: 63.59%
Epoch: 159 Loss: 0.0090 valLoss: 0.1530 valAcc: 87.32% AUC: 63.50%
Epoch: 160 Loss: 0.0087 valLoss: 0.1532 valAcc: 87.12% AUC: 63.37%
Epoch: 161 Loss: 0.0081 valLoss: 0.1536 valAcc: 86.98% AUC: 63.31%
Epoch: 162 Loss: 0.0073 valLoss: 0.1538 valAcc: 86.87% AUC: 63.36%
Epoch: 163 Loss: 0.0075 valLoss: 0.1541 valAcc: 86.74% AUC: 63.45%
Epoch: 164 Loss: 0.0084 valLoss: 0.1547 valAcc: 86.77% AUC: 63.57%
Epoch: 165 Loss: 0.0085 valLoss: 0.1555 valAcc: 86.70% AUC: 63.50%
Epoch: 166 Loss: 0.0075 valLoss: 0.1567 valAcc: 86.60% AUC: 63.42%
Epoch: 167 Loss: 0.0064 valLoss: 0.1578 valAcc: 86.87% AUC: 63.38%
Epoch: 168 Loss: 0.0063 valLoss: 0.1588 valAcc: 86.98% AUC: 63.36%
Epoch: 169 Loss: 0.0067 valLoss: 0.1597 valAcc: 87.12% AUC: 63.31%
Epoch: 170 Loss: 0.0083 valLoss: 0.1598 valAcc: 86.91% AUC: 63.10%
Epoch: 171 Loss: 0.0069 valLoss: 0.1598 valAcc: 86.87% AUC: 62.92%
Epoch: 172 Loss: 0.0069 valLoss: 0.1604 valAcc: 86.80% AUC: 62.86%
Epoch: 173 Loss: 0.0063 valLoss: 0.1612 valAcc: 86.80% AUC: 63.06%
Epoch: 174 Loss: 0.0060 valLoss: 0.1621 valAcc: 86.87% AUC: 63.34%
Epoch: 175 Loss: 0.0065 valLoss: 0.1635 valAcc: 86.77% AUC: 63.52%
Epoch: 176 Loss: 0.0055 valLoss: 0.1644 valAcc: 86.87% AUC: 63.62%
Epoch: 177 Loss: 0.0055 valLoss: 0.1656 valAcc: 87.01% AUC: 63.63%
Epoch: 178 Loss: 0.0074 valLoss: 0.1667 valAcc: 87.05% AUC: 63.61%
Epoch: 179 Loss: 0.0055 valLoss: 0.1681 valAcc: 87.18% AUC: 63.50%
Epoch: 180 Loss: 0.0064 valLoss: 0.1690 valAcc: 87.25% AUC: 63.42%
Epoch: 181 Loss: 0.0062 valLoss: 0.1697 valAcc: 87.25% AUC: 63.42%
Epoch: 182 Loss: 0.0054 valLoss: 0.1706 valAcc: 87.39% AUC: 63.46%
Epoch: 183 Loss: 0.0049 valLoss: 0.1717 valAcc: 87.50% AUC: 63.52%
Epoch: 184 Loss: 0.0047 valLoss: 0.1727 valAcc: 87.50% AUC: 63.61%
Epoch: 185 Loss: 0.0060 valLoss: 0.1735 valAcc: 87.53% AUC: 63.67%
Epoch: 186 Loss: 0.0058 valLoss: 0.1739 valAcc: 87.53% AUC: 63.68%
Epoch: 187 Loss: 0.0071 valLoss: 0.1744 valAcc: 87.67% AUC: 63.58%
Epoch: 188 Loss: 0.0066 valLoss: 0.1748 valAcc: 87.63% AUC: 63.56%
Epoch: 189 Loss: 0.0049 valLoss: 0.1757 valAcc: 87.81% AUC: 63.59%
Epoch: 190 Loss: 0.0058 valLoss: 0.1773 valAcc: 88.05% AUC: 63.55%
Epoch: 191 Loss: 0.0052 valLoss: 0.1795 valAcc: 88.15% AUC: 63.57%
Epoch: 192 Loss: 0.0059 valLoss: 0.1809 valAcc: 88.22% AUC: 63.49%
Epoch: 193 Loss: 0.0067 valLoss: 0.1817 valAcc: 88.29% AUC: 63.32%
Epoch: 194 Loss: 0.0047 valLoss: 0.1822 valAcc: 88.22% AUC: 63.22%
Epoch: 195 Loss: 0.0071 valLoss: 0.1820 valAcc: 88.01% AUC: 63.23%
Epoch: 196 Loss: 0.0050 valLoss: 0.1814 valAcc: 88.05% AUC: 63.43%
Epoch: 197 Loss: 0.0050 valLoss: 0.1816 valAcc: 87.84% AUC: 63.60%
Epoch: 198 Loss: 0.0067 valLoss: 0.1817 valAcc: 87.74% AUC: 63.72%
Epoch: 199 Loss: 0.0052 valLoss: 0.1821 valAcc: 87.74% AUC: 63.85%
Epoch: 200 Loss: 0.0047 valLoss: 0.1832 valAcc: 87.91% AUC: 63.97%
Epoch: 201 Loss: 0.0044 valLoss: 0.1846 valAcc: 87.94% AUC: 64.10%
Epoch: 202 Loss: 0.0051 valLoss: 0.1855 valAcc: 87.98% AUC: 64.27%
Epoch: 203 Loss: 0.0046 valLoss: 0.1861 valAcc: 87.98% AUC: 64.32%
Epoch: 204 Loss: 0.0042 valLoss: 0.1867 valAcc: 88.08% AUC: 64.27%
Epoch: 205 Loss: 0.0038 valLoss: 0.1875 valAcc: 88.15% AUC: 64.21%
Epoch: 206 Loss: 0.0042 valLoss: 0.1883 valAcc: 88.15% AUC: 64.21%
Epoch: 207 Loss: 0.0042 valLoss: 0.1895 valAcc: 88.15% AUC: 64.19%
Epoch: 208 Loss: 0.0046 valLoss: 0.1905 valAcc: 88.08% AUC: 64.13%
Epoch: 209 Loss: 0.0038 valLoss: 0.1914 valAcc: 88.15% AUC: 64.04%
Epoch: 210 Loss: 0.0055 valLoss: 0.1920 valAcc: 88.12% AUC: 63.96%
Epoch: 211 Loss: 0.0037 valLoss: 0.1929 valAcc: 88.22% AUC: 63.94%
Epoch: 212 Loss: 0.0039 valLoss: 0.1942 valAcc: 88.26% AUC: 63.96%
Epoch: 213 Loss: 0.0040 valLoss: 0.1956 valAcc: 88.32% AUC: 64.00%
Epoch: 214 Loss: 0.0038 valLoss: 0.1969 valAcc: 88.57% AUC: 64.05%
Epoch: 215 Loss: 0.0037 valLoss: 0.1982 valAcc: 88.60% AUC: 64.12%
Epoch: 216 Loss: 0.0052 valLoss: 0.1987 valAcc: 88.60% AUC: 64.12%
Epoch: 217 Loss: 0.0048 valLoss: 0.1988 valAcc: 88.53% AUC: 64.04%
Epoch: 218 Loss: 0.0042 valLoss: 0.1987 valAcc: 88.53% AUC: 63.95%
Epoch: 219 Loss: 0.0038 valLoss: 0.1987 valAcc: 88.53% AUC: 63.88%
Epoch: 220 Loss: 0.0055 valLoss: 0.1983 valAcc: 88.29% AUC: 63.85%
Epoch: 221 Loss: 0.0040 valLoss: 0.1980 valAcc: 88.19% AUC: 63.81%
Epoch: 222 Loss: 0.0038 valLoss: 0.1981 valAcc: 88.15% AUC: 63.79%
Epoch: 223 Loss: 0.0035 valLoss: 0.1981 valAcc: 88.26% AUC: 63.87%
Epoch: 224 Loss: 0.0038 valLoss: 0.1986 valAcc: 88.26% AUC: 63.91%
Epoch: 225 Loss: 0.0043 valLoss: 0.1991 valAcc: 88.26% AUC: 63.97%
Epoch: 226 Loss: 0.0041 valLoss: 0.1993 valAcc: 88.26% AUC: 64.02%
Epoch: 227 Loss: 0.0037 valLoss: 0.2002 valAcc: 88.26% AUC: 63.97%
Epoch: 228 Loss: 0.0028 valLoss: 0.2012 valAcc: 88.43% AUC: 63.94%
Epoch: 229 Loss: 0.0042 valLoss: 0.2007 valAcc: 88.22% AUC: 63.94%
Epoch: 230 Loss: 0.0037 valLoss: 0.2006 valAcc: 88.22% AUC: 63.94%
Epoch: 231 Loss: 0.0031 valLoss: 0.2008 valAcc: 88.19% AUC: 63.92%
Epoch: 232 Loss: 0.0039 valLoss: 0.2012 valAcc: 88.36% AUC: 63.86%
Epoch: 233 Loss: 0.0036 valLoss: 0.2017 valAcc: 88.36% AUC: 63.88%
Epoch: 234 Loss: 0.0046 valLoss: 0.2021 valAcc: 88.39% AUC: 64.01%
Epoch: 235 Loss: 0.0048 valLoss: 0.2028 valAcc: 88.46% AUC: 64.18%
Epoch: 236 Loss: 0.0037 valLoss: 0.2035 valAcc: 88.50% AUC: 64.28%
Epoch: 237 Loss: 0.0031 valLoss: 0.2049 valAcc: 88.60% AUC: 64.35%
Epoch: 238 Loss: 0.0032 valLoss: 0.2065 valAcc: 88.74% AUC: 64.42%
Epoch: 239 Loss: 0.0035 valLoss: 0.2074 valAcc: 88.70% AUC: 64.39%
Epoch: 240 Loss: 0.0038 valLoss: 0.2080 valAcc: 88.74% AUC: 64.31%
Epoch: 241 Loss: 0.0038 valLoss: 0.2096 valAcc: 88.77% AUC: 64.14%
Epoch: 242 Loss: 0.0035 valLoss: 0.2112 valAcc: 88.88% AUC: 63.98%
Epoch: 243 Loss: 0.0025 valLoss: 0.2131 valAcc: 89.02% AUC: 63.93%
Epoch: 244 Loss: 0.0030 valLoss: 0.2147 valAcc: 89.15% AUC: 63.95%
Epoch: 245 Loss: 0.0030 valLoss: 0.2162 valAcc: 89.15% AUC: 63.98%
Epoch: 246 Loss: 0.0025 valLoss: 0.2177 valAcc: 89.19% AUC: 64.00%
Epoch: 247 Loss: 0.0027 valLoss: 0.2187 valAcc: 89.22% AUC: 64.02%
Epoch: 248 Loss: 0.0035 valLoss: 0.2189 valAcc: 89.22% AUC: 64.03%
Epoch: 249 Loss: 0.0025 valLoss: 0.2190 valAcc: 89.12% AUC: 64.07%
Epoch: 250 Loss: 0.0023 valLoss: 0.2191 valAcc: 89.12% AUC: 64.10%
Epoch: 251 Loss: 0.0031 valLoss: 0.2191 valAcc: 89.15% AUC: 64.12%
Epoch: 252 Loss: 0.0034 valLoss: 0.2186 valAcc: 89.15% AUC: 64.18%
Epoch: 253 Loss: 0.0029 valLoss: 0.2181 valAcc: 89.05% AUC: 64.24%
Epoch: 254 Loss: 0.0030 valLoss: 0.2179 valAcc: 89.02% AUC: 64.23%
Epoch: 255 Loss: 0.0028 valLoss: 0.2181 valAcc: 88.98% AUC: 64.20%
Epoch: 256 Loss: 0.0037 valLoss: 0.2180 valAcc: 88.91% AUC: 64.19%
Epoch: 257 Loss: 0.0027 valLoss: 0.2179 valAcc: 88.81% AUC: 64.21%
Epoch: 258 Loss: 0.0035 valLoss: 0.2179 valAcc: 88.74% AUC: 64.14%
Epoch: 259 Loss: 0.0030 valLoss: 0.2184 valAcc: 88.81% AUC: 64.13%
Epoch: 260 Loss: 0.0031 valLoss: 0.2192 valAcc: 88.81% AUC: 64.19%
Epoch: 261 Loss: 0.0026 valLoss: 0.2203 valAcc: 88.98% AUC: 64.27%
Epoch: 262 Loss: 0.0028 valLoss: 0.2215 valAcc: 88.98% AUC: 64.33%
Epoch: 263 Loss: 0.0032 valLoss: 0.2226 valAcc: 89.02% AUC: 64.30%
Epoch: 264 Loss: 0.0026 valLoss: 0.2236 valAcc: 89.08% AUC: 64.29%
Epoch: 265 Loss: 0.0035 valLoss: 0.2228 valAcc: 89.02% AUC: 64.28%
Epoch: 266 Loss: 0.0025 valLoss: 0.2224 valAcc: 88.95% AUC: 64.25%
Epoch: 267 Loss: 0.0039 valLoss: 0.2225 valAcc: 88.88% AUC: 64.23%
Epoch: 268 Loss: 0.0029 valLoss: 0.2228 valAcc: 88.77% AUC: 64.28%
Epoch: 269 Loss: 0.0025 valLoss: 0.2236 valAcc: 88.88% AUC: 64.35%
Epoch: 270 Loss: 0.0025 valLoss: 0.2247 valAcc: 88.95% AUC: 64.45%
Epoch: 271 Loss: 0.0030 valLoss: 0.2261 valAcc: 89.08% AUC: 64.54%
Epoch: 272 Loss: 0.0024 valLoss: 0.2275 valAcc: 89.08% AUC: 64.53%
Epoch: 273 Loss: 0.0026 valLoss: 0.2291 valAcc: 89.12% AUC: 64.54%
Epoch: 274 Loss: 0.0029 valLoss: 0.2298 valAcc: 89.12% AUC: 64.50%
Epoch: 275 Loss: 0.0028 valLoss: 0.2300 valAcc: 89.12% AUC: 64.44%
Epoch: 276 Loss: 0.0031 valLoss: 0.2298 valAcc: 89.12% AUC: 64.36%
Epoch: 277 Loss: 0.0029 valLoss: 0.2302 valAcc: 89.22% AUC: 64.28%
Epoch: 278 Loss: 0.0019 valLoss: 0.2308 valAcc: 89.26% AUC: 64.27%
Epoch: 279 Loss: 0.0031 valLoss: 0.2310 valAcc: 89.29% AUC: 64.32%
Epoch: 280 Loss: 0.0023 valLoss: 0.2314 valAcc: 89.33% AUC: 64.37%
Epoch: 281 Loss: 0.0023 valLoss: 0.2319 valAcc: 89.33% AUC: 64.45%
Epoch: 282 Loss: 0.0020 valLoss: 0.2330 valAcc: 89.36% AUC: 64.51%
Epoch: 283 Loss: 0.0028 valLoss: 0.2335 valAcc: 89.43% AUC: 64.53%
Epoch: 284 Loss: 0.0025 valLoss: 0.2335 valAcc: 89.46% AUC: 64.53%
Epoch: 285 Loss: 0.0028 valLoss: 0.2336 valAcc: 89.46% AUC: 64.37%
Epoch: 286 Loss: 0.0020 valLoss: 0.2335 valAcc: 89.36% AUC: 64.22%
Epoch: 287 Loss: 0.0026 valLoss: 0.2336 valAcc: 89.12% AUC: 64.16%
Epoch: 288 Loss: 0.0027 valLoss: 0.2339 valAcc: 89.22% AUC: 64.20%
Epoch: 289 Loss: 0.0023 valLoss: 0.2347 valAcc: 89.22% AUC: 64.29%
Epoch: 290 Loss: 0.0020 valLoss: 0.2358 valAcc: 89.40% AUC: 64.39%
Epoch: 291 Loss: 0.0022 valLoss: 0.2369 valAcc: 89.46% AUC: 64.46%
Epoch: 292 Loss: 0.0025 valLoss: 0.2378 valAcc: 89.43% AUC: 64.51%
Epoch: 293 Loss: 0.0020 valLoss: 0.2382 valAcc: 89.40% AUC: 64.46%
Epoch: 294 Loss: 0.0027 valLoss: 0.2379 valAcc: 89.19% AUC: 64.42%
Epoch: 295 Loss: 0.0023 valLoss: 0.2376 valAcc: 89.15% AUC: 64.38%
Epoch: 296 Loss: 0.0021 valLoss: 0.2378 valAcc: 89.15% AUC: 64.37%
Epoch: 297 Loss: 0.0020 valLoss: 0.2381 valAcc: 89.15% AUC: 64.38%
Epoch: 298 Loss: 0.0021 valLoss: 0.2384 valAcc: 89.22% AUC: 64.38%
Epoch: 299 Loss: 0.0025 valLoss: 0.2393 valAcc: 89.26% AUC: 64.36%
Restoring model weights from the best epoch: 271 with the best VAL_AUC: 64.54%
Training time: 0:05:35.144882
## Testing ../data/prepared_epitope_testing.csv at: 21-12-2023#11:06:18 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.177544
The best cut-off value is: 0.31
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[14928  1197]
 [ 1197   188]]
ValAcc: 86.33% specScore: 92.58% presScore: 13.57% recallScore: 13.57% F1Score: 13.57% MCC: 6.15% AUC: 62.33% AP: 11.54%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-12-2023#12:38:43 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-12-2023#13:31:06 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 2)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1464 valLoss: 0.1166 valAcc: 8.29% AUC: 54.91%
Epoch: 01 Loss: 0.1338 valLoss: 0.1164 valAcc: 8.29% AUC: 56.25%
Epoch: 02 Loss: 0.1306 valLoss: 0.1164 valAcc: 8.29% AUC: 56.89%
Epoch: 03 Loss: 0.1250 valLoss: 0.1163 valAcc: 8.29% AUC: 57.37%
Epoch: 04 Loss: 0.1202 valLoss: 0.1164 valAcc: 8.29% AUC: 57.08%
Epoch: 05 Loss: 0.1247 valLoss: 0.1164 valAcc: 8.29% AUC: 56.91%
Epoch: 06 Loss: 0.1149 valLoss: 0.1164 valAcc: 8.29% AUC: 56.96%
Epoch: 07 Loss: 0.1154 valLoss: 0.1164 valAcc: 8.29% AUC: 56.82%
Epoch: 08 Loss: 0.1131 valLoss: 0.1165 valAcc: 8.29% AUC: 56.79%
Epoch: 09 Loss: 0.1111 valLoss: 0.1165 valAcc: 8.29% AUC: 56.82%
Epoch: 10 Loss: 0.1062 valLoss: 0.1165 valAcc: 8.29% AUC: 56.87%
Epoch: 11 Loss: 0.1015 valLoss: 0.1165 valAcc: 8.29% AUC: 56.99%
Epoch: 12 Loss: 0.1004 valLoss: 0.1164 valAcc: 8.29% AUC: 57.38%
Epoch: 13 Loss: 0.0991 valLoss: 0.1163 valAcc: 8.29% AUC: 57.68%
Epoch: 14 Loss: 0.0975 valLoss: 0.1162 valAcc: 8.29% AUC: 58.01%
Epoch: 15 Loss: 0.0990 valLoss: 0.1161 valAcc: 8.32% AUC: 58.28%
Epoch: 16 Loss: 0.0990 valLoss: 0.1160 valAcc: 8.36% AUC: 58.71%
Epoch: 17 Loss: 0.0936 valLoss: 0.1159 valAcc: 8.46% AUC: 59.13%
Epoch: 18 Loss: 0.0900 valLoss: 0.1158 valAcc: 8.60% AUC: 59.30%
Epoch: 19 Loss: 0.0892 valLoss: 0.1157 valAcc: 8.98% AUC: 59.60%
Epoch: 20 Loss: 0.0901 valLoss: 0.1155 valAcc: 9.29% AUC: 59.90%
Epoch: 21 Loss: 0.0844 valLoss: 0.1154 valAcc: 9.88% AUC: 60.19%
Epoch: 22 Loss: 0.0868 valLoss: 0.1153 valAcc: 10.50% AUC: 60.58%
Epoch: 23 Loss: 0.0816 valLoss: 0.1152 valAcc: 10.78% AUC: 60.87%
Epoch: 24 Loss: 0.0779 valLoss: 0.1151 valAcc: 11.61% AUC: 60.94%
Epoch: 25 Loss: 0.0808 valLoss: 0.1150 valAcc: 12.61% AUC: 60.93%
Epoch: 26 Loss: 0.0801 valLoss: 0.1149 valAcc: 13.47% AUC: 61.02%
Epoch: 27 Loss: 0.0771 valLoss: 0.1148 valAcc: 14.75% AUC: 61.05%
Epoch: 28 Loss: 0.0738 valLoss: 0.1147 valAcc: 16.41% AUC: 60.96%
Epoch: 29 Loss: 0.0749 valLoss: 0.1146 valAcc: 17.75% AUC: 61.09%
Epoch: 30 Loss: 0.0718 valLoss: 0.1144 valAcc: 18.65% AUC: 61.40%
Epoch: 31 Loss: 0.0685 valLoss: 0.1141 valAcc: 19.76% AUC: 61.91%
Epoch: 32 Loss: 0.0702 valLoss: 0.1139 valAcc: 21.07% AUC: 62.43%
Epoch: 33 Loss: 0.0678 valLoss: 0.1136 valAcc: 22.31% AUC: 62.85%
Epoch: 34 Loss: 0.0623 valLoss: 0.1134 valAcc: 23.25% AUC: 63.13%
Epoch: 35 Loss: 0.0663 valLoss: 0.1133 valAcc: 24.53% AUC: 63.32%
Epoch: 36 Loss: 0.0617 valLoss: 0.1131 valAcc: 25.39% AUC: 63.45%
Epoch: 37 Loss: 0.0629 valLoss: 0.1131 valAcc: 26.46% AUC: 63.54%
Epoch: 38 Loss: 0.0587 valLoss: 0.1130 valAcc: 27.05% AUC: 63.63%
Epoch: 39 Loss: 0.0586 valLoss: 0.1129 valAcc: 27.39% AUC: 63.73%
Epoch: 40 Loss: 0.0572 valLoss: 0.1128 valAcc: 27.88% AUC: 63.93%
Epoch: 41 Loss: 0.0597 valLoss: 0.1127 valAcc: 27.98% AUC: 64.02%
Epoch: 42 Loss: 0.0540 valLoss: 0.1126 valAcc: 28.26% AUC: 64.06%
Epoch: 43 Loss: 0.0513 valLoss: 0.1125 valAcc: 28.64% AUC: 64.15%
Epoch: 44 Loss: 0.0515 valLoss: 0.1125 valAcc: 29.19% AUC: 64.13%
Epoch: 45 Loss: 0.0518 valLoss: 0.1125 valAcc: 29.60% AUC: 64.10%
Epoch: 46 Loss: 0.0498 valLoss: 0.1124 valAcc: 30.26% AUC: 64.05%
Epoch: 47 Loss: 0.0501 valLoss: 0.1124 valAcc: 30.54% AUC: 64.11%
Epoch: 48 Loss: 0.0500 valLoss: 0.1123 valAcc: 30.78% AUC: 64.22%
Epoch: 49 Loss: 0.0510 valLoss: 0.1122 valAcc: 30.78% AUC: 64.36%
Epoch: 50 Loss: 0.0472 valLoss: 0.1121 valAcc: 30.95% AUC: 64.51%
Epoch: 51 Loss: 0.0494 valLoss: 0.1120 valAcc: 30.85% AUC: 64.70%
Epoch: 52 Loss: 0.0499 valLoss: 0.1120 valAcc: 30.43% AUC: 64.79%
Epoch: 53 Loss: 0.0441 valLoss: 0.1120 valAcc: 29.91% AUC: 64.84%
Epoch: 54 Loss: 0.0443 valLoss: 0.1120 valAcc: 29.64% AUC: 64.88%
Epoch: 55 Loss: 0.0417 valLoss: 0.1120 valAcc: 29.46% AUC: 65.04%
Epoch: 56 Loss: 0.0409 valLoss: 0.1119 valAcc: 29.53% AUC: 65.17%
Epoch: 57 Loss: 0.0412 valLoss: 0.1118 valAcc: 29.84% AUC: 65.38%
Epoch: 58 Loss: 0.0398 valLoss: 0.1116 valAcc: 30.33% AUC: 65.66%
Epoch: 59 Loss: 0.0379 valLoss: 0.1114 valAcc: 30.92% AUC: 65.90%
Epoch: 60 Loss: 0.0378 valLoss: 0.1112 valAcc: 32.02% AUC: 66.07%
Epoch: 61 Loss: 0.0378 valLoss: 0.1112 valAcc: 32.33% AUC: 66.07%
Epoch: 62 Loss: 0.0373 valLoss: 0.1111 valAcc: 32.88% AUC: 66.07%
Epoch: 63 Loss: 0.0363 valLoss: 0.1111 valAcc: 33.54% AUC: 66.02%
Epoch: 64 Loss: 0.0349 valLoss: 0.1111 valAcc: 34.02% AUC: 65.95%
Epoch: 65 Loss: 0.0379 valLoss: 0.1111 valAcc: 34.13% AUC: 65.90%
Epoch: 66 Loss: 0.0348 valLoss: 0.1111 valAcc: 34.13% AUC: 65.85%
Epoch: 67 Loss: 0.0334 valLoss: 0.1111 valAcc: 34.30% AUC: 65.86%
Epoch: 68 Loss: 0.0329 valLoss: 0.1110 valAcc: 34.75% AUC: 65.90%
Epoch: 69 Loss: 0.0334 valLoss: 0.1109 valAcc: 36.03% AUC: 66.00%
Epoch: 70 Loss: 0.0326 valLoss: 0.1108 valAcc: 36.86% AUC: 66.05%
Epoch: 71 Loss: 0.0330 valLoss: 0.1108 valAcc: 37.69% AUC: 66.12%
Epoch: 72 Loss: 0.0298 valLoss: 0.1107 valAcc: 38.38% AUC: 66.23%
Epoch: 73 Loss: 0.0306 valLoss: 0.1106 valAcc: 38.62% AUC: 66.33%
Epoch: 74 Loss: 0.0287 valLoss: 0.1105 valAcc: 39.03% AUC: 66.42%
Epoch: 75 Loss: 0.0300 valLoss: 0.1104 valAcc: 39.59% AUC: 66.52%
Epoch: 76 Loss: 0.0273 valLoss: 0.1103 valAcc: 40.00% AUC: 66.59%
Epoch: 77 Loss: 0.0280 valLoss: 0.1102 valAcc: 40.45% AUC: 66.69%
Epoch: 78 Loss: 0.0287 valLoss: 0.1102 valAcc: 40.79% AUC: 66.67%
Epoch: 79 Loss: 0.0269 valLoss: 0.1102 valAcc: 41.04% AUC: 66.70%
Epoch: 80 Loss: 0.0261 valLoss: 0.1102 valAcc: 41.52% AUC: 66.71%
Epoch: 81 Loss: 0.0259 valLoss: 0.1101 valAcc: 42.11% AUC: 66.74%
Epoch: 82 Loss: 0.0251 valLoss: 0.1100 valAcc: 43.01% AUC: 66.80%
Epoch: 83 Loss: 0.0270 valLoss: 0.1099 valAcc: 43.42% AUC: 66.92%
Epoch: 84 Loss: 0.0256 valLoss: 0.1098 valAcc: 43.42% AUC: 67.05%
Epoch: 85 Loss: 0.0283 valLoss: 0.1097 valAcc: 43.07% AUC: 67.15%
Epoch: 86 Loss: 0.0246 valLoss: 0.1096 valAcc: 42.94% AUC: 67.26%
Epoch: 87 Loss: 0.0236 valLoss: 0.1096 valAcc: 43.25% AUC: 67.34%
Epoch: 88 Loss: 0.0229 valLoss: 0.1095 valAcc: 43.59% AUC: 67.37%
Epoch: 89 Loss: 0.0237 valLoss: 0.1094 valAcc: 44.39% AUC: 67.44%
Epoch: 90 Loss: 0.0215 valLoss: 0.1093 valAcc: 45.56% AUC: 67.53%
Epoch: 91 Loss: 0.0240 valLoss: 0.1093 valAcc: 47.12% AUC: 67.58%
Epoch: 92 Loss: 0.0222 valLoss: 0.1093 valAcc: 48.57% AUC: 67.57%
Epoch: 93 Loss: 0.0227 valLoss: 0.1092 valAcc: 49.88% AUC: 67.57%
Epoch: 94 Loss: 0.0239 valLoss: 0.1092 valAcc: 50.64% AUC: 67.56%
Epoch: 95 Loss: 0.0234 valLoss: 0.1094 valAcc: 50.36% AUC: 67.44%
Epoch: 96 Loss: 0.0214 valLoss: 0.1095 valAcc: 50.02% AUC: 67.31%
Epoch: 97 Loss: 0.0224 valLoss: 0.1095 valAcc: 50.05% AUC: 67.28%
Epoch: 98 Loss: 0.0204 valLoss: 0.1095 valAcc: 50.88% AUC: 67.34%
Epoch: 99 Loss: 0.0214 valLoss: 0.1093 valAcc: 52.44% AUC: 67.49%
Epoch: 100 Loss: 0.0213 valLoss: 0.1092 valAcc: 53.58% AUC: 67.60%
Epoch: 101 Loss: 0.0192 valLoss: 0.1092 valAcc: 54.58% AUC: 67.70%
Epoch: 102 Loss: 0.0184 valLoss: 0.1092 valAcc: 55.20% AUC: 67.70%
Epoch: 103 Loss: 0.0188 valLoss: 0.1093 valAcc: 55.75% AUC: 67.61%
Epoch: 104 Loss: 0.0189 valLoss: 0.1094 valAcc: 56.03% AUC: 67.49%
Epoch: 105 Loss: 0.0190 valLoss: 0.1095 valAcc: 56.17% AUC: 67.39%
Epoch: 106 Loss: 0.0168 valLoss: 0.1096 valAcc: 57.06% AUC: 67.33%
Epoch: 107 Loss: 0.0179 valLoss: 0.1097 valAcc: 57.93% AUC: 67.35%
Epoch: 108 Loss: 0.0166 valLoss: 0.1098 valAcc: 59.24% AUC: 67.36%
Epoch: 109 Loss: 0.0178 valLoss: 0.1099 valAcc: 60.07% AUC: 67.37%
Epoch: 110 Loss: 0.0166 valLoss: 0.1100 valAcc: 61.04% AUC: 67.36%
Epoch: 111 Loss: 0.0171 valLoss: 0.1101 valAcc: 61.49% AUC: 67.32%
Epoch: 112 Loss: 0.0174 valLoss: 0.1103 valAcc: 61.93% AUC: 67.26%
Epoch: 113 Loss: 0.0162 valLoss: 0.1104 valAcc: 62.69% AUC: 67.23%
Epoch: 114 Loss: 0.0153 valLoss: 0.1105 valAcc: 63.77% AUC: 67.31%
Epoch: 115 Loss: 0.0148 valLoss: 0.1105 valAcc: 64.84% AUC: 67.45%
Epoch: 116 Loss: 0.0151 valLoss: 0.1107 valAcc: 66.01% AUC: 67.45%
Epoch: 117 Loss: 0.0152 valLoss: 0.1109 valAcc: 66.67% AUC: 67.41%
Epoch: 118 Loss: 0.0156 valLoss: 0.1111 valAcc: 66.94% AUC: 67.34%
Epoch: 119 Loss: 0.0159 valLoss: 0.1114 valAcc: 66.94% AUC: 67.13%
Epoch: 120 Loss: 0.0143 valLoss: 0.1115 valAcc: 67.15% AUC: 67.13%
Epoch: 121 Loss: 0.0138 valLoss: 0.1115 valAcc: 67.84% AUC: 67.25%
Epoch: 122 Loss: 0.0148 valLoss: 0.1114 valAcc: 68.60% AUC: 67.48%
Epoch: 123 Loss: 0.0145 valLoss: 0.1114 valAcc: 69.19% AUC: 67.63%
Epoch: 124 Loss: 0.0137 valLoss: 0.1115 valAcc: 69.78% AUC: 67.70%
Epoch: 125 Loss: 0.0139 valLoss: 0.1116 valAcc: 70.43% AUC: 67.74%
Epoch: 126 Loss: 0.0131 valLoss: 0.1118 valAcc: 70.81% AUC: 67.75%
Epoch: 127 Loss: 0.0127 valLoss: 0.1120 valAcc: 71.30% AUC: 67.72%
Epoch: 128 Loss: 0.0132 valLoss: 0.1122 valAcc: 71.71% AUC: 67.67%
Epoch: 129 Loss: 0.0132 valLoss: 0.1125 valAcc: 71.99% AUC: 67.59%
Epoch: 130 Loss: 0.0141 valLoss: 0.1128 valAcc: 71.88% AUC: 67.44%
Epoch: 131 Loss: 0.0125 valLoss: 0.1131 valAcc: 72.19% AUC: 67.35%
Epoch: 132 Loss: 0.0130 valLoss: 0.1135 valAcc: 73.23% AUC: 67.27%
Epoch: 133 Loss: 0.0116 valLoss: 0.1139 valAcc: 74.06% AUC: 67.22%
Epoch: 134 Loss: 0.0132 valLoss: 0.1143 valAcc: 74.51% AUC: 67.14%
Epoch: 135 Loss: 0.0114 valLoss: 0.1147 valAcc: 75.16% AUC: 67.17%
Epoch: 136 Loss: 0.0108 valLoss: 0.1151 valAcc: 76.20% AUC: 67.24%
Epoch: 137 Loss: 0.0112 valLoss: 0.1156 valAcc: 76.72% AUC: 67.28%
Epoch: 138 Loss: 0.0116 valLoss: 0.1161 valAcc: 77.24% AUC: 67.24%
Epoch: 139 Loss: 0.0107 valLoss: 0.1167 valAcc: 77.82% AUC: 67.15%
Epoch: 140 Loss: 0.0107 valLoss: 0.1171 valAcc: 78.13% AUC: 67.08%
Epoch: 141 Loss: 0.0107 valLoss: 0.1176 valAcc: 78.27% AUC: 66.96%
Epoch: 142 Loss: 0.0110 valLoss: 0.1180 valAcc: 78.65% AUC: 66.81%
Epoch: 143 Loss: 0.0115 valLoss: 0.1184 valAcc: 78.89% AUC: 66.70%
Epoch: 144 Loss: 0.0096 valLoss: 0.1187 valAcc: 79.14% AUC: 66.77%
Epoch: 145 Loss: 0.0100 valLoss: 0.1191 valAcc: 79.21% AUC: 66.85%
Epoch: 146 Loss: 0.0101 valLoss: 0.1194 valAcc: 79.52% AUC: 66.87%
Epoch: 147 Loss: 0.0097 valLoss: 0.1198 valAcc: 79.62% AUC: 66.83%
Epoch: 148 Loss: 0.0098 valLoss: 0.1203 valAcc: 79.72% AUC: 66.78%
Epoch: 149 Loss: 0.0111 valLoss: 0.1204 valAcc: 79.86% AUC: 66.81%
Epoch: 150 Loss: 0.0091 valLoss: 0.1206 valAcc: 79.97% AUC: 66.88%
Epoch: 151 Loss: 0.0095 valLoss: 0.1209 valAcc: 80.10% AUC: 66.91%
Epoch: 152 Loss: 0.0091 valLoss: 0.1214 valAcc: 80.73% AUC: 66.96%
Epoch: 153 Loss: 0.0097 valLoss: 0.1218 valAcc: 81.00% AUC: 67.03%
Epoch: 154 Loss: 0.0093 valLoss: 0.1221 valAcc: 81.28% AUC: 67.09%
Epoch: 155 Loss: 0.0098 valLoss: 0.1225 valAcc: 81.38% AUC: 67.09%
Epoch: 156 Loss: 0.0095 valLoss: 0.1229 valAcc: 81.55% AUC: 67.07%
Epoch: 157 Loss: 0.0095 valLoss: 0.1232 valAcc: 81.76% AUC: 67.07%
Epoch: 158 Loss: 0.0092 valLoss: 0.1236 valAcc: 81.76% AUC: 67.02%
Epoch: 159 Loss: 0.0093 valLoss: 0.1239 valAcc: 81.87% AUC: 66.98%
Epoch: 160 Loss: 0.0085 valLoss: 0.1242 valAcc: 81.93% AUC: 67.01%
Epoch: 161 Loss: 0.0084 valLoss: 0.1244 valAcc: 82.00% AUC: 67.10%
Epoch: 162 Loss: 0.0087 valLoss: 0.1248 valAcc: 82.21% AUC: 67.15%
Epoch: 163 Loss: 0.0085 valLoss: 0.1253 valAcc: 82.49% AUC: 67.23%
Epoch: 164 Loss: 0.0089 valLoss: 0.1259 valAcc: 82.73% AUC: 67.29%
Epoch: 165 Loss: 0.0084 valLoss: 0.1265 valAcc: 82.94% AUC: 67.33%
Epoch: 166 Loss: 0.0089 valLoss: 0.1269 valAcc: 83.21% AUC: 67.34%
Epoch: 167 Loss: 0.0083 valLoss: 0.1273 valAcc: 83.45% AUC: 67.29%
Epoch: 168 Loss: 0.0080 valLoss: 0.1277 valAcc: 83.59% AUC: 67.26%
Epoch: 169 Loss: 0.0079 valLoss: 0.1281 valAcc: 83.90% AUC: 67.27%
Epoch: 170 Loss: 0.0075 valLoss: 0.1285 valAcc: 84.04% AUC: 67.32%
Epoch: 171 Loss: 0.0075 valLoss: 0.1292 valAcc: 84.32% AUC: 67.35%
Epoch: 172 Loss: 0.0073 valLoss: 0.1300 valAcc: 84.56% AUC: 67.41%
Epoch: 173 Loss: 0.0071 valLoss: 0.1309 valAcc: 84.49% AUC: 67.45%
Epoch: 174 Loss: 0.0079 valLoss: 0.1315 valAcc: 84.77% AUC: 67.48%
Epoch: 175 Loss: 0.0074 valLoss: 0.1320 valAcc: 85.08% AUC: 67.51%
Epoch: 176 Loss: 0.0077 valLoss: 0.1325 valAcc: 85.32% AUC: 67.50%
Epoch: 177 Loss: 0.0072 valLoss: 0.1331 valAcc: 85.39% AUC: 67.37%
Epoch: 178 Loss: 0.0075 valLoss: 0.1336 valAcc: 85.63% AUC: 67.19%
Epoch: 179 Loss: 0.0081 valLoss: 0.1341 valAcc: 85.77% AUC: 66.98%
Epoch: 180 Loss: 0.0072 valLoss: 0.1344 valAcc: 85.63% AUC: 66.88%
Epoch: 181 Loss: 0.0064 valLoss: 0.1349 valAcc: 85.84% AUC: 66.88%
Restoring model weights from the best epoch: 101 with the best VAL_AUC: 67.70%
Training time: 0:03:40.125686
## Testing ../data/prepared_epitope_testing.csv at: 21-12-2023#13:37:22 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 2)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.197170
The best cut-off value is: 0.62
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[14957  1168]
 [ 1168   217]]
ValAcc: 86.66% specScore: 92.76% presScore: 15.67% recallScore: 15.67% F1Score: 15.67% MCC: 8.42% AUC: 65.94% AP: 13.25%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 21-12-2023#14:20:05 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 2)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1629 valLoss: 0.1173 valAcc: 8.32% AUC: 51.48%
Epoch: 01 Loss: 0.1548 valLoss: 0.1172 valAcc: 8.32% AUC: 52.65%
Epoch: 02 Loss: 0.1419 valLoss: 0.1172 valAcc: 8.32% AUC: 53.48%
Epoch: 03 Loss: 0.1340 valLoss: 0.1171 valAcc: 8.32% AUC: 54.65%
Epoch: 04 Loss: 0.1327 valLoss: 0.1170 valAcc: 8.32% AUC: 55.41%
Epoch: 05 Loss: 0.1299 valLoss: 0.1169 valAcc: 8.36% AUC: 55.62%
Epoch: 06 Loss: 0.1223 valLoss: 0.1168 valAcc: 8.36% AUC: 56.37%
Epoch: 07 Loss: 0.1216 valLoss: 0.1166 valAcc: 8.32% AUC: 57.39%
Epoch: 08 Loss: 0.1220 valLoss: 0.1165 valAcc: 8.32% AUC: 57.93%
Epoch: 09 Loss: 0.1165 valLoss: 0.1164 valAcc: 8.32% AUC: 58.39%
Epoch: 10 Loss: 0.1101 valLoss: 0.1163 valAcc: 8.36% AUC: 58.73%
Epoch: 11 Loss: 0.1084 valLoss: 0.1162 valAcc: 8.36% AUC: 58.89%
Epoch: 12 Loss: 0.1091 valLoss: 0.1162 valAcc: 8.39% AUC: 58.93%
Epoch: 13 Loss: 0.1062 valLoss: 0.1161 valAcc: 8.39% AUC: 59.02%
Epoch: 14 Loss: 0.1001 valLoss: 0.1159 valAcc: 8.39% AUC: 59.30%
Epoch: 15 Loss: 0.1034 valLoss: 0.1158 valAcc: 8.43% AUC: 59.60%
Epoch: 16 Loss: 0.1003 valLoss: 0.1157 valAcc: 8.39% AUC: 59.79%
Epoch: 17 Loss: 0.0973 valLoss: 0.1155 valAcc: 8.57% AUC: 60.02%
Epoch: 18 Loss: 0.0955 valLoss: 0.1154 valAcc: 8.81% AUC: 60.32%
Epoch: 19 Loss: 0.0981 valLoss: 0.1153 valAcc: 9.22% AUC: 60.60%
Epoch: 20 Loss: 0.0924 valLoss: 0.1151 valAcc: 9.78% AUC: 60.95%
Epoch: 21 Loss: 0.0906 valLoss: 0.1150 valAcc: 10.22% AUC: 61.33%
Epoch: 22 Loss: 0.0881 valLoss: 0.1149 valAcc: 10.98% AUC: 61.66%
Epoch: 23 Loss: 0.0856 valLoss: 0.1148 valAcc: 11.85% AUC: 61.74%
Epoch: 24 Loss: 0.0840 valLoss: 0.1146 valAcc: 12.71% AUC: 61.90%
Epoch: 25 Loss: 0.0851 valLoss: 0.1144 valAcc: 13.58% AUC: 62.13%
Epoch: 26 Loss: 0.0782 valLoss: 0.1143 valAcc: 15.23% AUC: 62.24%
Epoch: 27 Loss: 0.0824 valLoss: 0.1142 valAcc: 17.41% AUC: 62.38%
Epoch: 28 Loss: 0.0789 valLoss: 0.1140 valAcc: 19.27% AUC: 62.56%
Epoch: 29 Loss: 0.0755 valLoss: 0.1138 valAcc: 21.35% AUC: 62.77%
Epoch: 30 Loss: 0.0729 valLoss: 0.1137 valAcc: 23.45% AUC: 62.94%
Epoch: 31 Loss: 0.0724 valLoss: 0.1136 valAcc: 25.32% AUC: 63.11%
Epoch: 32 Loss: 0.0734 valLoss: 0.1134 valAcc: 27.05% AUC: 63.27%
Epoch: 33 Loss: 0.0674 valLoss: 0.1133 valAcc: 28.43% AUC: 63.40%
Epoch: 34 Loss: 0.0675 valLoss: 0.1132 valAcc: 29.98% AUC: 63.52%
Epoch: 35 Loss: 0.0664 valLoss: 0.1131 valAcc: 31.57% AUC: 63.58%
Epoch: 36 Loss: 0.0679 valLoss: 0.1131 valAcc: 33.06% AUC: 63.64%
Epoch: 37 Loss: 0.0683 valLoss: 0.1129 valAcc: 34.13% AUC: 63.80%
Epoch: 38 Loss: 0.0618 valLoss: 0.1128 valAcc: 35.44% AUC: 63.92%
Epoch: 39 Loss: 0.0619 valLoss: 0.1127 valAcc: 36.61% AUC: 64.11%
Epoch: 40 Loss: 0.0586 valLoss: 0.1126 valAcc: 38.69% AUC: 64.23%
Epoch: 41 Loss: 0.0598 valLoss: 0.1125 valAcc: 40.73% AUC: 64.41%
Epoch: 42 Loss: 0.0598 valLoss: 0.1124 valAcc: 42.45% AUC: 64.55%
Epoch: 43 Loss: 0.0588 valLoss: 0.1123 valAcc: 44.15% AUC: 64.68%
Epoch: 44 Loss: 0.0547 valLoss: 0.1123 valAcc: 45.49% AUC: 64.81%
Epoch: 45 Loss: 0.0528 valLoss: 0.1122 valAcc: 46.74% AUC: 64.94%
Epoch: 46 Loss: 0.0539 valLoss: 0.1122 valAcc: 47.98% AUC: 65.00%
Epoch: 47 Loss: 0.0509 valLoss: 0.1122 valAcc: 49.29% AUC: 65.03%
Epoch: 48 Loss: 0.0511 valLoss: 0.1122 valAcc: 50.40% AUC: 65.06%
Epoch: 49 Loss: 0.0507 valLoss: 0.1122 valAcc: 51.09% AUC: 65.06%
Epoch: 50 Loss: 0.0514 valLoss: 0.1123 valAcc: 51.74% AUC: 65.01%
Epoch: 51 Loss: 0.0490 valLoss: 0.1123 valAcc: 52.44% AUC: 65.05%
Epoch: 52 Loss: 0.0482 valLoss: 0.1123 valAcc: 53.02% AUC: 64.99%
Epoch: 53 Loss: 0.0492 valLoss: 0.1124 valAcc: 53.85% AUC: 64.85%
Epoch: 54 Loss: 0.0451 valLoss: 0.1124 valAcc: 54.78% AUC: 64.87%
Epoch: 55 Loss: 0.0450 valLoss: 0.1124 valAcc: 55.85% AUC: 65.01%
Epoch: 56 Loss: 0.0433 valLoss: 0.1123 valAcc: 56.93% AUC: 65.18%
Epoch: 57 Loss: 0.0424 valLoss: 0.1122 valAcc: 57.96% AUC: 65.35%
Epoch: 58 Loss: 0.0414 valLoss: 0.1122 valAcc: 58.51% AUC: 65.45%
Epoch: 59 Loss: 0.0367 valLoss: 0.1121 valAcc: 59.17% AUC: 65.55%
Epoch: 60 Loss: 0.0412 valLoss: 0.1121 valAcc: 59.38% AUC: 65.53%
Epoch: 61 Loss: 0.0417 valLoss: 0.1122 valAcc: 59.38% AUC: 65.36%
Epoch: 62 Loss: 0.0367 valLoss: 0.1124 valAcc: 59.97% AUC: 65.18%
Epoch: 63 Loss: 0.0377 valLoss: 0.1125 valAcc: 60.10% AUC: 65.00%
Epoch: 64 Loss: 0.0347 valLoss: 0.1126 valAcc: 60.55% AUC: 64.87%
Epoch: 65 Loss: 0.0344 valLoss: 0.1127 valAcc: 60.76% AUC: 64.79%
Epoch: 66 Loss: 0.0378 valLoss: 0.1127 valAcc: 60.90% AUC: 64.74%
Epoch: 67 Loss: 0.0366 valLoss: 0.1126 valAcc: 61.38% AUC: 64.82%
Epoch: 68 Loss: 0.0330 valLoss: 0.1127 valAcc: 61.73% AUC: 64.84%
Epoch: 69 Loss: 0.0341 valLoss: 0.1127 valAcc: 62.31% AUC: 64.87%
Epoch: 70 Loss: 0.0326 valLoss: 0.1128 valAcc: 63.01% AUC: 64.96%
Epoch: 71 Loss: 0.0321 valLoss: 0.1128 valAcc: 63.25% AUC: 64.96%
Epoch: 72 Loss: 0.0326 valLoss: 0.1128 valAcc: 63.39% AUC: 64.91%
Epoch: 73 Loss: 0.0359 valLoss: 0.1129 valAcc: 63.32% AUC: 64.83%
Epoch: 74 Loss: 0.0330 valLoss: 0.1129 valAcc: 63.18% AUC: 64.79%
Epoch: 75 Loss: 0.0305 valLoss: 0.1129 valAcc: 62.94% AUC: 64.71%
Epoch: 76 Loss: 0.0308 valLoss: 0.1129 valAcc: 63.14% AUC: 64.67%
Epoch: 77 Loss: 0.0292 valLoss: 0.1129 valAcc: 64.04% AUC: 64.75%
Epoch: 78 Loss: 0.0296 valLoss: 0.1129 valAcc: 64.42% AUC: 64.89%
Epoch: 79 Loss: 0.0259 valLoss: 0.1129 valAcc: 65.08% AUC: 65.01%
Epoch: 80 Loss: 0.0299 valLoss: 0.1129 valAcc: 65.60% AUC: 65.07%
Epoch: 81 Loss: 0.0279 valLoss: 0.1129 valAcc: 66.15% AUC: 65.10%
Epoch: 82 Loss: 0.0281 valLoss: 0.1129 valAcc: 66.15% AUC: 65.12%
Epoch: 83 Loss: 0.0229 valLoss: 0.1129 valAcc: 66.67% AUC: 65.16%
Epoch: 84 Loss: 0.0266 valLoss: 0.1128 valAcc: 66.29% AUC: 65.21%
Epoch: 85 Loss: 0.0277 valLoss: 0.1126 valAcc: 65.77% AUC: 65.33%
Epoch: 86 Loss: 0.0264 valLoss: 0.1125 valAcc: 65.18% AUC: 65.39%
Epoch: 87 Loss: 0.0255 valLoss: 0.1124 valAcc: 65.28% AUC: 65.52%
Epoch: 88 Loss: 0.0234 valLoss: 0.1124 valAcc: 65.66% AUC: 65.66%
Epoch: 89 Loss: 0.0238 valLoss: 0.1124 valAcc: 66.42% AUC: 65.74%
Epoch: 90 Loss: 0.0225 valLoss: 0.1126 valAcc: 67.60% AUC: 65.79%
Epoch: 91 Loss: 0.0230 valLoss: 0.1128 valAcc: 68.60% AUC: 65.82%
Epoch: 92 Loss: 0.0224 valLoss: 0.1129 valAcc: 69.12% AUC: 65.88%
Epoch: 93 Loss: 0.0222 valLoss: 0.1129 valAcc: 69.46% AUC: 65.94%
Epoch: 94 Loss: 0.0213 valLoss: 0.1130 valAcc: 69.71% AUC: 65.99%
Epoch: 95 Loss: 0.0213 valLoss: 0.1130 valAcc: 69.67% AUC: 65.98%
Epoch: 96 Loss: 0.0226 valLoss: 0.1130 valAcc: 69.33% AUC: 65.92%
Epoch: 97 Loss: 0.0195 valLoss: 0.1130 valAcc: 69.08% AUC: 65.87%
Epoch: 98 Loss: 0.0191 valLoss: 0.1130 valAcc: 69.26% AUC: 65.87%
Epoch: 99 Loss: 0.0213 valLoss: 0.1130 valAcc: 69.43% AUC: 65.94%
Epoch: 100 Loss: 0.0203 valLoss: 0.1131 valAcc: 70.09% AUC: 65.95%
Epoch: 101 Loss: 0.0202 valLoss: 0.1132 valAcc: 70.26% AUC: 65.98%
Epoch: 102 Loss: 0.0202 valLoss: 0.1133 valAcc: 70.54% AUC: 65.93%
Epoch: 103 Loss: 0.0190 valLoss: 0.1135 valAcc: 70.71% AUC: 65.91%
Epoch: 104 Loss: 0.0176 valLoss: 0.1136 valAcc: 70.71% AUC: 65.83%
Epoch: 105 Loss: 0.0190 valLoss: 0.1138 valAcc: 70.95% AUC: 65.78%
Epoch: 106 Loss: 0.0163 valLoss: 0.1139 valAcc: 70.88% AUC: 65.70%
Epoch: 107 Loss: 0.0195 valLoss: 0.1139 valAcc: 70.81% AUC: 65.62%
Epoch: 108 Loss: 0.0159 valLoss: 0.1140 valAcc: 70.88% AUC: 65.59%
Epoch: 109 Loss: 0.0184 valLoss: 0.1140 valAcc: 70.88% AUC: 65.63%
Epoch: 110 Loss: 0.0171 valLoss: 0.1140 valAcc: 70.95% AUC: 65.66%
Epoch: 111 Loss: 0.0187 valLoss: 0.1140 valAcc: 71.16% AUC: 65.65%
Epoch: 112 Loss: 0.0189 valLoss: 0.1140 valAcc: 70.92% AUC: 65.59%
Epoch: 113 Loss: 0.0160 valLoss: 0.1140 valAcc: 70.95% AUC: 65.60%
Epoch: 114 Loss: 0.0144 valLoss: 0.1142 valAcc: 71.23% AUC: 65.63%
Epoch: 115 Loss: 0.0162 valLoss: 0.1143 valAcc: 71.68% AUC: 65.71%
Epoch: 116 Loss: 0.0152 valLoss: 0.1143 valAcc: 72.02% AUC: 65.76%
Epoch: 117 Loss: 0.0145 valLoss: 0.1145 valAcc: 72.12% AUC: 65.76%
Epoch: 118 Loss: 0.0156 valLoss: 0.1146 valAcc: 72.16% AUC: 65.81%
Epoch: 119 Loss: 0.0145 valLoss: 0.1147 valAcc: 72.44% AUC: 65.79%
Epoch: 120 Loss: 0.0155 valLoss: 0.1150 valAcc: 72.78% AUC: 65.74%
Epoch: 121 Loss: 0.0131 valLoss: 0.1152 valAcc: 73.09% AUC: 65.74%
Epoch: 122 Loss: 0.0139 valLoss: 0.1156 valAcc: 73.82% AUC: 65.72%
Epoch: 123 Loss: 0.0134 valLoss: 0.1161 valAcc: 74.58% AUC: 65.69%
Epoch: 124 Loss: 0.0150 valLoss: 0.1163 valAcc: 74.96% AUC: 65.75%
Epoch: 125 Loss: 0.0126 valLoss: 0.1167 valAcc: 75.34% AUC: 65.78%
Epoch: 126 Loss: 0.0128 valLoss: 0.1170 valAcc: 75.65% AUC: 65.80%
Epoch: 127 Loss: 0.0118 valLoss: 0.1172 valAcc: 75.96% AUC: 65.85%
Epoch: 128 Loss: 0.0144 valLoss: 0.1172 valAcc: 75.75% AUC: 65.88%
Epoch: 129 Loss: 0.0108 valLoss: 0.1173 valAcc: 76.06% AUC: 65.90%
Epoch: 130 Loss: 0.0121 valLoss: 0.1175 valAcc: 76.23% AUC: 65.93%
Epoch: 131 Loss: 0.0126 valLoss: 0.1177 valAcc: 76.20% AUC: 65.91%
Epoch: 132 Loss: 0.0115 valLoss: 0.1176 valAcc: 75.89% AUC: 65.87%
Epoch: 133 Loss: 0.0109 valLoss: 0.1177 valAcc: 75.96% AUC: 65.87%
Epoch: 134 Loss: 0.0116 valLoss: 0.1177 valAcc: 75.89% AUC: 65.89%
Epoch: 135 Loss: 0.0123 valLoss: 0.1178 valAcc: 75.96% AUC: 65.87%
Epoch: 136 Loss: 0.0117 valLoss: 0.1180 valAcc: 76.20% AUC: 65.87%
Epoch: 137 Loss: 0.0120 valLoss: 0.1182 valAcc: 76.27% AUC: 65.87%
Epoch: 138 Loss: 0.0137 valLoss: 0.1182 valAcc: 76.06% AUC: 65.84%
Epoch: 139 Loss: 0.0110 valLoss: 0.1183 valAcc: 75.96% AUC: 65.85%
Epoch: 140 Loss: 0.0112 valLoss: 0.1184 valAcc: 75.85% AUC: 65.87%
Epoch: 141 Loss: 0.0105 valLoss: 0.1185 valAcc: 75.99% AUC: 65.92%
Epoch: 142 Loss: 0.0107 valLoss: 0.1188 valAcc: 76.13% AUC: 65.96%
Epoch: 143 Loss: 0.0121 valLoss: 0.1191 valAcc: 76.44% AUC: 65.99%
Epoch: 144 Loss: 0.0108 valLoss: 0.1194 valAcc: 76.72% AUC: 66.01%
Epoch: 145 Loss: 0.0095 valLoss: 0.1197 valAcc: 76.89% AUC: 66.02%
Epoch: 146 Loss: 0.0088 valLoss: 0.1200 valAcc: 76.89% AUC: 66.05%
Epoch: 147 Loss: 0.0100 valLoss: 0.1201 valAcc: 76.96% AUC: 66.07%
Epoch: 148 Loss: 0.0095 valLoss: 0.1204 valAcc: 77.10% AUC: 66.10%
Epoch: 149 Loss: 0.0098 valLoss: 0.1207 valAcc: 77.44% AUC: 66.13%
Epoch: 150 Loss: 0.0105 valLoss: 0.1211 valAcc: 77.55% AUC: 66.11%
Epoch: 151 Loss: 0.0100 valLoss: 0.1215 valAcc: 77.96% AUC: 66.08%
Epoch: 152 Loss: 0.0090 valLoss: 0.1218 valAcc: 78.17% AUC: 66.03%
Epoch: 153 Loss: 0.0095 valLoss: 0.1220 valAcc: 78.10% AUC: 65.97%
Epoch: 154 Loss: 0.0087 valLoss: 0.1222 valAcc: 78.13% AUC: 65.85%
Epoch: 155 Loss: 0.0094 valLoss: 0.1223 valAcc: 78.03% AUC: 65.77%
Epoch: 156 Loss: 0.0102 valLoss: 0.1224 valAcc: 77.96% AUC: 65.76%
Epoch: 157 Loss: 0.0096 valLoss: 0.1228 valAcc: 78.17% AUC: 65.83%
Epoch: 158 Loss: 0.0082 valLoss: 0.1233 valAcc: 78.41% AUC: 65.88%
Epoch: 159 Loss: 0.0084 valLoss: 0.1240 valAcc: 79.03% AUC: 65.97%
Epoch: 160 Loss: 0.0077 valLoss: 0.1245 valAcc: 79.55% AUC: 66.13%
Epoch: 161 Loss: 0.0080 valLoss: 0.1248 valAcc: 79.86% AUC: 66.29%
Epoch: 162 Loss: 0.0084 valLoss: 0.1251 valAcc: 80.41% AUC: 66.36%
Epoch: 163 Loss: 0.0084 valLoss: 0.1253 valAcc: 80.35% AUC: 66.43%
Epoch: 164 Loss: 0.0089 valLoss: 0.1253 valAcc: 80.35% AUC: 66.52%
Epoch: 165 Loss: 0.0089 valLoss: 0.1251 valAcc: 80.21% AUC: 66.57%
Epoch: 166 Loss: 0.0073 valLoss: 0.1252 valAcc: 80.17% AUC: 66.65%
Epoch: 167 Loss: 0.0071 valLoss: 0.1255 valAcc: 80.28% AUC: 66.69%
Epoch: 168 Loss: 0.0075 valLoss: 0.1259 valAcc: 80.24% AUC: 66.70%
Epoch: 169 Loss: 0.0070 valLoss: 0.1266 valAcc: 80.38% AUC: 66.65%
Epoch: 170 Loss: 0.0086 valLoss: 0.1271 valAcc: 80.38% AUC: 66.57%
Epoch: 171 Loss: 0.0083 valLoss: 0.1275 valAcc: 80.48% AUC: 66.46%
Epoch: 172 Loss: 0.0069 valLoss: 0.1281 valAcc: 80.45% AUC: 66.38%
Epoch: 173 Loss: 0.0073 valLoss: 0.1285 valAcc: 80.45% AUC: 66.27%
Epoch: 174 Loss: 0.0071 valLoss: 0.1292 valAcc: 80.62% AUC: 66.15%
Epoch: 175 Loss: 0.0074 valLoss: 0.1301 valAcc: 80.90% AUC: 66.03%
Epoch: 176 Loss: 0.0070 valLoss: 0.1309 valAcc: 81.07% AUC: 65.96%
Epoch: 177 Loss: 0.0072 valLoss: 0.1319 valAcc: 81.59% AUC: 65.92%
Epoch: 178 Loss: 0.0064 valLoss: 0.1329 valAcc: 81.93% AUC: 65.91%
Epoch: 179 Loss: 0.0062 valLoss: 0.1334 valAcc: 82.00% AUC: 65.98%
Epoch: 180 Loss: 0.0068 valLoss: 0.1334 valAcc: 81.97% AUC: 66.07%
Epoch: 181 Loss: 0.0064 valLoss: 0.1335 valAcc: 82.04% AUC: 66.17%
Epoch: 182 Loss: 0.0059 valLoss: 0.1335 valAcc: 82.04% AUC: 66.26%
Epoch: 183 Loss: 0.0062 valLoss: 0.1338 valAcc: 82.07% AUC: 66.33%
Epoch: 184 Loss: 0.0070 valLoss: 0.1342 valAcc: 82.04% AUC: 66.45%
Epoch: 185 Loss: 0.0062 valLoss: 0.1347 valAcc: 82.21% AUC: 66.51%
Epoch: 186 Loss: 0.0068 valLoss: 0.1350 valAcc: 82.25% AUC: 66.53%
Epoch: 187 Loss: 0.0069 valLoss: 0.1350 valAcc: 82.14% AUC: 66.50%
Epoch: 188 Loss: 0.0060 valLoss: 0.1352 valAcc: 82.25% AUC: 66.46%
Epoch: 189 Loss: 0.0052 valLoss: 0.1358 valAcc: 82.31% AUC: 66.44%
Epoch: 190 Loss: 0.0058 valLoss: 0.1362 valAcc: 82.42% AUC: 66.43%
Epoch: 191 Loss: 0.0056 valLoss: 0.1367 valAcc: 82.42% AUC: 66.45%
Epoch: 192 Loss: 0.0065 valLoss: 0.1371 valAcc: 82.49% AUC: 66.46%
Epoch: 193 Loss: 0.0055 valLoss: 0.1375 valAcc: 82.45% AUC: 66.40%
Epoch: 194 Loss: 0.0056 valLoss: 0.1378 valAcc: 82.45% AUC: 66.34%
Epoch: 195 Loss: 0.0057 valLoss: 0.1381 valAcc: 82.42% AUC: 66.31%
Epoch: 196 Loss: 0.0053 valLoss: 0.1384 valAcc: 82.28% AUC: 66.30%
Epoch: 197 Loss: 0.0052 valLoss: 0.1390 valAcc: 82.28% AUC: 66.30%
Epoch: 198 Loss: 0.0051 valLoss: 0.1398 valAcc: 82.35% AUC: 66.29%
Epoch: 199 Loss: 0.0060 valLoss: 0.1404 valAcc: 82.45% AUC: 66.30%
Epoch: 200 Loss: 0.0050 valLoss: 0.1411 valAcc: 82.59% AUC: 66.28%
Epoch: 201 Loss: 0.0057 valLoss: 0.1418 valAcc: 82.76% AUC: 66.29%
Epoch: 202 Loss: 0.0056 valLoss: 0.1421 valAcc: 83.01% AUC: 66.33%
Epoch: 203 Loss: 0.0052 valLoss: 0.1424 valAcc: 83.01% AUC: 66.32%
Epoch: 204 Loss: 0.0050 valLoss: 0.1429 valAcc: 83.11% AUC: 66.32%
Epoch: 205 Loss: 0.0051 valLoss: 0.1432 valAcc: 83.25% AUC: 66.30%
Epoch: 206 Loss: 0.0053 valLoss: 0.1434 valAcc: 83.28% AUC: 66.27%
Epoch: 207 Loss: 0.0057 valLoss: 0.1434 valAcc: 83.11% AUC: 66.22%
Epoch: 208 Loss: 0.0054 valLoss: 0.1434 valAcc: 82.87% AUC: 66.23%
Epoch: 209 Loss: 0.0049 valLoss: 0.1437 valAcc: 82.87% AUC: 66.28%
Epoch: 210 Loss: 0.0051 valLoss: 0.1440 valAcc: 82.97% AUC: 66.32%
Epoch: 211 Loss: 0.0046 valLoss: 0.1444 valAcc: 83.11% AUC: 66.33%
Epoch: 212 Loss: 0.0045 valLoss: 0.1452 valAcc: 83.32% AUC: 66.31%
Epoch: 213 Loss: 0.0043 valLoss: 0.1461 valAcc: 83.56% AUC: 66.29%
Epoch: 214 Loss: 0.0041 valLoss: 0.1470 valAcc: 83.70% AUC: 66.28%
Epoch: 215 Loss: 0.0056 valLoss: 0.1477 valAcc: 83.73% AUC: 66.27%
Epoch: 216 Loss: 0.0049 valLoss: 0.1484 valAcc: 83.94% AUC: 66.28%
Epoch: 217 Loss: 0.0050 valLoss: 0.1489 valAcc: 84.08% AUC: 66.27%
Epoch: 218 Loss: 0.0051 valLoss: 0.1492 valAcc: 84.11% AUC: 66.27%
Epoch: 219 Loss: 0.0040 valLoss: 0.1493 valAcc: 84.04% AUC: 66.30%
Epoch: 220 Loss: 0.0043 valLoss: 0.1492 valAcc: 83.97% AUC: 66.32%
Epoch: 221 Loss: 0.0038 valLoss: 0.1495 valAcc: 84.04% AUC: 66.31%
Epoch: 222 Loss: 0.0044 valLoss: 0.1496 valAcc: 83.97% AUC: 66.31%
Epoch: 223 Loss: 0.0035 valLoss: 0.1499 valAcc: 83.94% AUC: 66.30%
Epoch: 224 Loss: 0.0044 valLoss: 0.1503 valAcc: 83.90% AUC: 66.31%
Epoch: 225 Loss: 0.0050 valLoss: 0.1506 valAcc: 84.11% AUC: 66.31%
Epoch: 226 Loss: 0.0043 valLoss: 0.1508 valAcc: 84.08% AUC: 66.27%
Epoch: 227 Loss: 0.0043 valLoss: 0.1514 valAcc: 84.08% AUC: 66.20%
Epoch: 228 Loss: 0.0047 valLoss: 0.1519 valAcc: 84.08% AUC: 66.18%
Epoch: 229 Loss: 0.0041 valLoss: 0.1523 valAcc: 84.18% AUC: 66.16%
Epoch: 230 Loss: 0.0039 valLoss: 0.1526 valAcc: 84.18% AUC: 66.17%
Epoch: 231 Loss: 0.0043 valLoss: 0.1528 valAcc: 83.97% AUC: 66.12%
Epoch: 232 Loss: 0.0034 valLoss: 0.1531 valAcc: 83.83% AUC: 66.06%
Epoch: 233 Loss: 0.0035 valLoss: 0.1535 valAcc: 83.73% AUC: 66.05%
Epoch: 234 Loss: 0.0039 valLoss: 0.1541 valAcc: 83.83% AUC: 66.03%
Epoch: 235 Loss: 0.0042 valLoss: 0.1545 valAcc: 83.94% AUC: 66.08%
Epoch: 236 Loss: 0.0033 valLoss: 0.1551 valAcc: 84.01% AUC: 66.12%
Epoch: 237 Loss: 0.0040 valLoss: 0.1557 valAcc: 83.97% AUC: 66.14%
Epoch: 238 Loss: 0.0042 valLoss: 0.1563 valAcc: 84.11% AUC: 66.17%
Epoch: 239 Loss: 0.0037 valLoss: 0.1566 valAcc: 84.08% AUC: 66.15%
Epoch: 240 Loss: 0.0036 valLoss: 0.1567 valAcc: 84.08% AUC: 66.11%
Epoch: 241 Loss: 0.0035 valLoss: 0.1569 valAcc: 83.94% AUC: 66.06%
Epoch: 242 Loss: 0.0030 valLoss: 0.1573 valAcc: 84.01% AUC: 66.04%
Epoch: 243 Loss: 0.0034 valLoss: 0.1578 valAcc: 84.04% AUC: 66.05%
Epoch: 244 Loss: 0.0033 valLoss: 0.1586 valAcc: 84.04% AUC: 66.09%
Epoch: 245 Loss: 0.0039 valLoss: 0.1586 valAcc: 84.21% AUC: 66.17%
Epoch: 246 Loss: 0.0035 valLoss: 0.1590 valAcc: 84.28% AUC: 66.23%
Epoch: 247 Loss: 0.0034 valLoss: 0.1591 valAcc: 84.42% AUC: 66.34%
Restoring model weights from the best epoch: 167 with the best VAL_AUC: 66.69%
Training time: 0:04:16.987324
## Testing ../data/prepared_epitope_testing.csv at: 21-12-2023#14:26:18 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 2)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.203104
The best cut-off value is: 0.54
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[14957  1168]
 [ 1168   217]]
ValAcc: 86.66% specScore: 92.76% presScore: 15.67% recallScore: 15.67% F1Score: 15.67% MCC: 8.42% AUC: 62.97% AP: 12.08%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 09-01-2024#12:16:02 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1681 valLoss: 0.1340 valAcc: 11.33% AUC: 53.24%
Epoch: 01 Loss: 0.1432 valLoss: 0.1326 valAcc: 13.75% AUC: 59.14%
Epoch: 02 Loss: 0.1375 valLoss: 0.1321 valAcc: 22.11% AUC: 60.78%
Epoch: 03 Loss: 0.1357 valLoss: 0.1314 valAcc: 32.73% AUC: 62.13%
Epoch: 04 Loss: 0.1330 valLoss: 0.1304 valAcc: 41.49% AUC: 63.86%
Epoch: 05 Loss: 0.1298 valLoss: 0.1305 valAcc: 51.10% AUC: 64.85%
Epoch: 06 Loss: 0.1279 valLoss: 0.1326 valAcc: 61.12% AUC: 65.65%
Epoch: 07 Loss: 0.1259 valLoss: 0.1312 valAcc: 59.57% AUC: 66.48%
Epoch: 08 Loss: 0.1244 valLoss: 0.1303 valAcc: 58.67% AUC: 66.95%
Epoch: 09 Loss: 0.1228 valLoss: 0.1318 valAcc: 61.00% AUC: 67.05%
Epoch: 10 Loss: 0.1202 valLoss: 0.1308 valAcc: 59.55% AUC: 67.58%
Epoch: 11 Loss: 0.1190 valLoss: 0.1303 valAcc: 56.29% AUC: 67.51%
Epoch: 12 Loss: 0.1171 valLoss: 0.1331 valAcc: 60.16% AUC: 67.60%
Epoch: 13 Loss: 0.1148 valLoss: 0.1325 valAcc: 56.93% AUC: 67.74%
Epoch: 14 Loss: 0.1124 valLoss: 0.1336 valAcc: 55.90% AUC: 67.65%
Epoch: 15 Loss: 0.1105 valLoss: 0.1407 valAcc: 60.69% AUC: 67.20%
Epoch: 16 Loss: 0.1082 valLoss: 0.1459 valAcc: 62.56% AUC: 67.42%
Epoch: 17 Loss: 0.1061 valLoss: 0.1508 valAcc: 64.01% AUC: 67.45%
Epoch: 18 Loss: 0.1023 valLoss: 0.1569 valAcc: 65.04% AUC: 67.28%
Epoch: 19 Loss: 0.0992 valLoss: 0.1668 valAcc: 66.61% AUC: 67.19%
Epoch: 20 Loss: 0.0982 valLoss: 0.1781 valAcc: 69.18% AUC: 66.83%
Epoch: 21 Loss: 0.0942 valLoss: 0.1963 valAcc: 71.67% AUC: 66.98%
Epoch: 22 Loss: 0.0913 valLoss: 0.1957 valAcc: 71.28% AUC: 67.07%
Epoch: 23 Loss: 0.0887 valLoss: 0.2171 valAcc: 73.42% AUC: 66.60%
Epoch: 24 Loss: 0.0869 valLoss: 0.2134 valAcc: 72.31% AUC: 66.52%
Epoch: 25 Loss: 0.0829 valLoss: 0.2261 valAcc: 73.77% AUC: 66.77%
Epoch: 26 Loss: 0.0812 valLoss: 0.2268 valAcc: 72.76% AUC: 66.91%
Epoch: 27 Loss: 0.0777 valLoss: 0.2508 valAcc: 76.22% AUC: 66.66%
Epoch: 28 Loss: 0.0760 valLoss: 0.2637 valAcc: 75.57% AUC: 66.65%
Epoch: 29 Loss: 0.0728 valLoss: 0.2473 valAcc: 73.54% AUC: 66.71%
Epoch: 30 Loss: 0.0710 valLoss: 0.2866 valAcc: 77.47% AUC: 66.97%
Epoch: 31 Loss: 0.0677 valLoss: 0.2761 valAcc: 75.89% AUC: 66.97%
Epoch: 32 Loss: 0.0656 valLoss: 0.2807 valAcc: 76.48% AUC: 67.16%
Epoch: 33 Loss: 0.0640 valLoss: 0.2630 valAcc: 73.18% AUC: 66.35%
Epoch: 34 Loss: 0.0615 valLoss: 0.3127 valAcc: 78.29% AUC: 67.35%
Epoch: 35 Loss: 0.0596 valLoss: 0.3349 valAcc: 79.04% AUC: 67.39%
Epoch: 36 Loss: 0.0582 valLoss: 0.3232 valAcc: 79.31% AUC: 67.98%
Epoch: 37 Loss: 0.0558 valLoss: 0.3184 valAcc: 78.73% AUC: 67.85%
Epoch: 38 Loss: 0.0537 valLoss: 0.3196 valAcc: 78.71% AUC: 67.54%
Epoch: 39 Loss: 0.0512 valLoss: 0.3446 valAcc: 79.37% AUC: 67.52%
Epoch: 40 Loss: 0.0505 valLoss: 0.3803 valAcc: 81.09% AUC: 67.53%
Epoch: 41 Loss: 0.0486 valLoss: 0.3471 valAcc: 78.84% AUC: 67.95%
Epoch: 42 Loss: 0.0473 valLoss: 0.3247 valAcc: 77.81% AUC: 67.51%
Epoch: 43 Loss: 0.0457 valLoss: 0.3535 valAcc: 79.02% AUC: 67.09%
Epoch: 44 Loss: 0.0428 valLoss: 0.3810 valAcc: 80.92% AUC: 67.84%
Epoch: 45 Loss: 0.0423 valLoss: 0.3908 valAcc: 80.98% AUC: 67.43%
Epoch: 46 Loss: 0.0406 valLoss: 0.3874 valAcc: 79.97% AUC: 67.06%
Epoch: 47 Loss: 0.0392 valLoss: 0.3588 valAcc: 79.30% AUC: 67.19%
Epoch: 48 Loss: 0.0383 valLoss: 0.3566 valAcc: 78.93% AUC: 67.29%
Epoch: 49 Loss: 0.0370 valLoss: 0.3826 valAcc: 80.50% AUC: 67.34%
Epoch: 50 Loss: 0.0361 valLoss: 0.4425 valAcc: 82.72% AUC: 67.09%
Epoch: 51 Loss: 0.0347 valLoss: 0.3500 valAcc: 77.16% AUC: 66.95%
Epoch: 52 Loss: 0.0341 valLoss: 0.3751 valAcc: 79.56% AUC: 66.91%
Epoch: 53 Loss: 0.0332 valLoss: 0.3902 valAcc: 79.82% AUC: 66.96%
Epoch: 54 Loss: 0.0322 valLoss: 0.3914 valAcc: 79.89% AUC: 67.45%
Epoch: 55 Loss: 0.0317 valLoss: 0.3955 valAcc: 79.98% AUC: 67.85%
Epoch: 56 Loss: 0.0304 valLoss: 0.3735 valAcc: 78.38% AUC: 67.22%
Epoch: 57 Loss: 0.0300 valLoss: 0.3955 valAcc: 79.45% AUC: 67.37%
Epoch: 58 Loss: 0.0288 valLoss: 0.4162 valAcc: 80.27% AUC: 67.27%
Epoch: 59 Loss: 0.0288 valLoss: 0.4253 valAcc: 80.61% AUC: 67.58%
Epoch: 60 Loss: 0.0285 valLoss: 0.4261 valAcc: 80.52% AUC: 67.43%
Epoch: 61 Loss: 0.0287 valLoss: 0.4447 valAcc: 81.46% AUC: 67.33%
Epoch: 62 Loss: 0.0284 valLoss: 0.4779 valAcc: 82.80% AUC: 67.08%
Epoch: 63 Loss: 0.0269 valLoss: 0.5282 valAcc: 83.83% AUC: 67.20%
Epoch: 64 Loss: 0.0265 valLoss: 0.4963 valAcc: 83.05% AUC: 66.50%
Epoch: 65 Loss: 0.0261 valLoss: 0.4650 valAcc: 82.12% AUC: 67.43%
Epoch: 66 Loss: 0.0230 valLoss: 0.4798 valAcc: 82.47% AUC: 67.23%
Epoch: 67 Loss: 0.0224 valLoss: 0.4998 valAcc: 83.20% AUC: 67.41%
Epoch: 68 Loss: 0.0216 valLoss: 0.4798 valAcc: 82.05% AUC: 67.14%
Epoch: 69 Loss: 0.0221 valLoss: 0.4940 valAcc: 82.70% AUC: 67.18%
Epoch: 70 Loss: 0.0204 valLoss: 0.4819 valAcc: 81.93% AUC: 67.05%
Epoch: 71 Loss: 0.0198 valLoss: 0.5179 valAcc: 82.91% AUC: 67.09%
Epoch: 72 Loss: 0.0195 valLoss: 0.4940 valAcc: 81.45% AUC: 66.78%
Epoch: 73 Loss: 0.0193 valLoss: 0.4750 valAcc: 81.77% AUC: 67.20%
Epoch: 74 Loss: 0.0181 valLoss: 0.5304 valAcc: 83.44% AUC: 67.11%
Epoch: 75 Loss: 0.0172 valLoss: 0.5296 valAcc: 83.01% AUC: 66.89%
Epoch: 76 Loss: 0.0165 valLoss: 0.5239 valAcc: 82.92% AUC: 67.08%
Epoch: 77 Loss: 0.0164 valLoss: 0.5295 valAcc: 83.16% AUC: 66.93%
Epoch: 78 Loss: 0.0159 valLoss: 0.5095 valAcc: 81.99% AUC: 66.63%
Epoch: 79 Loss: 0.0160 valLoss: 0.4641 valAcc: 80.34% AUC: 67.07%
Epoch: 80 Loss: 0.0158 valLoss: 0.5037 valAcc: 82.01% AUC: 67.28%
Epoch: 81 Loss: 0.0156 valLoss: 0.5092 valAcc: 82.22% AUC: 66.71%
Epoch: 82 Loss: 0.0161 valLoss: 0.5123 valAcc: 82.21% AUC: 67.10%
Epoch: 83 Loss: 0.0150 valLoss: 0.5189 valAcc: 81.79% AUC: 66.90%
Epoch: 84 Loss: 0.0150 valLoss: 0.5268 valAcc: 82.28% AUC: 66.97%
Epoch: 85 Loss: 0.0143 valLoss: 0.4932 valAcc: 80.68% AUC: 67.12%
Epoch: 86 Loss: 0.0148 valLoss: 0.5169 valAcc: 82.21% AUC: 66.81%
Epoch: 87 Loss: 0.0150 valLoss: 0.4918 valAcc: 80.93% AUC: 66.68%
Epoch: 88 Loss: 0.0137 valLoss: 0.5238 valAcc: 81.94% AUC: 66.76%
Epoch: 89 Loss: 0.0140 valLoss: 0.5453 valAcc: 82.40% AUC: 67.15%
Epoch: 90 Loss: 0.0139 valLoss: 0.5357 valAcc: 81.97% AUC: 66.99%
Epoch: 91 Loss: 0.0130 valLoss: 0.5206 valAcc: 81.19% AUC: 67.25%
Epoch: 92 Loss: 0.0130 valLoss: 0.5456 valAcc: 82.16% AUC: 66.58%
Epoch: 93 Loss: 0.0127 valLoss: 0.6100 valAcc: 84.38% AUC: 66.85%
Epoch: 94 Loss: 0.0130 valLoss: 0.5933 valAcc: 83.84% AUC: 66.76%
Epoch: 95 Loss: 0.0119 valLoss: 0.5840 valAcc: 83.57% AUC: 67.09%
Epoch: 96 Loss: 0.0105 valLoss: 0.5929 valAcc: 83.66% AUC: 66.81%
Epoch: 97 Loss: 0.0106 valLoss: 0.5652 valAcc: 82.76% AUC: 67.01%
Epoch: 98 Loss: 0.0109 valLoss: 0.5937 valAcc: 83.74% AUC: 66.78%
Epoch: 99 Loss: 0.0104 valLoss: 0.5768 valAcc: 83.16% AUC: 67.04%
Epoch: 100 Loss: 0.0114 valLoss: 0.5640 valAcc: 82.59% AUC: 66.51%
Epoch: 101 Loss: 0.0097 valLoss: 0.5901 valAcc: 83.36% AUC: 66.61%
Epoch: 102 Loss: 0.0107 valLoss: 0.5602 valAcc: 82.94% AUC: 66.82%
Epoch: 103 Loss: 0.0097 valLoss: 0.5850 valAcc: 82.89% AUC: 66.39%
Epoch: 104 Loss: 0.0093 valLoss: 0.5790 valAcc: 82.95% AUC: 66.64%
Epoch: 105 Loss: 0.0088 valLoss: 0.5921 valAcc: 83.01% AUC: 66.63%
Epoch: 106 Loss: 0.0095 valLoss: 0.5592 valAcc: 81.88% AUC: 66.86%
Epoch: 107 Loss: 0.0097 valLoss: 0.5667 valAcc: 82.10% AUC: 66.45%
Epoch: 108 Loss: 0.0090 valLoss: 0.5655 valAcc: 82.12% AUC: 66.51%
Epoch: 109 Loss: 0.0091 valLoss: 0.5814 valAcc: 82.28% AUC: 66.49%
Epoch: 110 Loss: 0.0092 valLoss: 0.5753 valAcc: 82.02% AUC: 66.52%
Epoch: 111 Loss: 0.0077 valLoss: 0.6393 valAcc: 83.78% AUC: 66.13%
Epoch: 112 Loss: 0.0079 valLoss: 0.5959 valAcc: 82.96% AUC: 66.94%
Epoch: 113 Loss: 0.0076 valLoss: 0.6295 valAcc: 83.66% AUC: 66.17%
Epoch: 114 Loss: 0.0082 valLoss: 0.6236 valAcc: 83.11% AUC: 65.88%
Epoch: 115 Loss: 0.0078 valLoss: 0.6082 valAcc: 82.97% AUC: 66.38%
Epoch: 116 Loss: 0.0077 valLoss: 0.6175 valAcc: 82.80% AUC: 65.80%
Restoring model weights from the best epoch: 36 with the best VAL_AUC: 67.98%
Training time: 0:35:24.428691
## Testing ../data/prepared_epitope_testing.csv at: 09-01-2024#12:53:48 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.196772
The best cut-off value is: 0.48
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15041  1084]
 [ 1084   301]]
ValAcc: 87.62% specScore: 93.28% presScore: 21.73% recallScore: 21.73% F1Score: 21.73% MCC: 15.01% AUC: 70.09% AP: 16.64%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 09-01-2024#13:17:53 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1673 valLoss: 0.1329 valAcc: 11.32% AUC: 54.60%
Epoch: 01 Loss: 0.1451 valLoss: 0.1319 valAcc: 11.32% AUC: 57.86%
Epoch: 02 Loss: 0.1387 valLoss: 0.1313 valAcc: 11.32% AUC: 59.69%
Epoch: 03 Loss: 0.1350 valLoss: 0.1309 valAcc: 11.35% AUC: 60.35%
Epoch: 04 Loss: 0.1335 valLoss: 0.1305 valAcc: 11.52% AUC: 60.94%
Epoch: 05 Loss: 0.1317 valLoss: 0.1301 valAcc: 11.88% AUC: 61.97%
Epoch: 06 Loss: 0.1293 valLoss: 0.1293 valAcc: 13.45% AUC: 63.27%
Epoch: 07 Loss: 0.1270 valLoss: 0.1282 valAcc: 20.42% AUC: 64.16%
Epoch: 08 Loss: 0.1254 valLoss: 0.1275 valAcc: 23.33% AUC: 64.88%
Epoch: 09 Loss: 0.1228 valLoss: 0.1267 valAcc: 25.45% AUC: 65.69%
Epoch: 10 Loss: 0.1204 valLoss: 0.1263 valAcc: 32.49% AUC: 66.17%
Epoch: 11 Loss: 0.1187 valLoss: 0.1260 valAcc: 33.03% AUC: 66.60%
Epoch: 12 Loss: 0.1160 valLoss: 0.1267 valAcc: 39.56% AUC: 66.93%
Epoch: 13 Loss: 0.1127 valLoss: 0.1313 valAcc: 50.06% AUC: 67.15%
Epoch: 14 Loss: 0.1101 valLoss: 0.1322 valAcc: 49.25% AUC: 67.14%
Epoch: 15 Loss: 0.1072 valLoss: 0.1346 valAcc: 50.57% AUC: 67.23%
Epoch: 16 Loss: 0.1044 valLoss: 0.1469 valAcc: 57.50% AUC: 66.74%
Epoch: 17 Loss: 0.1011 valLoss: 0.1541 valAcc: 58.16% AUC: 66.61%
Epoch: 18 Loss: 0.0980 valLoss: 0.1680 valAcc: 62.53% AUC: 66.89%
Epoch: 19 Loss: 0.0947 valLoss: 0.1866 valAcc: 66.86% AUC: 66.57%
Epoch: 20 Loss: 0.0931 valLoss: 0.1840 valAcc: 64.26% AUC: 65.93%
Epoch: 21 Loss: 0.0898 valLoss: 0.2081 valAcc: 69.25% AUC: 65.83%
Epoch: 22 Loss: 0.0883 valLoss: 0.1873 valAcc: 63.65% AUC: 66.26%
Epoch: 23 Loss: 0.0848 valLoss: 0.1921 valAcc: 63.86% AUC: 65.74%
Epoch: 24 Loss: 0.0824 valLoss: 0.2011 valAcc: 64.86% AUC: 65.31%
Epoch: 25 Loss: 0.0800 valLoss: 0.1985 valAcc: 62.63% AUC: 65.25%
Epoch: 26 Loss: 0.0773 valLoss: 0.1985 valAcc: 64.43% AUC: 65.24%
Epoch: 27 Loss: 0.0752 valLoss: 0.2000 valAcc: 64.18% AUC: 65.72%
Epoch: 28 Loss: 0.0717 valLoss: 0.2162 valAcc: 67.06% AUC: 65.60%
Epoch: 29 Loss: 0.0690 valLoss: 0.2211 valAcc: 68.68% AUC: 65.53%
Epoch: 30 Loss: 0.0677 valLoss: 0.2161 valAcc: 66.64% AUC: 65.22%
Epoch: 31 Loss: 0.0652 valLoss: 0.2160 valAcc: 67.83% AUC: 65.38%
Epoch: 32 Loss: 0.0618 valLoss: 0.2412 valAcc: 72.02% AUC: 65.73%
Epoch: 33 Loss: 0.0603 valLoss: 0.2527 valAcc: 73.30% AUC: 65.96%
Epoch: 34 Loss: 0.0575 valLoss: 0.2541 valAcc: 72.83% AUC: 66.00%
Epoch: 35 Loss: 0.0555 valLoss: 0.2360 valAcc: 71.12% AUC: 66.29%
Epoch: 36 Loss: 0.0533 valLoss: 0.2716 valAcc: 75.72% AUC: 66.12%
Epoch: 37 Loss: 0.0516 valLoss: 0.2607 valAcc: 73.51% AUC: 66.35%
Epoch: 38 Loss: 0.0495 valLoss: 0.2773 valAcc: 75.52% AUC: 65.97%
Epoch: 39 Loss: 0.0490 valLoss: 0.2884 valAcc: 76.63% AUC: 65.80%
Epoch: 40 Loss: 0.0460 valLoss: 0.2911 valAcc: 76.86% AUC: 66.08%
Epoch: 41 Loss: 0.0450 valLoss: 0.3007 valAcc: 77.68% AUC: 66.32%
Epoch: 42 Loss: 0.0428 valLoss: 0.3033 valAcc: 77.15% AUC: 66.08%
Epoch: 43 Loss: 0.0413 valLoss: 0.3031 valAcc: 77.43% AUC: 66.60%
Epoch: 44 Loss: 0.0399 valLoss: 0.2893 valAcc: 75.95% AUC: 66.36%
Epoch: 45 Loss: 0.0391 valLoss: 0.3247 valAcc: 79.00% AUC: 66.36%
Epoch: 46 Loss: 0.0377 valLoss: 0.3023 valAcc: 76.90% AUC: 66.54%
Epoch: 47 Loss: 0.0363 valLoss: 0.3153 valAcc: 77.51% AUC: 65.68%
Epoch: 48 Loss: 0.0344 valLoss: 0.3223 valAcc: 77.71% AUC: 65.84%
Epoch: 49 Loss: 0.0339 valLoss: 0.3132 valAcc: 77.36% AUC: 66.08%
Epoch: 50 Loss: 0.0332 valLoss: 0.3051 valAcc: 76.48% AUC: 66.45%
Epoch: 51 Loss: 0.0316 valLoss: 0.3013 valAcc: 75.04% AUC: 65.94%
Epoch: 52 Loss: 0.0309 valLoss: 0.3076 valAcc: 76.18% AUC: 66.18%
Epoch: 53 Loss: 0.0302 valLoss: 0.3110 valAcc: 76.36% AUC: 66.33%
Epoch: 54 Loss: 0.0299 valLoss: 0.2932 valAcc: 73.52% AUC: 66.45%
Epoch: 55 Loss: 0.0294 valLoss: 0.2891 valAcc: 73.42% AUC: 66.34%
Epoch: 56 Loss: 0.0282 valLoss: 0.3232 valAcc: 75.91% AUC: 66.39%
Epoch: 57 Loss: 0.0270 valLoss: 0.3094 valAcc: 74.69% AUC: 66.24%
Epoch: 58 Loss: 0.0273 valLoss: 0.3137 valAcc: 74.96% AUC: 66.45%
Epoch: 59 Loss: 0.0256 valLoss: 0.3321 valAcc: 76.33% AUC: 66.36%
Epoch: 60 Loss: 0.0240 valLoss: 0.3265 valAcc: 75.99% AUC: 66.48%
Epoch: 61 Loss: 0.0225 valLoss: 0.3406 valAcc: 77.30% AUC: 66.68%
Epoch: 62 Loss: 0.0221 valLoss: 0.3545 valAcc: 77.62% AUC: 66.37%
Epoch: 63 Loss: 0.0220 valLoss: 0.3386 valAcc: 75.84% AUC: 65.96%
Epoch: 64 Loss: 0.0215 valLoss: 0.3776 valAcc: 78.97% AUC: 66.48%
Epoch: 65 Loss: 0.0205 valLoss: 0.4107 valAcc: 80.36% AUC: 66.24%
Epoch: 66 Loss: 0.0201 valLoss: 0.3739 valAcc: 77.94% AUC: 66.47%
Epoch: 67 Loss: 0.0191 valLoss: 0.3958 valAcc: 79.48% AUC: 66.04%
Epoch: 68 Loss: 0.0181 valLoss: 0.4030 valAcc: 79.19% AUC: 65.58%
Epoch: 69 Loss: 0.0182 valLoss: 0.4104 valAcc: 80.54% AUC: 66.08%
Epoch: 70 Loss: 0.0181 valLoss: 0.4197 valAcc: 80.47% AUC: 65.62%
Epoch: 71 Loss: 0.0170 valLoss: 0.4181 valAcc: 79.86% AUC: 65.44%
Epoch: 72 Loss: 0.0169 valLoss: 0.4110 valAcc: 79.24% AUC: 65.28%
Epoch: 73 Loss: 0.0167 valLoss: 0.4083 valAcc: 79.08% AUC: 65.62%
Epoch: 74 Loss: 0.0159 valLoss: 0.4519 valAcc: 81.17% AUC: 65.52%
Epoch: 75 Loss: 0.0154 valLoss: 0.4206 valAcc: 79.81% AUC: 65.51%
Epoch: 76 Loss: 0.0152 valLoss: 0.4390 valAcc: 80.70% AUC: 65.40%
Epoch: 77 Loss: 0.0146 valLoss: 0.4385 valAcc: 80.09% AUC: 65.61%
Epoch: 78 Loss: 0.0142 valLoss: 0.4635 valAcc: 80.80% AUC: 65.55%
Epoch: 79 Loss: 0.0144 valLoss: 0.4541 valAcc: 80.72% AUC: 65.81%
Epoch: 80 Loss: 0.0144 valLoss: 0.4271 valAcc: 79.60% AUC: 65.79%
Epoch: 81 Loss: 0.0142 valLoss: 0.4108 valAcc: 77.94% AUC: 65.61%
Epoch: 82 Loss: 0.0145 valLoss: 0.3788 valAcc: 75.30% AUC: 65.86%
Epoch: 83 Loss: 0.0138 valLoss: 0.4197 valAcc: 78.70% AUC: 65.79%
Epoch: 84 Loss: 0.0128 valLoss: 0.4402 valAcc: 78.67% AUC: 64.90%
Epoch: 85 Loss: 0.0127 valLoss: 0.4030 valAcc: 76.71% AUC: 65.35%
Epoch: 86 Loss: 0.0125 valLoss: 0.4330 valAcc: 78.82% AUC: 65.33%
Epoch: 87 Loss: 0.0115 valLoss: 0.4415 valAcc: 78.63% AUC: 65.08%
Epoch: 88 Loss: 0.0116 valLoss: 0.4751 valAcc: 80.64% AUC: 65.66%
Epoch: 89 Loss: 0.0106 valLoss: 0.5006 valAcc: 81.50% AUC: 65.68%
Epoch: 90 Loss: 0.0103 valLoss: 0.4889 valAcc: 81.39% AUC: 66.23%
Epoch: 91 Loss: 0.0105 valLoss: 0.4607 valAcc: 80.04% AUC: 66.09%
Epoch: 92 Loss: 0.0108 valLoss: 0.4914 valAcc: 80.99% AUC: 65.17%
Epoch: 93 Loss: 0.0099 valLoss: 0.4494 valAcc: 79.63% AUC: 65.58%
Restoring model weights from the best epoch: 13 with the best VAL_AUC: 67.15%
Training time: 0:27:52.599500
## Testing ../data/prepared_epitope_testing.csv at: 09-01-2024#13:47:07 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.199234
The best cut-off value is: 0.68
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15043  1082]
 [ 1082   303]]
ValAcc: 87.64% specScore: 93.29% presScore: 21.88% recallScore: 21.88% F1Score: 21.88% MCC: 15.17% AUC: 70.49% AP: 16.80%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 09-01-2024#13:52:29 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
Epoch: 00 Loss: 0.1619 valLoss: 0.1329 valAcc: 11.32% AUC: 55.88%
Epoch: 01 Loss: 0.1404 valLoss: 0.1316 valAcc: 11.32% AUC: 59.42%
Epoch: 02 Loss: 0.1367 valLoss: 0.1307 valAcc: 11.33% AUC: 61.35%
Epoch: 03 Loss: 0.1349 valLoss: 0.1300 valAcc: 11.66% AUC: 62.01%
Epoch: 04 Loss: 0.1325 valLoss: 0.1299 valAcc: 14.82% AUC: 62.83%
Epoch: 05 Loss: 0.1299 valLoss: 0.1294 valAcc: 27.64% AUC: 64.20%
Epoch: 06 Loss: 0.1270 valLoss: 0.1290 valAcc: 43.37% AUC: 65.27%
Epoch: 07 Loss: 0.1250 valLoss: 0.1289 valAcc: 52.23% AUC: 66.38%
Epoch: 08 Loss: 0.1226 valLoss: 0.1284 valAcc: 56.05% AUC: 67.28%
Epoch: 09 Loss: 0.1210 valLoss: 0.1277 valAcc: 55.39% AUC: 67.71%
Epoch: 10 Loss: 0.1184 valLoss: 0.1303 valAcc: 62.60% AUC: 68.33%
Epoch: 11 Loss: 0.1163 valLoss: 0.1303 valAcc: 62.65% AUC: 68.63%
Epoch: 12 Loss: 0.1134 valLoss: 0.1294 valAcc: 60.68% AUC: 68.78%
Epoch: 13 Loss: 0.1101 valLoss: 0.1311 valAcc: 62.04% AUC: 68.75%
Epoch: 14 Loss: 0.1070 valLoss: 0.1336 valAcc: 62.19% AUC: 68.65%
Epoch: 15 Loss: 0.1043 valLoss: 0.1474 valAcc: 69.18% AUC: 68.25%
Epoch: 16 Loss: 0.1007 valLoss: 0.1488 valAcc: 67.46% AUC: 67.99%
Epoch: 17 Loss: 0.0976 valLoss: 0.1544 valAcc: 68.66% AUC: 67.98%
Epoch: 18 Loss: 0.0942 valLoss: 0.1576 valAcc: 68.68% AUC: 67.98%
Epoch: 19 Loss: 0.0905 valLoss: 0.1799 valAcc: 72.64% AUC: 67.77%
Epoch: 20 Loss: 0.0890 valLoss: 0.1646 valAcc: 66.38% AUC: 67.32%
Epoch: 21 Loss: 0.0854 valLoss: 0.1820 valAcc: 70.11% AUC: 67.25%
Epoch: 22 Loss: 0.0820 valLoss: 0.1678 valAcc: 65.63% AUC: 67.53%
Epoch: 23 Loss: 0.0792 valLoss: 0.1940 valAcc: 71.54% AUC: 67.65%
Epoch: 24 Loss: 0.0768 valLoss: 0.2013 valAcc: 71.06% AUC: 67.55%
Epoch: 25 Loss: 0.0737 valLoss: 0.1999 valAcc: 70.38% AUC: 67.71%
Epoch: 26 Loss: 0.0703 valLoss: 0.2156 valAcc: 72.56% AUC: 67.53%
Epoch: 27 Loss: 0.0678 valLoss: 0.2231 valAcc: 72.32% AUC: 67.73%
Epoch: 28 Loss: 0.0666 valLoss: 0.2298 valAcc: 74.05% AUC: 68.10%
Epoch: 29 Loss: 0.0638 valLoss: 0.2369 valAcc: 73.94% AUC: 67.30%
Epoch: 30 Loss: 0.0617 valLoss: 0.2261 valAcc: 71.84% AUC: 67.39%
Epoch: 31 Loss: 0.0591 valLoss: 0.2755 valAcc: 76.97% AUC: 67.39%
Epoch: 32 Loss: 0.0557 valLoss: 0.2678 valAcc: 76.85% AUC: 67.62%
Epoch: 33 Loss: 0.0536 valLoss: 0.2579 valAcc: 75.32% AUC: 67.86%
Epoch: 34 Loss: 0.0518 valLoss: 0.2632 valAcc: 75.73% AUC: 67.68%
Epoch: 35 Loss: 0.0501 valLoss: 0.2772 valAcc: 76.15% AUC: 67.86%
Epoch: 36 Loss: 0.0482 valLoss: 0.2948 valAcc: 77.51% AUC: 67.66%
Epoch: 37 Loss: 0.0463 valLoss: 0.2888 valAcc: 76.49% AUC: 67.85%
Epoch: 38 Loss: 0.0449 valLoss: 0.3148 valAcc: 79.16% AUC: 67.57%
Epoch: 39 Loss: 0.0440 valLoss: 0.2981 valAcc: 77.88% AUC: 67.85%
Epoch: 40 Loss: 0.0418 valLoss: 0.3037 valAcc: 77.29% AUC: 67.97%
Epoch: 41 Loss: 0.0404 valLoss: 0.3186 valAcc: 78.80% AUC: 67.54%
Epoch: 42 Loss: 0.0393 valLoss: 0.3079 valAcc: 77.64% AUC: 67.60%
Epoch: 43 Loss: 0.0384 valLoss: 0.3209 valAcc: 78.62% AUC: 67.83%
Epoch: 44 Loss: 0.0366 valLoss: 0.3219 valAcc: 78.16% AUC: 67.68%
Epoch: 45 Loss: 0.0355 valLoss: 0.3547 valAcc: 79.54% AUC: 67.20%
Epoch: 46 Loss: 0.0347 valLoss: 0.3638 valAcc: 80.64% AUC: 67.54%
Epoch: 47 Loss: 0.0337 valLoss: 0.3458 valAcc: 79.35% AUC: 67.42%
Epoch: 48 Loss: 0.0323 valLoss: 0.3239 valAcc: 78.31% AUC: 67.66%
Epoch: 49 Loss: 0.0302 valLoss: 0.3389 valAcc: 79.38% AUC: 68.22%
Epoch: 50 Loss: 0.0297 valLoss: 0.3299 valAcc: 78.52% AUC: 67.83%
Epoch: 51 Loss: 0.0289 valLoss: 0.3601 valAcc: 80.13% AUC: 67.36%
Epoch: 52 Loss: 0.0288 valLoss: 0.3618 valAcc: 79.75% AUC: 67.36%
Epoch: 53 Loss: 0.0279 valLoss: 0.3886 valAcc: 81.61% AUC: 67.49%
Epoch: 54 Loss: 0.0262 valLoss: 0.3881 valAcc: 81.77% AUC: 67.58%
Epoch: 55 Loss: 0.0255 valLoss: 0.3723 valAcc: 80.99% AUC: 68.00%
Epoch: 56 Loss: 0.0251 valLoss: 0.3888 valAcc: 81.65% AUC: 67.58%
Epoch: 57 Loss: 0.0245 valLoss: 0.3743 valAcc: 80.29% AUC: 67.74%
Epoch: 58 Loss: 0.0236 valLoss: 0.3948 valAcc: 80.99% AUC: 67.52%

## Starting Training & Testing;; @@ trained on: ../data/prepared_epitope_training.csv at: 09-01-2024#14:11:47 @@
## tf-version: 2.14.0|| float_type: float64
Model: "model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1 (InputLayer)        [(None, 1170, 1025)]      0         
                                                                 
 conv1d (Conv1D)             (None, 1170, 64)          459200    
                                                                 
 dropout (Dropout)           (None, 1170, 64)          0         
                                                                 
 batch_normalization (Batch  (None, 1170, 64)          256       
 Normalization)                                                  
                                                                 
 p_re_lu (PReLU)             (None, 1170, 64)          74880     
                                                                 
 conv1d_1 (Conv1D)           (None, 1170, 128)         57344     
                                                                 
 dropout_1 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_1 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_1 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_2 (Conv1D)           (None, 1170, 128)         114688    
                                                                 
 dropout_2 (Dropout)         (None, 1170, 128)         0         
                                                                 
 batch_normalization_2 (Bat  (None, 1170, 128)         512       
 chNormalization)                                                
                                                                 
 p_re_lu_2 (PReLU)           (None, 1170, 128)         149760    
                                                                 
 conv1d_3 (Conv1D)           (None, 1170, 64)          57344     
                                                                 
 dropout_3 (Dropout)         (None, 1170, 64)          0         
                                                                 
 batch_normalization_3 (Bat  (None, 1170, 64)          256       
 chNormalization)                                                
                                                                 
 p_re_lu_3 (PReLU)           (None, 1170, 64)          74880     
                                                                 
 conv1d_4 (Conv1D)           (None, 1170, 32)          14336     
                                                                 
 dropout_4 (Dropout)         (None, 1170, 32)          0         
                                                                 
 batch_normalization_4 (Bat  (None, 1170, 32)          128       
 chNormalization)                                                
                                                                 
 p_re_lu_4 (PReLU)           (None, 1170, 32)          37440     
                                                                 
 time_distributed (TimeDist  (None, 1170, 1)           33        
 ributed)                                                        
                                                                 
=================================================================
Total params: 1191329 (9.09 MB)
Trainable params: 1190497 (9.08 MB)
Non-trainable params: 832 (6.50 KB)
_________________________________________________________________
Shape featuresTable: (343, 2) | Shape labelvec: (343, 1)
                                                                                                                                                                                                               Shape inputData: (343, 1170, 1025)
Shape labelData: (343, 1170, 1)
                                                                 Epoch: 00 Loss: 0.1681 valLoss: 0.1326 valAcc: 11.32% AUC: 56.73%
                                                                  Epoch: 01 Loss: 0.1448 valLoss: 0.1318 valAcc: 11.38% AUC: 58.27%
Epoch: 02 Loss: 0.1393 valLoss: 0.1312 valAcc: 11.64% AUC: 59.59%
Epoch: 03 Loss: 0.1360 valLoss: 0.1306 valAcc: 11.91% AUC: 60.85%
Epoch: 04 Loss: 0.1335 valLoss: 0.1297 valAcc: 13.32% AUC: 61.99%
Epoch: 05 Loss: 0.1312 valLoss: 0.1288 valAcc: 17.30% AUC: 63.02%
Epoch: 06 Loss: 0.1298 valLoss: 0.1280 valAcc: 22.56% AUC: 63.79%
Epoch: 07 Loss: 0.1272 valLoss: 0.1273 valAcc: 30.08% AUC: 64.54%
Epoch: 08 Loss: 0.1246 valLoss: 0.1265 valAcc: 34.38% AUC: 65.28%
Epoch: 09 Loss: 0.1230 valLoss: 0.1258 valAcc: 36.77% AUC: 66.06%
                                                                  Epoch: 10 Loss: 0.1213 valLoss: 0.1259 valAcc: 42.66% AUC: 66.95%
Epoch: 11 Loss: 0.1196 valLoss: 0.1253 valAcc: 42.44% AUC: 67.25%
                                                                  Epoch: 12 Loss: 0.1171 valLoss: 0.1269 valAcc: 48.10% AUC: 67.72%
Epoch: 13 Loss: 0.1149 valLoss: 0.1300 valAcc: 53.79% AUC: 68.43%
Epoch: 14 Loss: 0.1121 valLoss: 0.1295 valAcc: 52.76% AUC: 68.74%
Epoch: 15 Loss: 0.1100 valLoss: 0.1432 valAcc: 61.93% AUC: 68.14%
                                                                  Epoch: 16 Loss: 0.1076 valLoss: 0.1395 valAcc: 58.40% AUC: 68.38%
Epoch: 17 Loss: 0.1041 valLoss: 0.1494 valAcc: 62.25% AUC: 68.49%
                                                                  Epoch: 18 Loss: 0.1004 valLoss: 0.1677 valAcc: 66.58% AUC: 68.15%
Epoch: 19 Loss: 0.0986 valLoss: 0.1869 valAcc: 70.17% AUC: 67.72%
Epoch: 20 Loss: 0.0953 valLoss: 0.2067 valAcc: 73.89% AUC: 67.81%
Epoch: 82 Loss: 0.0149 valLoss: 0.4228 valAcc: 80.14% AUC: 67.68%
Epoch: 22 Loss: 0.0912 valLoss: 0.2055 valAcc: 71.94% AUC: 67.32%
Epoch: 23 Loss: 0.0881 valLoss: 0.2187 valAcc: 73.36% AUC: 67.56%
                                                                  Epoch: 24 Loss: 0.0860 valLoss: 0.2272 valAcc: 74.14% AUC: 67.27%
Epoch: 25 Loss: 0.0832 valLoss: 0.2424 valAcc: 74.43% AUC: 67.10%
Epoch: 26 Loss: 0.0807 valLoss: 0.2743 valAcc: 78.61% AUC: 67.08%
Epoch: 27 Loss: 0.0774 valLoss: 0.2720 valAcc: 78.17% AUC: 67.30%
Epoch: 28 Loss: 0.0750 valLoss: 0.2679 valAcc: 77.25% AUC: 67.24%
Epoch: 29 Loss: 0.0724 valLoss: 0.2802 valAcc: 77.89% AUC: 67.06%
                                                                  Epoch: 30 Loss: 0.0704 valLoss: 0.2897 valAcc: 77.88% AUC: 66.82%
Epoch: 31 Loss: 0.0675 valLoss: 0.3180 valAcc: 80.53% AUC: 67.08%
Epoch: 32 Loss: 0.0654 valLoss: 0.3504 valAcc: 81.21% AUC: 66.66%
Epoch: 33 Loss: 0.0640 valLoss: 0.3296 valAcc: 80.43% AUC: 66.91%
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Epoch: 34 Loss: 0.0621 valLoss: 0.3337 valAcc: 80.08% AUC: 66.70%
Epoch: 35 Loss: 0.0600 valLoss: 0.3504 valAcc: 81.08% AUC: 66.83%
Epoch: 36 Loss: 0.0585 valLoss: 0.3772 valAcc: 82.00% AUC: 66.69%
Epoch: 37 Loss: 0.0558 valLoss: 0.3159 valAcc: 78.86% AUC: 67.05%
Epoch: 38 Loss: 0.0539 valLoss: 0.3513 valAcc: 81.10% AUC: 66.91%
Epoch: 39 Loss: 0.0512 valLoss: 0.3875 valAcc: 82.11% AUC: 66.93%
Epoch: 40 Loss: 0.0497 valLoss: 0.3303 valAcc: 79.91% AUC: 67.04%
Epoch: 41 Loss: 0.0489 valLoss: 0.3559 valAcc: 80.34% AUC: 66.95%
Epoch: 42 Loss: 0.0467 valLoss: 0.3863 valAcc: 82.35% AUC: 67.17%
Epoch: 43 Loss: 0.0439 valLoss: 0.4040 valAcc: 81.85% AUC: 66.75%
Epoch: 44 Loss: 0.0440 valLoss: 0.3950 valAcc: 81.57% AUC: 67.09%
Epoch: 45 Loss: 0.0426 valLoss: 0.3562 valAcc: 80.45% AUC: 66.91%
Epoch: 46 Loss: 0.0408 valLoss: 0.3530 valAcc: 80.43% AUC: 67.18%
Epoch: 47 Loss: 0.0387 valLoss: 0.4157 valAcc: 82.90% AUC: 67.08%
Epoch: 48 Loss: 0.0383 valLoss: 0.3822 valAcc: 80.91% AUC: 66.77%
Epoch: 49 Loss: 0.0365 valLoss: 0.4071 valAcc: 81.92% AUC: 66.58%
Epoch: 50 Loss: 0.0358 valLoss: 0.3958 valAcc: 81.12% AUC: 67.00%
Epoch: 51 Loss: 0.0343 valLoss: 0.3998 valAcc: 81.29% AUC: 66.89%
Epoch: 52 Loss: 0.0337 valLoss: 0.3718 valAcc: 79.91% AUC: 66.92%
Epoch: 53 Loss: 0.0323 valLoss: 0.4152 valAcc: 82.04% AUC: 66.24%
Epoch: 54 Loss: 0.0309 valLoss: 0.4733 valAcc: 83.98% AUC: 66.58%
Epoch: 55 Loss: 0.0305 valLoss: 0.4623 valAcc: 83.69% AUC: 66.52%
Epoch: 56 Loss: 0.0293 valLoss: 0.4873 valAcc: 83.37% AUC: 66.00%
Epoch: 57 Loss: 0.0286 valLoss: 0.4715 valAcc: 83.54% AUC: 66.73%
Epoch: 58 Loss: 0.0277 valLoss: 0.4531 valAcc: 83.23% AUC: 67.05%
Epoch: 59 Loss: 0.0265 valLoss: 0.4473 valAcc: 82.55% AUC: 66.45%
Epoch: 60 Loss: 0.0256 valLoss: 0.4673 valAcc: 83.32% AUC: 66.93%
Epoch: 61 Loss: 0.0249 valLoss: 0.4748 valAcc: 83.77% AUC: 67.07%
Epoch: 62 Loss: 0.0252 valLoss: 0.4892 valAcc: 83.69% AUC: 66.80%
Epoch: 63 Loss: 0.0239 valLoss: 0.4735 valAcc: 83.29% AUC: 66.63%
Epoch: 64 Loss: 0.0227 valLoss: 0.4830 valAcc: 83.62% AUC: 66.35%
Epoch: 65 Loss: 0.0224 valLoss: 0.5011 valAcc: 84.29% AUC: 66.55%
Epoch: 66 Loss: 0.0218 valLoss: 0.4612 valAcc: 82.83% AUC: 66.57%
Epoch: 67 Loss: 0.0210 valLoss: 0.4619 valAcc: 83.10% AUC: 67.13%
Epoch: 68 Loss: 0.0210 valLoss: 0.5052 valAcc: 84.00% AUC: 66.84%
Epoch: 69 Loss: 0.0201 valLoss: 0.5293 valAcc: 84.72% AUC: 66.86%
Epoch: 70 Loss: 0.0206 valLoss: 0.5104 valAcc: 84.10% AUC: 66.27%
Epoch: 71 Loss: 0.0205 valLoss: 0.4925 valAcc: 83.78% AUC: 66.79%
Epoch: 72 Loss: 0.0193 valLoss: 0.4964 valAcc: 83.78% AUC: 66.95%
Epoch: 73 Loss: 0.0186 valLoss: 0.5031 valAcc: 83.78% AUC: 66.84%
Epoch: 74 Loss: 0.0194 valLoss: 0.5146 valAcc: 84.02% AUC: 66.82%
Epoch: 75 Loss: 0.0193 valLoss: 0.5253 valAcc: 84.15% AUC: 66.44%
Epoch: 76 Loss: 0.0190 valLoss: 0.5653 valAcc: 85.03% AUC: 66.76%
Epoch: 77 Loss: 0.0183 valLoss: 0.5891 valAcc: 85.32% AUC: 66.55%
Epoch: 78 Loss: 0.0179 valLoss: 0.5463 valAcc: 84.51% AUC: 66.36%
Epoch: 79 Loss: 0.0172 valLoss: 0.5422 valAcc: 84.59% AUC: 66.75%
Epoch: 80 Loss: 0.0163 valLoss: 0.5271 valAcc: 83.51% AUC: 66.36%
Epoch: 81 Loss: 0.0162 valLoss: 0.5075 valAcc: 83.21% AUC: 66.56%
Epoch: 82 Loss: 0.0146 valLoss: 0.5194 valAcc: 83.18% AUC: 66.49%
Epoch: 83 Loss: 0.0146 valLoss: 0.5467 valAcc: 83.85% AUC: 65.58%
Epoch: 84 Loss: 0.0141 valLoss: 0.5326 valAcc: 83.39% AUC: 65.88%
Epoch: 85 Loss: 0.0141 valLoss: 0.5122 valAcc: 82.51% AUC: 66.08%
Epoch: 86 Loss: 0.0135 valLoss: 0.5069 valAcc: 82.46% AUC: 65.79%
Epoch: 87 Loss: 0.0132 valLoss: 0.4936 valAcc: 81.50% AUC: 66.16%
Epoch: 88 Loss: 0.0127 valLoss: 0.5295 valAcc: 83.16% AUC: 66.23%
Epoch: 89 Loss: 0.0119 valLoss: 0.5143 valAcc: 82.25% AUC: 66.36%
Epoch: 90 Loss: 0.0119 valLoss: 0.5455 valAcc: 83.19% AUC: 65.89%
Epoch: 91 Loss: 0.0115 valLoss: 0.5748 valAcc: 84.18% AUC: 65.82%
Epoch: 92 Loss: 0.0112 valLoss: 0.5728 valAcc: 84.28% AUC: 66.03%
Epoch: 93 Loss: 0.0118 valLoss: 0.5573 valAcc: 84.04% AUC: 65.94%
Epoch: 94 Loss: 0.0114 valLoss: 0.5807 valAcc: 84.44% AUC: 66.18%
Restoring model weights from the best epoch: 14 with the best VAL_AUC: 68.74%
Training time: 0:23:40.183874
## Testing ../data/prepared_epitope_testing.csv at: 09-01-2024#14:36:48 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.117839
The best cut-off value is: 0.65
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15055  1070]
 [ 1070   315]]
ValAcc: 87.78% specScore: 93.36% presScore: 22.74% recallScore: 22.74% F1Score: 22.74% MCC: 16.11% AUC: 72.06% AP: 17.96%
## Testing ../data/prepared_epitope_testing.csv at: 11-01-2024#11:07:33 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:01.714371
The best cut-off value is: 0.65
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15055  1070]
 [ 1070   315]]
ValAcc: 87.78% specScore: 93.36% presScore: 22.74% recallScore: 22.74% F1Score: 22.74% MCC: 16.11% AUC: 72.06% AP: 17.96%
## Testing ../data/prepared_epitope_testing.csv at: 23-01-2024#13:05:13 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
## Testing ../data/prepared_epitope_testing.csv at: 23-01-2024#13:09:02 ##
Shape featuresTable: (56, 2) | Shape labelvec: (56, 1)
Shape inputData: (58, 1170, 1025)
Shape labelData: (58, 1170, 1)
Testing time: 0:00:06.398771
The best cut-off value is: 0.65
confusion_matrix: [actual_neg=[TN, FP]; actual_pos=[FN, TP]]
[[15055  1070]
 [ 1070   315]]
ValAcc: 87.78% specScore: 93.36% presScore: 22.74% recallScore: 22.74% F1Score: 22.74% MCC: 16.11% AUC: 72.06% AP: 17.96%
